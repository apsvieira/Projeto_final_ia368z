{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch as tc\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import sys,os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import models\n",
    "from models import weights_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using CUDA. If it is not what you want, manually set this as False!\n"
     ]
    }
   ],
   "source": [
    "#Libera as funcionalidades da biblioteca cudnn\n",
    "cudnn.benchmark = True\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "#use_gpu = False\n",
    "if use_gpu:\n",
    "    print(\"You are using CUDA. If it is not what you want, manually set this as False!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nc = 3\n",
    "ngpu = 1\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "n_extra_d = 0\n",
    "n_extra_g = 1 # Aqui a jogada é que o gerador deve ser mais poderoso q o detetive\n",
    "imageSize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gabriel/Redes Neurais/Projeto_Final_GANS/Tutorial_2/images\r\n"
     ]
    }
   ],
   "source": [
    "!cd images && pwd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setando as transformações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'images/images2/': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls images/images2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anime-faces  min_anime-faces  teste\r\n"
     ]
    }
   ],
   "source": [
    "!ls dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = dset.ImageFolder(\n",
    "    root='/home/gabriel/Redes Neurais/Projeto_Final_GANS/Tutorial_2/dataset/min_anime-faces',\n",
    "    transform=transforms.Compose([\n",
    "            transforms.Scale((imageSize, imageSize)),\n",
    "            # transforms.CenterCrop(opt.imageSize),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)), # bring images to (-1,1)\n",
    "        ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setando o Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dataloader = tc.utils.data.DataLoader(dataset, batch_size=64, shuffle=True, num_workers=1)\n",
    "#126 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setando o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netD_1(nn.Module):\n",
    "    def __init__(self, ngpu, nz, nc, ndf,  n_extra_layers_d):\n",
    "        super(_netD_1, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        main = nn.Sequential(\n",
    "            # input is (nc) x 96 x 96\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False), # 5,3,1 for 96x96\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "        )\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers_d):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, ndf * 8),\n",
    "                            nn.Conv2d(ndf * 8, ndf * 8, 3, 1, 1, bias=False))\n",
    "            main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, ndf * 8),\n",
    "                            nn.BatchNorm2d(ndf * 8))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, ndf * 8),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "\n",
    "        main.add_module('final_layers.conv', nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False))\n",
    "        main.add_module('final_layers.sigmoid', nn.Sigmoid())\n",
    "        # state size. 1 x 1 x 1\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        gpu_ids = None\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            gpu_ids = range(self.ngpu)\n",
    "        output = nn.parallel.data_parallel(self.main, input, gpu_ids)\n",
    "        return output.view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DCGAN model, fully convolutional architecture\n",
    "class _netG_1(nn.Module):\n",
    "    def __init__(self, ngpu, nz, nc , ngf, n_extra_layers_g):\n",
    "        super(_netG_1, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        #self.nz = nz\n",
    "        #self.nc = nc\n",
    "        #self.ngf = ngf\n",
    "        main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            # state size. nz x 1 x 1\n",
    "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "        )\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers_g):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, ngf),\n",
    "                            nn.Conv2d(ngf, ngf, 3, 1, 1, bias=False))\n",
    "            main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, ngf),\n",
    "                            nn.BatchNorm2d(ngf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, ngf),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        main.add_module('final_layer.deconv', \n",
    "        \t             nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False)) # 5,3,1 for 96x96\n",
    "        main.add_module('final_layer.tanh', \n",
    "        \t             nn.Tanh())\n",
    "            # state size. (nc) x 96 x 96\n",
    "\n",
    "        self.main = main\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        gpu_ids = None\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            gpu_ids = range(self.ngpu)\n",
    "        return nn.parallel.data_parallel(self.main, input, gpu_ids), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#netG = _netG_1(ngpu, nz, nc, ngf, n_extra_g)\n",
    "netG_parallel = torch.nn.DataParallel(_netG_1(ngpu, nz, nc, ngf, n_extra_g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel (\n",
      "  (module): _netG_1 (\n",
      "    (main): Sequential (\n",
      "      (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (2): LeakyReLU (0.2, inplace)\n",
      "      (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (5): LeakyReLU (0.2, inplace)\n",
      "      (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (8): LeakyReLU (0.2, inplace)\n",
      "      (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (11): LeakyReLU (0.2, inplace)\n",
      "      (extra-layers-0.64.conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (extra-layers-0.64.batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (extra-layers-0.64.relu): LeakyReLU (0.2, inplace)\n",
      "      (final_layer.deconv): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (final_layer.tanh): Tanh ()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#print(netG)\n",
    "print(netG_parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#netD = _netD_1(ngpu, nz, nc, ndf, n_extra_d)\n",
    "\n",
    "netD_parallel = torch.nn.DataParallel(_netD_1(ngpu, nz, nc, ndf, n_extra_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel (\n",
      "  (module): _netD_1 (\n",
      "    (main): Sequential (\n",
      "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): LeakyReLU (0.2, inplace)\n",
      "      (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (4): LeakyReLU (0.2, inplace)\n",
      "      (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (7): LeakyReLU (0.2, inplace)\n",
      "      (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (10): LeakyReLU (0.2, inplace)\n",
      "      (final_layers.conv): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "      (final_layers.sigmoid): Sigmoid ()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(netD_parallel)\n",
    "#print(netD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando pesos pré-treinados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "#Parece ser um inicializador de pesos hardcoded\n",
    "#netG.apply(weights_init)\n",
    "netG_parallel.apply(weights_init)\n",
    "#netD.apply(weights_init)\n",
    "netD_parallel.apply(weights_init)\n",
    "print(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "load=False\n",
    "if load:\n",
    "    netD.load_state_dict(tc.load('path_d'))\n",
    "    \n",
    "    netG.load_state_dict(torch.load('path_G'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametros de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "criterion_MSE = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 64, 64])\n",
      "torch.Size([64, 100, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input = torch.FloatTensor(batch_size, 3, imageSize, imageSize)\n",
    "print(input.size())\n",
    "noise = torch.FloatTensor(batch_size, nz, 1, 1)\n",
    "print(noise.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parser.add_argument('--binary', action='store_true', help='z from bernoulli distribution, with prob=0.5')\n",
    "binary=False\n",
    "#Ele testa pergunta se vc quer que o seu Z venha da distribuição bernoulli\n",
    "if binary:\n",
    "    bernoulli_prob = torch.FloatTensor(batch_size, nz, 1, 1).fill_(0.5)\n",
    "    fixed_noise = torch.bernoulli(bernoulli_prob)\n",
    "else:\n",
    "    fixed_noise = torch.FloatTensor(batch_size, nz, 1, 1).normal_(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label = torch.FloatTensor(batch_size)\n",
    "real_label = 1\n",
    "fake_label = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast para CUDA, se quiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    #netD.cuda()\n",
    "    #netG.cuda()\n",
    "    netD_parallel.cuda()\n",
    "    netG_parallel.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    criterion_MSE = criterion_MSE.cuda()\n",
    "    input,label = input.cuda(), label.cuda()\n",
    "    noise, fixed_noise = noise.cuda(), fixed_noise.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformando tudo em variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "input = Variable(input)\n",
    "label = Variable(label)\n",
    "noise = Variable(noise)\n",
    "fixed_noise = Variable(fixed_noise)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setando o optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1, beta2 = 0.9,0.999\n",
    "lr = 2.0e-4\n",
    "#optimizerD = optim.Adam(netD.parameters(), lr = lr, betas = (beta1, beta2))\n",
    "optimizerD = optim.Adam(netD_parallel.parameters(), lr = lr, betas = (beta1, beta2))\n",
    "\n",
    "#optimizerG = optim.Adam(netG.parameters(), lr = lr, betas = (beta1, beta2))\n",
    "optimizerG = optim.Adam(netG_parallel.parameters(), lr = lr, betas = (beta1, beta2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando o diretório vazio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputDir = 'outputdir_parallel'\n",
    "\n",
    "try:\n",
    "    os.makedirs(outputDir)\n",
    "except OSError as err:\n",
    "    print(\"OS error: {0}\".format(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([64, 3, 64, 64])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(type(input))\n",
    "print(input.size())\n",
    "print(label.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_both_networks(num_epochs, dataloader, netD, netG, d_labelSmooth, outputDir, model_option =1,binary = False):\n",
    "    use_gpu = tc.cuda.is_available()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            start_iter = time.time()\n",
    "            ############################\n",
    "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            ###########################\n",
    "            # train with real\n",
    "            netD.zero_grad()\n",
    "            real_cpu, _ = data\n",
    "            batch_size = real_cpu.size(0)\n",
    "            input.data.resize_(real_cpu.size()).copy_(real_cpu)\n",
    "            label.data.resize_(batch_size).fill_(real_label - d_labelSmooth) # use smooth label for discriminator\n",
    "\n",
    "            output = netD(input)\n",
    "            errD_real = criterion(output, label)\n",
    "            errD_real.backward()\n",
    "            D_x = output.data.mean()\n",
    "            # train with fake\n",
    "            noise.data.resize_(batch_size, nz, 1, 1)\n",
    "            if binary:\n",
    "                bernoulli_prob.resize_(noise.data.size())\n",
    "                noise.data.copy_(2*(torch.bernoulli(bernoulli_prob)-0.5))\n",
    "            else:\n",
    "                noise.data.normal_(0, 1)\n",
    "            fake,z_prediction = netG(noise)\n",
    "            label.data.fill_(fake_label)\n",
    "            output = netD(fake.detach()) # add \".detach()\" to avoid backprop through G\n",
    "            errD_fake = criterion(output, label)\n",
    "            errD_fake.backward() # gradients for fake/real will be accumulated\n",
    "            D_G_z1 = output.data.mean()\n",
    "            errD = errD_real + errD_fake\n",
    "            optimizerD.step() # .step() can be called once the gradients are computed\n",
    "\n",
    "            ############################\n",
    "            # (2) Update G network: maximize log(D(G(z)))\n",
    "            ###########################\n",
    "            netG.zero_grad()\n",
    "            label.data.fill_(real_label) # fake labels are real for generator cost\n",
    "            output = netD(fake)\n",
    "            errG = criterion(output, label)\n",
    "            errG.backward(retain_variables=True) # True if backward through the graph for the second time\n",
    "            if model_option == 2: # with z predictor\n",
    "                errG_z = criterion_MSE(z_prediction, noise)\n",
    "                errG_z.backward()\n",
    "            D_G_z2 = output.data.mean()\n",
    "            optimizerG.step()\n",
    "\n",
    "            end_iter = time.time()\n",
    "            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f Elapsed %.2f s'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.data[0], errG.data[0], D_x, D_G_z1, D_G_z2, end_iter-start_iter))\n",
    "            if i % 100 == 0:\n",
    "                # the first 64 samples from the mini-batch are saved.\n",
    "                vutils.save_image(real_cpu[0:64,:,:,:],\n",
    "                        '%s/real_samples.png' % outputDir, nrow=8)\n",
    "                fake,_ = netG(fixed_noise)\n",
    "                vutils.save_image(fake.data[0:64,:,:,:],\n",
    "                        '%s/fake_samples_epoch_%03d.png' % (outputDir, epoch), nrow=8)\n",
    "        if epoch % 1 == 0:\n",
    "            # do checkpointing\n",
    "            torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (outputDir, epoch))\n",
    "            torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (outputDir, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/anaconda3/envs/py35/lib/python3.5/site-packages/torch/nn/functional.py:767: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "/home/gabriel/anaconda3/envs/py35/lib/python3.5/site-packages/torch/autograd/__init__.py:92: UserWarning: retain_variables option is deprecated and will be removed in 0.3. Use retain_graph instead.\n",
      "  warnings.warn(\"retain_variables option is deprecated and will be removed in 0.3. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/50][0/472] Loss_D: 0.5621 Loss_G: 5.1006 D(x): 0.6846 D(G(z)): 0.0074 / 0.0085 Elapsed 0.08 s\n",
      "[0/50][1/472] Loss_D: 0.5265 Loss_G: 5.0220 D(x): 0.8248 D(G(z)): 0.0096 / 0.0094 Elapsed 0.06 s\n",
      "[0/50][2/472] Loss_D: 0.5631 Loss_G: 4.9666 D(x): 0.8771 D(G(z)): 0.0164 / 0.0104 Elapsed 0.06 s\n",
      "[0/50][3/472] Loss_D: 0.5530 Loss_G: 5.1331 D(x): 0.8617 D(G(z)): 0.0182 / 0.0088 Elapsed 0.06 s\n",
      "[0/50][4/472] Loss_D: 0.5412 Loss_G: 5.6734 D(x): 0.7434 D(G(z)): 0.0071 / 0.0046 Elapsed 0.06 s\n",
      "[0/50][5/472] Loss_D: 0.5365 Loss_G: 5.6497 D(x): 0.7574 D(G(z)): 0.0062 / 0.0050 Elapsed 0.06 s\n",
      "[0/50][6/472] Loss_D: 0.5836 Loss_G: 4.8470 D(x): 0.6550 D(G(z)): 0.0075 / 0.0109 Elapsed 0.06 s\n",
      "[0/50][7/472] Loss_D: 0.5613 Loss_G: 4.2667 D(x): 0.6841 D(G(z)): 0.0090 / 0.0190 Elapsed 0.06 s\n",
      "[0/50][8/472] Loss_D: 0.5963 Loss_G: 3.6835 D(x): 0.8914 D(G(z)): 0.0343 / 0.0346 Elapsed 0.06 s\n",
      "[0/50][9/472] Loss_D: 0.6759 Loss_G: 4.0941 D(x): 0.9393 D(G(z)): 0.0442 / 0.0204 Elapsed 0.06 s\n",
      "[0/50][10/472] Loss_D: 0.5588 Loss_G: 5.0739 D(x): 0.8714 D(G(z)): 0.0235 / 0.0084 Elapsed 0.06 s\n",
      "[0/50][11/472] Loss_D: 0.5405 Loss_G: 5.5847 D(x): 0.8214 D(G(z)): 0.0144 / 0.0046 Elapsed 0.06 s\n",
      "[0/50][12/472] Loss_D: 0.6507 Loss_G: 5.7060 D(x): 0.5661 D(G(z)): 0.0045 / 0.0044 Elapsed 0.06 s\n",
      "[0/50][13/472] Loss_D: 0.5961 Loss_G: 4.7204 D(x): 0.6277 D(G(z)): 0.0057 / 0.0110 Elapsed 0.06 s\n",
      "[0/50][14/472] Loss_D: 0.5360 Loss_G: 3.9533 D(x): 0.7999 D(G(z)): 0.0163 / 0.0257 Elapsed 0.06 s\n",
      "[0/50][15/472] Loss_D: 0.5678 Loss_G: 3.6225 D(x): 0.8553 D(G(z)): 0.0347 / 0.0347 Elapsed 0.05 s\n",
      "[0/50][16/472] Loss_D: 0.5924 Loss_G: 3.6495 D(x): 0.8620 D(G(z)): 0.0569 / 0.0340 Elapsed 0.06 s\n",
      "[0/50][17/472] Loss_D: 0.5536 Loss_G: 4.7528 D(x): 0.8382 D(G(z)): 0.0267 / 0.0127 Elapsed 0.06 s\n",
      "[0/50][18/472] Loss_D: 0.5630 Loss_G: 4.8102 D(x): 0.7104 D(G(z)): 0.0162 / 0.0107 Elapsed 0.06 s\n",
      "[0/50][19/472] Loss_D: 0.5935 Loss_G: 4.3522 D(x): 0.6526 D(G(z)): 0.0147 / 0.0165 Elapsed 0.06 s\n",
      "[0/50][20/472] Loss_D: 0.5383 Loss_G: 4.1714 D(x): 0.7602 D(G(z)): 0.0175 / 0.0196 Elapsed 0.06 s\n",
      "[0/50][21/472] Loss_D: 0.5533 Loss_G: 4.2021 D(x): 0.8518 D(G(z)): 0.0240 / 0.0194 Elapsed 0.06 s\n",
      "[0/50][22/472] Loss_D: 0.5386 Loss_G: 4.7139 D(x): 0.8229 D(G(z)): 0.0174 / 0.0120 Elapsed 0.06 s\n",
      "[0/50][23/472] Loss_D: 0.5460 Loss_G: 4.9502 D(x): 0.8402 D(G(z)): 0.0167 / 0.0097 Elapsed 0.06 s\n",
      "[0/50][24/472] Loss_D: 0.5273 Loss_G: 5.7099 D(x): 0.7914 D(G(z)): 0.0071 / 0.0043 Elapsed 0.06 s\n",
      "[0/50][25/472] Loss_D: 0.5336 Loss_G: 5.2504 D(x): 0.7575 D(G(z)): 0.0094 / 0.0065 Elapsed 0.06 s\n",
      "[0/50][26/472] Loss_D: 0.5454 Loss_G: 4.9602 D(x): 0.7152 D(G(z)): 0.0094 / 0.0093 Elapsed 0.06 s\n",
      "[0/50][27/472] Loss_D: 0.5531 Loss_G: 5.3531 D(x): 0.7002 D(G(z)): 0.0049 / 0.0067 Elapsed 0.06 s\n",
      "[0/50][28/472] Loss_D: 0.5470 Loss_G: 3.9079 D(x): 0.7976 D(G(z)): 0.0243 / 0.0267 Elapsed 0.06 s\n",
      "[0/50][29/472] Loss_D: 0.5600 Loss_G: 3.9679 D(x): 0.8423 D(G(z)): 0.0365 / 0.0273 Elapsed 0.06 s\n",
      "[0/50][30/472] Loss_D: 0.5467 Loss_G: 4.6064 D(x): 0.8329 D(G(z)): 0.0217 / 0.0136 Elapsed 0.06 s\n",
      "[0/50][31/472] Loss_D: 0.5493 Loss_G: 4.6409 D(x): 0.7654 D(G(z)): 0.0202 / 0.0136 Elapsed 0.06 s\n",
      "[0/50][32/472] Loss_D: 0.5525 Loss_G: 4.2571 D(x): 0.7572 D(G(z)): 0.0271 / 0.0191 Elapsed 0.05 s\n",
      "[0/50][33/472] Loss_D: 0.5743 Loss_G: 4.0415 D(x): 0.7211 D(G(z)): 0.0262 / 0.0227 Elapsed 0.05 s\n",
      "[0/50][34/472] Loss_D: 0.5401 Loss_G: 5.4496 D(x): 0.8513 D(G(z)): 0.0094 / 0.0062 Elapsed 0.05 s\n",
      "[0/50][35/472] Loss_D: 0.5463 Loss_G: 5.2154 D(x): 0.8582 D(G(z)): 0.0151 / 0.0078 Elapsed 0.06 s\n",
      "[0/50][36/472] Loss_D: 0.5336 Loss_G: 5.7329 D(x): 0.7710 D(G(z)): 0.0076 / 0.0045 Elapsed 0.06 s\n",
      "[0/50][37/472] Loss_D: 0.5283 Loss_G: 5.7259 D(x): 0.7817 D(G(z)): 0.0064 / 0.0044 Elapsed 0.06 s\n",
      "[0/50][38/472] Loss_D: 0.5295 Loss_G: 5.7252 D(x): 0.7453 D(G(z)): 0.0056 / 0.0049 Elapsed 0.06 s\n",
      "[0/50][39/472] Loss_D: 0.6279 Loss_G: 5.2455 D(x): 0.6036 D(G(z)): 0.0029 / 0.0067 Elapsed 0.06 s\n",
      "[0/50][40/472] Loss_D: 0.5465 Loss_G: 3.8734 D(x): 0.8290 D(G(z)): 0.0180 / 0.0278 Elapsed 0.06 s\n",
      "[0/50][41/472] Loss_D: 0.5641 Loss_G: 3.7671 D(x): 0.8491 D(G(z)): 0.0344 / 0.0328 Elapsed 0.06 s\n",
      "[0/50][42/472] Loss_D: 0.5897 Loss_G: 4.2393 D(x): 0.8961 D(G(z)): 0.0337 / 0.0186 Elapsed 0.06 s\n",
      "[0/50][43/472] Loss_D: 0.5565 Loss_G: 4.9821 D(x): 0.8669 D(G(z)): 0.0251 / 0.0089 Elapsed 0.06 s\n",
      "[0/50][44/472] Loss_D: 0.5587 Loss_G: 5.2396 D(x): 0.6932 D(G(z)): 0.0110 / 0.0069 Elapsed 0.06 s\n",
      "[0/50][45/472] Loss_D: 0.6317 Loss_G: 4.2830 D(x): 0.5980 D(G(z)): 0.0118 / 0.0178 Elapsed 0.06 s\n",
      "[0/50][46/472] Loss_D: 0.5397 Loss_G: 4.0236 D(x): 0.8300 D(G(z)): 0.0205 / 0.0225 Elapsed 0.06 s\n",
      "[0/50][47/472] Loss_D: 0.5484 Loss_G: 3.9914 D(x): 0.8345 D(G(z)): 0.0271 / 0.0233 Elapsed 0.06 s\n",
      "[0/50][48/472] Loss_D: 0.5549 Loss_G: 4.6903 D(x): 0.8573 D(G(z)): 0.0179 / 0.0116 Elapsed 0.06 s\n",
      "[0/50][49/472] Loss_D: 0.5520 Loss_G: 4.8539 D(x): 0.8453 D(G(z)): 0.0209 / 0.0104 Elapsed 0.06 s\n",
      "[0/50][50/472] Loss_D: 0.5784 Loss_G: 4.7561 D(x): 0.6785 D(G(z)): 0.0146 / 0.0114 Elapsed 0.06 s\n",
      "[0/50][51/472] Loss_D: 0.5520 Loss_G: 4.3861 D(x): 0.7084 D(G(z)): 0.0150 / 0.0157 Elapsed 0.06 s\n",
      "[0/50][52/472] Loss_D: 0.5414 Loss_G: 4.3434 D(x): 0.8097 D(G(z)): 0.0203 / 0.0179 Elapsed 0.06 s\n",
      "[0/50][53/472] Loss_D: 0.5660 Loss_G: 3.9468 D(x): 0.7198 D(G(z)): 0.0241 / 0.0253 Elapsed 0.05 s\n",
      "[0/50][54/472] Loss_D: 0.5565 Loss_G: 3.7315 D(x): 0.8181 D(G(z)): 0.0365 / 0.0296 Elapsed 0.06 s\n",
      "[0/50][55/472] Loss_D: 0.5590 Loss_G: 4.4797 D(x): 0.8644 D(G(z)): 0.0246 / 0.0142 Elapsed 0.06 s\n",
      "[0/50][56/472] Loss_D: 0.5308 Loss_G: 5.4667 D(x): 0.7832 D(G(z)): 0.0105 / 0.0064 Elapsed 0.06 s\n",
      "[0/50][57/472] Loss_D: 0.5745 Loss_G: 4.3263 D(x): 0.7052 D(G(z)): 0.0242 / 0.0192 Elapsed 0.06 s\n",
      "[0/50][58/472] Loss_D: 0.5376 Loss_G: 5.0552 D(x): 0.7739 D(G(z)): 0.0096 / 0.0086 Elapsed 0.06 s\n",
      "[0/50][59/472] Loss_D: 0.5393 Loss_G: 4.3334 D(x): 0.7571 D(G(z)): 0.0188 / 0.0178 Elapsed 0.06 s\n",
      "[0/50][60/472] Loss_D: 0.5581 Loss_G: 4.9352 D(x): 0.8599 D(G(z)): 0.0135 / 0.0095 Elapsed 0.06 s\n",
      "[0/50][61/472] Loss_D: 0.5389 Loss_G: 4.9544 D(x): 0.8112 D(G(z)): 0.0141 / 0.0092 Elapsed 0.06 s\n",
      "[0/50][62/472] Loss_D: 0.5601 Loss_G: 4.7999 D(x): 0.7146 D(G(z)): 0.0114 / 0.0109 Elapsed 0.06 s\n",
      "[0/50][63/472] Loss_D: 0.5417 Loss_G: 5.9847 D(x): 0.7334 D(G(z)): 0.0027 / 0.0034 Elapsed 0.06 s\n",
      "[0/50][64/472] Loss_D: 0.5325 Loss_G: 4.7784 D(x): 0.8360 D(G(z)): 0.0118 / 0.0115 Elapsed 0.06 s\n",
      "[0/50][65/472] Loss_D: 0.5642 Loss_G: 4.6459 D(x): 0.8806 D(G(z)): 0.0222 / 0.0130 Elapsed 0.06 s\n",
      "[0/50][66/472] Loss_D: 0.5341 Loss_G: 5.0201 D(x): 0.7855 D(G(z)): 0.0149 / 0.0091 Elapsed 0.06 s\n",
      "[0/50][67/472] Loss_D: 0.5468 Loss_G: 5.3653 D(x): 0.7472 D(G(z)): 0.0079 / 0.0061 Elapsed 0.06 s\n",
      "[0/50][68/472] Loss_D: 0.5491 Loss_G: 4.3765 D(x): 0.7353 D(G(z)): 0.0172 / 0.0160 Elapsed 0.06 s\n",
      "[0/50][69/472] Loss_D: 0.5370 Loss_G: 4.8017 D(x): 0.8180 D(G(z)): 0.0134 / 0.0106 Elapsed 0.06 s\n",
      "[0/50][70/472] Loss_D: 0.5478 Loss_G: 4.7421 D(x): 0.7454 D(G(z)): 0.0139 / 0.0126 Elapsed 0.06 s\n",
      "[0/50][71/472] Loss_D: 0.5472 Loss_G: 4.2364 D(x): 0.7638 D(G(z)): 0.0196 / 0.0186 Elapsed 0.06 s\n",
      "[0/50][72/472] Loss_D: 0.5339 Loss_G: 4.5201 D(x): 0.7929 D(G(z)): 0.0160 / 0.0141 Elapsed 0.06 s\n",
      "[0/50][73/472] Loss_D: 0.6117 Loss_G: 4.0254 D(x): 0.6163 D(G(z)): 0.0115 / 0.0216 Elapsed 0.06 s\n",
      "[0/50][74/472] Loss_D: 0.6109 Loss_G: 3.8854 D(x): 0.9163 D(G(z)): 0.0290 / 0.0264 Elapsed 0.06 s\n",
      "[0/50][75/472] Loss_D: 0.7097 Loss_G: 4.0124 D(x): 0.9466 D(G(z)): 0.0575 / 0.0234 Elapsed 0.06 s\n",
      "[0/50][76/472] Loss_D: 0.5741 Loss_G: 5.5338 D(x): 0.8859 D(G(z)): 0.0201 / 0.0056 Elapsed 0.05 s\n",
      "[0/50][77/472] Loss_D: 0.5460 Loss_G: 6.3143 D(x): 0.7114 D(G(z)): 0.0062 / 0.0029 Elapsed 0.06 s\n",
      "[0/50][78/472] Loss_D: 0.6677 Loss_G: 5.1505 D(x): 0.5663 D(G(z)): 0.0063 / 0.0088 Elapsed 0.06 s\n",
      "[0/50][79/472] Loss_D: 0.5264 Loss_G: 4.6514 D(x): 0.7995 D(G(z)): 0.0102 / 0.0129 Elapsed 0.06 s\n",
      "[0/50][80/472] Loss_D: 0.5420 Loss_G: 4.1694 D(x): 0.7887 D(G(z)): 0.0184 / 0.0211 Elapsed 0.06 s\n",
      "[0/50][81/472] Loss_D: 0.5336 Loss_G: 4.5503 D(x): 0.8313 D(G(z)): 0.0146 / 0.0138 Elapsed 0.06 s\n",
      "[0/50][82/472] Loss_D: 0.5526 Loss_G: 4.1910 D(x): 0.8342 D(G(z)): 0.0310 / 0.0213 Elapsed 0.06 s\n",
      "[0/50][83/472] Loss_D: 0.5451 Loss_G: 4.5638 D(x): 0.8262 D(G(z)): 0.0243 / 0.0136 Elapsed 0.06 s\n",
      "[0/50][84/472] Loss_D: 0.5489 Loss_G: 5.1172 D(x): 0.7270 D(G(z)): 0.0111 / 0.0080 Elapsed 0.06 s\n",
      "[0/50][85/472] Loss_D: 0.5242 Loss_G: 5.7390 D(x): 0.8125 D(G(z)): 0.0075 / 0.0051 Elapsed 0.06 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/50][86/472] Loss_D: 0.5467 Loss_G: 4.6964 D(x): 0.7120 D(G(z)): 0.0123 / 0.0116 Elapsed 0.06 s\n",
      "[0/50][87/472] Loss_D: 0.5331 Loss_G: 5.1503 D(x): 0.8427 D(G(z)): 0.0111 / 0.0081 Elapsed 0.06 s\n",
      "[0/50][88/472] Loss_D: 0.5299 Loss_G: 5.1957 D(x): 0.7492 D(G(z)): 0.0078 / 0.0068 Elapsed 0.05 s\n",
      "[0/50][89/472] Loss_D: 0.5571 Loss_G: 4.7731 D(x): 0.6943 D(G(z)): 0.0093 / 0.0113 Elapsed 0.06 s\n",
      "[0/50][90/472] Loss_D: 0.5355 Loss_G: 4.1892 D(x): 0.7900 D(G(z)): 0.0179 / 0.0193 Elapsed 0.06 s\n",
      "[0/50][91/472] Loss_D: 0.5968 Loss_G: 4.6059 D(x): 0.9104 D(G(z)): 0.0191 / 0.0128 Elapsed 0.06 s\n",
      "[0/50][92/472] Loss_D: 0.5549 Loss_G: 5.2033 D(x): 0.8661 D(G(z)): 0.0162 / 0.0077 Elapsed 0.06 s\n",
      "[0/50][93/472] Loss_D: 0.5285 Loss_G: 5.5367 D(x): 0.7971 D(G(z)): 0.0108 / 0.0052 Elapsed 0.06 s\n",
      "[0/50][94/472] Loss_D: 0.5692 Loss_G: 5.3400 D(x): 0.6753 D(G(z)): 0.0072 / 0.0061 Elapsed 0.06 s\n",
      "[0/50][95/472] Loss_D: 0.5851 Loss_G: 4.9175 D(x): 0.6459 D(G(z)): 0.0065 / 0.0098 Elapsed 0.05 s\n",
      "[0/50][96/472] Loss_D: 0.5312 Loss_G: 4.5003 D(x): 0.8085 D(G(z)): 0.0110 / 0.0148 Elapsed 0.06 s\n",
      "[0/50][97/472] Loss_D: 0.5563 Loss_G: 4.3045 D(x): 0.8616 D(G(z)): 0.0203 / 0.0179 Elapsed 0.06 s\n",
      "[0/50][98/472] Loss_D: 0.5652 Loss_G: 4.5841 D(x): 0.8629 D(G(z)): 0.0234 / 0.0135 Elapsed 0.06 s\n",
      "[0/50][99/472] Loss_D: 0.5739 Loss_G: 5.3252 D(x): 0.8945 D(G(z)): 0.0173 / 0.0065 Elapsed 0.06 s\n",
      "[0/50][100/472] Loss_D: 0.5665 Loss_G: 5.5862 D(x): 0.7014 D(G(z)): 0.0084 / 0.0051 Elapsed 0.06 s\n",
      "[0/50][101/472] Loss_D: 0.5625 Loss_G: 5.3583 D(x): 0.6761 D(G(z)): 0.0053 / 0.0058 Elapsed 0.06 s\n",
      "[0/50][102/472] Loss_D: 0.5477 Loss_G: 4.8022 D(x): 0.7010 D(G(z)): 0.0079 / 0.0118 Elapsed 0.05 s\n",
      "[0/50][103/472] Loss_D: 0.5434 Loss_G: 4.2318 D(x): 0.8372 D(G(z)): 0.0186 / 0.0193 Elapsed 0.06 s\n",
      "[0/50][104/472] Loss_D: 0.5416 Loss_G: 4.5627 D(x): 0.8401 D(G(z)): 0.0176 / 0.0142 Elapsed 0.05 s\n",
      "[0/50][105/472] Loss_D: 0.5444 Loss_G: 4.5984 D(x): 0.8400 D(G(z)): 0.0219 / 0.0137 Elapsed 0.06 s\n",
      "[0/50][106/472] Loss_D: 0.5405 Loss_G: 4.9770 D(x): 0.8047 D(G(z)): 0.0166 / 0.0098 Elapsed 0.06 s\n",
      "[0/50][107/472] Loss_D: 0.5288 Loss_G: 5.3838 D(x): 0.8197 D(G(z)): 0.0117 / 0.0064 Elapsed 0.06 s\n",
      "[0/50][108/472] Loss_D: 0.5601 Loss_G: 5.3887 D(x): 0.6826 D(G(z)): 0.0071 / 0.0064 Elapsed 0.06 s\n",
      "[0/50][109/472] Loss_D: 0.5637 Loss_G: 5.1183 D(x): 0.6859 D(G(z)): 0.0064 / 0.0090 Elapsed 0.06 s\n",
      "[0/50][110/472] Loss_D: 0.5349 Loss_G: 4.1903 D(x): 0.8044 D(G(z)): 0.0166 / 0.0194 Elapsed 0.06 s\n",
      "[0/50][111/472] Loss_D: 0.5339 Loss_G: 4.6003 D(x): 0.8363 D(G(z)): 0.0131 / 0.0123 Elapsed 0.06 s\n",
      "[0/50][112/472] Loss_D: 0.5415 Loss_G: 4.2238 D(x): 0.7959 D(G(z)): 0.0242 / 0.0206 Elapsed 0.06 s\n",
      "[0/50][113/472] Loss_D: 0.5831 Loss_G: 4.7790 D(x): 0.8972 D(G(z)): 0.0208 / 0.0111 Elapsed 0.06 s\n",
      "[0/50][114/472] Loss_D: 0.5414 Loss_G: 4.9126 D(x): 0.8192 D(G(z)): 0.0219 / 0.0106 Elapsed 0.06 s\n",
      "[0/50][115/472] Loss_D: 0.5529 Loss_G: 5.3404 D(x): 0.6986 D(G(z)): 0.0092 / 0.0067 Elapsed 0.06 s\n",
      "[0/50][116/472] Loss_D: 0.5802 Loss_G: 4.6621 D(x): 0.6539 D(G(z)): 0.0096 / 0.0124 Elapsed 0.06 s\n",
      "[0/50][117/472] Loss_D: 0.5319 Loss_G: 4.5762 D(x): 0.7771 D(G(z)): 0.0116 / 0.0143 Elapsed 0.06 s\n",
      "[0/50][118/472] Loss_D: 0.5239 Loss_G: 4.8021 D(x): 0.7953 D(G(z)): 0.0102 / 0.0114 Elapsed 0.06 s\n",
      "[0/50][119/472] Loss_D: 0.5549 Loss_G: 3.8500 D(x): 0.8154 D(G(z)): 0.0335 / 0.0281 Elapsed 0.06 s\n",
      "[0/50][120/472] Loss_D: 0.6027 Loss_G: 4.5144 D(x): 0.9095 D(G(z)): 0.0278 / 0.0144 Elapsed 0.06 s\n",
      "[0/50][121/472] Loss_D: 0.5655 Loss_G: 5.4768 D(x): 0.8774 D(G(z)): 0.0187 / 0.0070 Elapsed 0.06 s\n",
      "[0/50][122/472] Loss_D: 0.5303 Loss_G: 6.2641 D(x): 0.8101 D(G(z)): 0.0080 / 0.0029 Elapsed 0.06 s\n",
      "[0/50][123/472] Loss_D: 0.5902 Loss_G: 5.6954 D(x): 0.6382 D(G(z)): 0.0065 / 0.0052 Elapsed 0.06 s\n",
      "[0/50][124/472] Loss_D: 0.6028 Loss_G: 5.2389 D(x): 0.6203 D(G(z)): 0.0041 / 0.0075 Elapsed 0.06 s\n",
      "[0/50][125/472] Loss_D: 0.5333 Loss_G: 4.7569 D(x): 0.7865 D(G(z)): 0.0067 / 0.0114 Elapsed 0.06 s\n",
      "[0/50][126/472] Loss_D: 0.5361 Loss_G: 4.2576 D(x): 0.8134 D(G(z)): 0.0142 / 0.0177 Elapsed 0.06 s\n",
      "[0/50][127/472] Loss_D: 0.6132 Loss_G: 4.2389 D(x): 0.9136 D(G(z)): 0.0260 / 0.0188 Elapsed 0.06 s\n",
      "[0/50][128/472] Loss_D: 0.5467 Loss_G: 4.8163 D(x): 0.8480 D(G(z)): 0.0172 / 0.0100 Elapsed 0.06 s\n",
      "[0/50][129/472] Loss_D: 0.5606 Loss_G: 4.9695 D(x): 0.8782 D(G(z)): 0.0208 / 0.0089 Elapsed 0.06 s\n",
      "[0/50][130/472] Loss_D: 0.5489 Loss_G: 5.6373 D(x): 0.7109 D(G(z)): 0.0089 / 0.0052 Elapsed 0.06 s\n",
      "[0/50][131/472] Loss_D: 0.5814 Loss_G: 5.1887 D(x): 0.6548 D(G(z)): 0.0067 / 0.0068 Elapsed 0.06 s\n",
      "[0/50][132/472] Loss_D: 0.6274 Loss_G: 4.3666 D(x): 0.5952 D(G(z)): 0.0075 / 0.0158 Elapsed 0.06 s\n",
      "[0/50][133/472] Loss_D: 0.5464 Loss_G: 3.5168 D(x): 0.8224 D(G(z)): 0.0239 / 0.0361 Elapsed 0.06 s\n",
      "[0/50][134/472] Loss_D: 0.6727 Loss_G: 3.8382 D(x): 0.9407 D(G(z)): 0.0374 / 0.0269 Elapsed 0.05 s\n",
      "[0/50][135/472] Loss_D: 0.5706 Loss_G: 4.1906 D(x): 0.8627 D(G(z)): 0.0386 / 0.0188 Elapsed 0.06 s\n",
      "[0/50][136/472] Loss_D: 0.5477 Loss_G: 5.3569 D(x): 0.8499 D(G(z)): 0.0153 / 0.0061 Elapsed 0.06 s\n",
      "[0/50][137/472] Loss_D: 0.5266 Loss_G: 5.8454 D(x): 0.7963 D(G(z)): 0.0095 / 0.0039 Elapsed 0.06 s\n",
      "[0/50][138/472] Loss_D: 0.6524 Loss_G: 5.1525 D(x): 0.5733 D(G(z)): 0.0059 / 0.0075 Elapsed 0.06 s\n",
      "[0/50][139/472] Loss_D: 0.5364 Loss_G: 4.6902 D(x): 0.7381 D(G(z)): 0.0071 / 0.0109 Elapsed 0.06 s\n",
      "[0/50][140/472] Loss_D: 0.5383 Loss_G: 4.2348 D(x): 0.8367 D(G(z)): 0.0172 / 0.0199 Elapsed 0.06 s\n",
      "[0/50][141/472] Loss_D: 0.5471 Loss_G: 4.4736 D(x): 0.8587 D(G(z)): 0.0189 / 0.0153 Elapsed 0.06 s\n",
      "[0/50][142/472] Loss_D: 0.5601 Loss_G: 4.7658 D(x): 0.8658 D(G(z)): 0.0205 / 0.0111 Elapsed 0.06 s\n",
      "[0/50][143/472] Loss_D: 0.5439 Loss_G: 4.8099 D(x): 0.7729 D(G(z)): 0.0186 / 0.0102 Elapsed 0.06 s\n",
      "[0/50][144/472] Loss_D: 0.5319 Loss_G: 5.9289 D(x): 0.7522 D(G(z)): 0.0055 / 0.0036 Elapsed 0.06 s\n",
      "[0/50][145/472] Loss_D: 0.5492 Loss_G: 4.9036 D(x): 0.7157 D(G(z)): 0.0102 / 0.0097 Elapsed 0.06 s\n",
      "[0/50][146/472] Loss_D: 0.5228 Loss_G: 5.3222 D(x): 0.8021 D(G(z)): 0.0071 / 0.0064 Elapsed 0.06 s\n",
      "[0/50][147/472] Loss_D: 0.5628 Loss_G: 5.1086 D(x): 0.6747 D(G(z)): 0.0055 / 0.0081 Elapsed 0.06 s\n",
      "[0/50][148/472] Loss_D: 0.5462 Loss_G: 4.1839 D(x): 0.8170 D(G(z)): 0.0184 / 0.0207 Elapsed 0.06 s\n",
      "[0/50][149/472] Loss_D: 0.5312 Loss_G: 8.0481 D(x): 0.8493 D(G(z)): 0.0004 / 0.0004 Elapsed 0.06 s\n",
      "[0/50][150/472] Loss_D: 0.5797 Loss_G: 4.6264 D(x): 0.8848 D(G(z)): 0.0227 / 0.0130 Elapsed 0.06 s\n",
      "[0/50][151/472] Loss_D: 0.5359 Loss_G: 5.1078 D(x): 0.8291 D(G(z)): 0.0164 / 0.0082 Elapsed 0.06 s\n",
      "[0/50][152/472] Loss_D: 0.5384 Loss_G: 5.2101 D(x): 0.8043 D(G(z)): 0.0149 / 0.0069 Elapsed 0.06 s\n",
      "[0/50][153/472] Loss_D: 0.5466 Loss_G: 5.5135 D(x): 0.7213 D(G(z)): 0.0077 / 0.0052 Elapsed 0.06 s\n",
      "[0/50][154/472] Loss_D: 0.5822 Loss_G: 5.1909 D(x): 0.6595 D(G(z)): 0.0051 / 0.0068 Elapsed 0.06 s\n",
      "[0/50][155/472] Loss_D: 0.5571 Loss_G: 4.5069 D(x): 0.7039 D(G(z)): 0.0093 / 0.0157 Elapsed 0.06 s\n",
      "[0/50][156/472] Loss_D: 0.5868 Loss_G: 4.2656 D(x): 0.8945 D(G(z)): 0.0193 / 0.0198 Elapsed 0.06 s\n",
      "[0/50][157/472] Loss_D: 0.5636 Loss_G: 4.0663 D(x): 0.8438 D(G(z)): 0.0332 / 0.0231 Elapsed 0.06 s\n",
      "[0/50][158/472] Loss_D: 0.5750 Loss_G: 4.8146 D(x): 0.8855 D(G(z)): 0.0264 / 0.0117 Elapsed 0.06 s\n",
      "[0/50][159/472] Loss_D: 0.5303 Loss_G: 5.3828 D(x): 0.7891 D(G(z)): 0.0146 / 0.0065 Elapsed 0.06 s\n",
      "[0/50][160/472] Loss_D: 0.5535 Loss_G: 5.5745 D(x): 0.7136 D(G(z)): 0.0073 / 0.0050 Elapsed 0.06 s\n",
      "[0/50][161/472] Loss_D: 0.5495 Loss_G: 5.3514 D(x): 0.7002 D(G(z)): 0.0060 / 0.0064 Elapsed 0.06 s\n",
      "[0/50][162/472] Loss_D: 0.5267 Loss_G: 4.9175 D(x): 0.7691 D(G(z)): 0.0089 / 0.0100 Elapsed 0.06 s\n",
      "[0/50][163/472] Loss_D: 0.5375 Loss_G: 4.2038 D(x): 0.7940 D(G(z)): 0.0198 / 0.0204 Elapsed 0.06 s\n",
      "[0/50][164/472] Loss_D: 0.5372 Loss_G: 4.5036 D(x): 0.7755 D(G(z)): 0.0152 / 0.0155 Elapsed 0.06 s\n",
      "[0/50][165/472] Loss_D: 0.5398 Loss_G: 4.3174 D(x): 0.8079 D(G(z)): 0.0205 / 0.0177 Elapsed 0.06 s\n",
      "[0/50][166/472] Loss_D: 0.5545 Loss_G: 4.1987 D(x): 0.7956 D(G(z)): 0.0286 / 0.0209 Elapsed 0.06 s\n",
      "[0/50][167/472] Loss_D: 0.5709 Loss_G: 4.3163 D(x): 0.8458 D(G(z)): 0.0344 / 0.0169 Elapsed 0.06 s\n",
      "[0/50][168/472] Loss_D: 0.5417 Loss_G: 5.4179 D(x): 0.7481 D(G(z)): 0.0098 / 0.0064 Elapsed 0.06 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/50][169/472] Loss_D: 0.5419 Loss_G: 5.0008 D(x): 0.7543 D(G(z)): 0.0128 / 0.0098 Elapsed 0.06 s\n",
      "[0/50][170/472] Loss_D: 0.6721 Loss_G: 4.1376 D(x): 0.5553 D(G(z)): 0.0087 / 0.0223 Elapsed 0.06 s\n",
      "[0/50][171/472] Loss_D: 0.5954 Loss_G: 3.6572 D(x): 0.9005 D(G(z)): 0.0261 / 0.0329 Elapsed 0.06 s\n",
      "[0/50][172/472] Loss_D: 0.6433 Loss_G: 3.5705 D(x): 0.9114 D(G(z)): 0.0613 / 0.0357 Elapsed 0.06 s\n",
      "[0/50][173/472] Loss_D: 0.5593 Loss_G: 4.9787 D(x): 0.8715 D(G(z)): 0.0237 / 0.0096 Elapsed 0.06 s\n",
      "[0/50][174/472] Loss_D: 0.5585 Loss_G: 4.5507 D(x): 0.7600 D(G(z)): 0.0330 / 0.0146 Elapsed 0.06 s\n",
      "[0/50][175/472] Loss_D: 0.5362 Loss_G: 5.7121 D(x): 0.8064 D(G(z)): 0.0101 / 0.0043 Elapsed 0.06 s\n",
      "[0/50][176/472] Loss_D: 0.6554 Loss_G: 5.4387 D(x): 0.5686 D(G(z)): 0.0049 / 0.0060 Elapsed 0.06 s\n",
      "[0/50][177/472] Loss_D: 0.5696 Loss_G: 4.4538 D(x): 0.6747 D(G(z)): 0.0081 / 0.0144 Elapsed 0.06 s\n",
      "[0/50][178/472] Loss_D: 0.5575 Loss_G: 3.7489 D(x): 0.8576 D(G(z)): 0.0284 / 0.0317 Elapsed 0.06 s\n",
      "[0/50][179/472] Loss_D: 0.5997 Loss_G: 4.4037 D(x): 0.9032 D(G(z)): 0.0291 / 0.0175 Elapsed 0.06 s\n",
      "[0/50][180/472] Loss_D: 0.5530 Loss_G: 5.2890 D(x): 0.8655 D(G(z)): 0.0146 / 0.0065 Elapsed 0.06 s\n",
      "[0/50][181/472] Loss_D: 0.5428 Loss_G: 5.3719 D(x): 0.7521 D(G(z)): 0.0125 / 0.0069 Elapsed 0.06 s\n",
      "[0/50][182/472] Loss_D: 0.5537 Loss_G: 5.6700 D(x): 0.7015 D(G(z)): 0.0049 / 0.0046 Elapsed 0.06 s\n",
      "[0/50][183/472] Loss_D: 0.5284 Loss_G: 5.3312 D(x): 0.7669 D(G(z)): 0.0065 / 0.0068 Elapsed 0.06 s\n",
      "[0/50][184/472] Loss_D: 0.5388 Loss_G: 4.7752 D(x): 0.7466 D(G(z)): 0.0095 / 0.0114 Elapsed 0.06 s\n",
      "[0/50][185/472] Loss_D: 0.5425 Loss_G: 4.0251 D(x): 0.7898 D(G(z)): 0.0223 / 0.0236 Elapsed 0.06 s\n",
      "[0/50][186/472] Loss_D: 0.5409 Loss_G: 4.2768 D(x): 0.8306 D(G(z)): 0.0221 / 0.0182 Elapsed 0.06 s\n",
      "[0/50][187/472] Loss_D: 0.5638 Loss_G: 4.7276 D(x): 0.8771 D(G(z)): 0.0219 / 0.0124 Elapsed 0.06 s\n",
      "[0/50][188/472] Loss_D: 0.5485 Loss_G: 5.4084 D(x): 0.7181 D(G(z)): 0.0079 / 0.0061 Elapsed 0.06 s\n",
      "[0/50][189/472] Loss_D: 0.5316 Loss_G: 4.9660 D(x): 0.8086 D(G(z)): 0.0144 / 0.0101 Elapsed 0.06 s\n",
      "[0/50][190/472] Loss_D: 0.5316 Loss_G: 5.1438 D(x): 0.8245 D(G(z)): 0.0115 / 0.0074 Elapsed 0.06 s\n",
      "[0/50][191/472] Loss_D: 0.5300 Loss_G: 5.2853 D(x): 0.8052 D(G(z)): 0.0103 / 0.0064 Elapsed 0.06 s\n",
      "[0/50][192/472] Loss_D: 0.5970 Loss_G: 5.0627 D(x): 0.6297 D(G(z)): 0.0069 / 0.0079 Elapsed 0.06 s\n",
      "[0/50][193/472] Loss_D: 0.5568 Loss_G: 4.4256 D(x): 0.7372 D(G(z)): 0.0117 / 0.0161 Elapsed 0.06 s\n",
      "[0/50][194/472] Loss_D: 0.5350 Loss_G: 4.0630 D(x): 0.8113 D(G(z)): 0.0197 / 0.0219 Elapsed 0.06 s\n",
      "[0/50][195/472] Loss_D: 0.5450 Loss_G: 4.2595 D(x): 0.8491 D(G(z)): 0.0187 / 0.0165 Elapsed 0.06 s\n",
      "[0/50][196/472] Loss_D: 0.5484 Loss_G: 4.4372 D(x): 0.8378 D(G(z)): 0.0235 / 0.0158 Elapsed 0.06 s\n",
      "[0/50][197/472] Loss_D: 0.5379 Loss_G: 4.8928 D(x): 0.8283 D(G(z)): 0.0161 / 0.0096 Elapsed 0.06 s\n",
      "[0/50][198/472] Loss_D: 0.5368 Loss_G: 5.4101 D(x): 0.7920 D(G(z)): 0.0105 / 0.0063 Elapsed 0.06 s\n",
      "[0/50][199/472] Loss_D: 0.5414 Loss_G: 5.2168 D(x): 0.7444 D(G(z)): 0.0096 / 0.0072 Elapsed 0.06 s\n",
      "[0/50][200/472] Loss_D: 0.5419 Loss_G: 5.3473 D(x): 0.7398 D(G(z)): 0.0059 / 0.0056 Elapsed 0.06 s\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "d_labelSmooth = 0.2\n",
    "\n",
    "train_both_networks(num_epochs, dataloader, netD_parallel,netG_parallel,d_labelSmooth, outputDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
