{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System properties and libs currently in use\n",
    "- We have developed using python 3.5.x, pytorch 0.2.1\n",
    "- No significant attention was given to backwards compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 3.5.4 |Continuum Analytics, Inc.| (default, Aug 14 2017, 13:41:13) [MSC v.1900 64 bit (AMD64)]\n",
      "__pyTorch VERSION: 0.2.1+a4fc05a\n",
      "__CUDA VERSION\n",
      "__CUDNN VERSION: None\n",
      "__Number CUDA Devices: 1\n",
      "__Devices\n",
      "Active CUDA Device: GPU 0\n"
     ]
    }
   ],
   "source": [
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION')\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "print('Active CUDA Device: GPU', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities\n",
    "- Saving images and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_images(netG, noise, outputDir,epoch):\n",
    "    '''\n",
    "    Generates a batch of images from the given 'noise'.\n",
    "    Saves 64 of the generated samples to 'outputDir' system path.\n",
    "    Inputs are the network (netG), a 'noise' input, system path to which images will be saved (outputDir) and current 'epoch'.\n",
    "    '''\n",
    "    noise = Variable(noise)\n",
    "    fake = netG(noise)\n",
    "    vutils.save_image(fake.data[0:64,:,:,:],'%s/fake_samples_epoch_%03d.png' % (outputDir, epoch), nrow=8)\n",
    "\n",
    "def save_models(netG, netD, outputDir, epoch):\n",
    "    '''\n",
    "    Saves model state dictionary for generator and discriminator networks.\n",
    "    Inputs are the networks (netG, netD), the system path in which to save(outputDir) and the current 'epoch'.\n",
    "    '''\n",
    "    torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (outputDir, epoch))\n",
    "    torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (outputDir, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using CUDA. If it is not what you want, manually set this as False!\n"
     ]
    }
   ],
   "source": [
    "cudnn.benchmark = True\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"You are using CUDA. If it is not what you want, manually set this as False!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Directory\n",
    "This is where images will be saved to.\n",
    "\n",
    "If directory does not exist, it is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS error: [WinError 183] Não é possível criar um arquivo já existente: 'outputdir_train_classifier'\n"
     ]
    }
   ],
   "source": [
    "outputDir = 'outputdir_train_classifier'\n",
    "\n",
    "try:\n",
    "    os.makedirs(outputDir)\n",
    "except OSError as err:\n",
    "    print(\"OS error: {0}\".format(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset definition and hyperparameter setting\n",
    "- Changing dataset name alters network architecture parameters\n",
    "- Currently supporting few datasets\n",
    "- Hyperparameters defined according to Radford et al. (2015)\n",
    "\n",
    "Valores típicos são\n",
    "\n",
    "nc = 3,\n",
    "ngpu = 1,\n",
    "nz = 100,\n",
    "ngf = 64,\n",
    "ndf = 64,\n",
    "n_extra_d = 0,\n",
    "n_extra_g = 1,\n",
    "imageSize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "chosen_dataset = 'MNIST'\n",
    "\n",
    "datasets = {\n",
    "    'MNIST': torchvision.datasets.MNIST,\n",
    "    'CIFAR10': torchvision.datasets.CIFAR10,\n",
    "    'ANIME': '/home/gabriel/Redes Neurais/Projeto_Final_GANS/Tutorial_2/dataset/min_anime-faces',\n",
    "}\n",
    "\n",
    "dataset = datasets[chosen_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "possible_parameters = {\n",
    "    'MNIST': {\n",
    "        'ndf': 64,\n",
    "        'ngf': 64,\n",
    "        'nz': 100,\n",
    "        'nc': 1,\n",
    "        'imageSize': 64,\n",
    "        'n_classes' : 10,\n",
    "        'ngpu': 1,\n",
    "    },\n",
    "    'CIFAR10': {\n",
    "        'ndf': 64,\n",
    "        'ngf': 64,\n",
    "        'nz': 100,\n",
    "        'nc': 3,\n",
    "        'imageSize' : 64,\n",
    "        'n_classes' : 10,\n",
    "        'ngpu' : 1,\n",
    "    },\n",
    "    'ANIME': {\n",
    "        'nc' : 3,\n",
    "        'ngpu' : 1,\n",
    "        'nz' : 100,\n",
    "        'ngf' : 64,\n",
    "        'ndf' : 64,\n",
    "        'imageSize' : 64,\n",
    "        'n_classes' : 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngf = possible_parameters[chosen_dataset]['ngf']\n",
    "ndf = possible_parameters[chosen_dataset]['ndf']\n",
    "nz = possible_parameters[chosen_dataset]['nz']\n",
    "nc = possible_parameters[chosen_dataset]['nc']\n",
    "imageSize = possible_parameters[chosen_dataset]['imageSize']\n",
    "n_classes = possible_parameters[chosen_dataset]['n_classes']\n",
    "ngpu = possible_parameters[chosen_dataset]['ngpu']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader length: 469\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'ANIME':\n",
    "    dataset = torchvision.datasets.ImageFolder(\n",
    "        root='/home/gabriel/Redes Neurais/Projeto_Final_GANS/Tutorial_2/dataset/min_anime-faces',\n",
    "        transform=transforms.Compose([\n",
    "                transforms.Scale((imageSize, imageSize)),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "    )\n",
    "else:\n",
    "    transform = transforms.Compose([\n",
    "                    transforms.Scale((imageSize, imageSize)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)), # bring images to (-1,1)\n",
    "                ]) \n",
    "    dataset_done = dataset('./datasets', train=True, download=True, transform=transform)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset_done, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "print('Dataloader length:', len(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição dos modelos\n",
    "- Model is a DCGAN\n",
    "- Images are sized (nc, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netD_DCGAN(nn.Module):\n",
    "    def __init__(self, ngpu, nz, nc, ndf, n_classes):\n",
    "        super(_netD_DCGAN, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.conv1 = nn.Conv2d(in_channels = nc, out_channels = ndf, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(in_channels = ndf, out_channels = ndf*2, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch2 = nn.BatchNorm2d(ndf * 2)\n",
    "        self.conv3 = nn.Conv2d(in_channels = ndf*2, out_channels = ndf*4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch3 = nn.BatchNorm2d(ndf * 4)\n",
    "        self.conv4 = nn.Conv2d(in_channels = ndf*4, out_channels = ndf*8, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch4 = nn.BatchNorm2d(ndf * 8)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(in_channels=ndf*8, out_channels=n_classes+1,kernel_size=4,stride=1,padding=0,bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2, inplace=True)\n",
    "        x = F.leaky_relu(self.batch2(self.conv2(x)), 0.2, inplace=True)\n",
    "        x = F.leaky_relu(self.batch3(self.conv3(x)), 0.2, inplace=True)\n",
    "        x = F.leaky_relu(self.batch4(self.conv4(x)), 0.2, inplace=True)\n",
    "        \n",
    "        x = self.final_conv(x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netG_DCGAN(nn.Module):\n",
    "    def __init__(self, ngpu, nz, nc , ngf):\n",
    "        super(_netG_DCGAN, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.convt1 = nn.ConvTranspose2d(in_channels=nz, out_channels=ngf * 8, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        self.batch1 = nn.BatchNorm2d(ngf*8)\n",
    "        self.convt2 = nn.ConvTranspose2d(in_channels=ngf * 8, out_channels=ngf * 4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch2 = nn.BatchNorm2d(ngf*4)\n",
    "        self.convt3 = nn.ConvTranspose2d(in_channels=ngf * 4, out_channels=ngf * 2, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch3 = nn.BatchNorm2d(ngf*2)\n",
    "        self.convt4 = nn.ConvTranspose2d(in_channels=ngf*2, out_channels=ngf, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch4 = nn.BatchNorm2d(ngf)\n",
    "        \n",
    "        self.final_convt = nn.ConvTranspose2d(in_channels=ngf, out_channels=nc, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.batch1(self.convt1(x)), 0.2, inplace=True)\n",
    "        x = F.leaky_relu(self.batch2(self.convt2(x)), 0.2, inplace=True)\n",
    "        x = F.leaky_relu(self.batch3(self.convt3(x)), 0.2, inplace=True)\n",
    "        x = F.leaky_relu(self.batch4(self.convt4(x)), 0.2, inplace=True)\n",
    "        \n",
    "        x = self.final_convt(x)\n",
    "        x = F.tanh(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "netG = _netG_DCGAN(ngpu, nz, nc, ngf)\n",
    "netD = _netD_DCGAN(ngpu, nz, nc, ndf, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializador de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netG_DCGAN (\n",
      "  (convt1): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  (batch1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (convt2): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (convt3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (convt4): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (final_convt): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      ") \n",
      " _netD_DCGAN (\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv4): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (final_conv): Conv2d(512, 11, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "print(netG, '\\n', netD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses\n",
    "- Binary Cross-Entropy is used to differentiate real and fake images\n",
    "- Class loss should be Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sizes of the tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input images size: torch.Size([128, 3, 64, 64])\n",
      "Code size: torch.Size([128, 100, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "input = torch.FloatTensor(batch_size, 3, imageSize, imageSize)\n",
    "print('Input images size:', input.size())\n",
    "noise = torch.FloatTensor(batch_size, nz, 1, 1)\n",
    "fixed_noise = torch.FloatTensor(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "print('Code size:', noise.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label size: torch.Size([128, 10])\n"
     ]
    }
   ],
   "source": [
    "label = torch.LongTensor(batch_size,n_classes)\n",
    "print('Label size:', label.size())\n",
    "fake_label = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    netD.cuda()\n",
    "    netG.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    input,label = input.cuda(), label.cuda()\n",
    "    noise, fixed_noise = noise.cuda(), fixed_noise.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Parameters\n",
    "- Following the lead of Radford et al., 2015:\n",
    "\n",
    "    <b>\n",
    "    1. beta1 = 0.5\n",
    "    2. beta2 = 0.999\n",
    "    3. lr = 0.0002\n",
    "    </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta1, beta2 = 0.5, 0.999\n",
    "lr = 2.0e-4\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = lr, betas = (beta1, beta2))\n",
    "#optimizerD = optim.Adam(netD_parallel.parameters(), lr = lr, betas = (beta1, beta2))\n",
    "\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = lr, betas = (beta1, beta2))\n",
    "#optimizerG = optim.Adam(netG_parallel.parameters(), lr = lr, betas = (beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator output size: torch.Size([64, 1, 64, 64])\n",
      "Discriminator output size: torch.Size([64, 11, 1, 1])\n",
      "0.2109224796295166\n"
     ]
    }
   ],
   "source": [
    "test_input_G = torch.randn(64, 100, 1, 1)\n",
    "test_input_G = Variable(test_input_G)\n",
    "test_output_G = netG(test_input_G.cuda())\n",
    "print('Generator output size:', test_output_G.size())\n",
    "then = time.time()\n",
    "test_output_D = netD(test_output_G)\n",
    "now = time.time()\n",
    "print('Discriminator output size:', test_output_D.size())\n",
    "print(now-then)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Treinamento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_gan(num_epochs, dataloader, netD, netG, outputDir,\n",
    "              real_labelSmooth=0, epoch_interval=100, D_steps=1, G_steps=1):\n",
    "    # This validation is subjective. WGAN-GP uses 100 steps on the critic.\n",
    "    assert D_steps < 5, \"Keep it low, D_steps is too high.\"\n",
    "    assert G_steps < 3, \"Keep it low, G_steps is too high.\"\n",
    "    #assert batch_size % D_steps == 0, \"Use batch_size multiple of D_steps.\"\n",
    "    \n",
    "    print('Lets train!')\n",
    "    for epoch in range(num_epochs):\n",
    "        start_iter = time.time()  \n",
    "        D_x = 0\n",
    "        D_G_z1 = 0\n",
    "        D_G_z2 = 0\n",
    "        errD_acum = 0\n",
    "        errG_acum = 0\n",
    "\n",
    "        for batch, data in enumerate(dataloader, 0):\n",
    "            if (epoch == 0 and batch == 0):\n",
    "                    vutils.save_image(data[0][0:64,:,:,:], '%s/real_samples.png' % outputDir, nrow=8)\n",
    "            for step in range(D_steps):\n",
    "                #############################################################\n",
    "                # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "                # 1A - Train the detective network in the Real Dataset\n",
    "                #############################################################\n",
    "                \n",
    "                netD.zero_grad()\n",
    "                start = step*(int(data[0].size()[0]/D_steps))\n",
    "                end = (step+1)*int(data[0].size()[0]/D_steps)\n",
    "                \n",
    "                real_cpu = data[0][start:end]\n",
    "                real_cpu = real_cpu.cuda()\n",
    "                batch_size = real_cpu.size(0)\n",
    "                if np.random.random_sample() > real_labelSmooth:\n",
    "                    target = data[1][start:end].long().cuda()\n",
    "                else:\n",
    "                    target = torch.from_numpy(np.random.randint(0, n_classes, batch_size)).type(torch.LongTensor).cuda()\n",
    "                \n",
    "                input, label = Variable(real_cpu), Variable(target)\n",
    "\n",
    "                output = netD(input)\n",
    "                errD_real = criterion(output.squeeze(),label)\n",
    "                errD_real.backward()\n",
    "                \n",
    "                D_x += output.data.mean()\n",
    "                \n",
    "                #######################################################\n",
    "                # 1B - Train the detective network in the False Dataset\n",
    "                #######################################################\n",
    "                \n",
    "                noise = Variable(torch.FloatTensor(batch_size, nz, 1, 1).normal_(0,1).cuda())\n",
    "                fake = netG(noise)\n",
    "                label = Variable(torch.ones(batch_size).long().fill_(fake_label).cuda())\n",
    "                output = netD(fake.detach()) # \".detach()\" to avoid backprop through G\n",
    "                errD_fake = criterion(output.squeeze(), label)\n",
    "                errD_fake.backward() # gradients for fake and real data will be accumulated\n",
    "                \n",
    "                D_G_z1 += output.data.mean()\n",
    "                errD_acum += errD_real.data[0] + errD_fake.data[0]\n",
    "                optimizerD.step() # .step() can be called once the gradients are computed\n",
    "\n",
    "            for step in range(G_steps):\n",
    "                ####################################################################################\n",
    "                # (2) Update G network: maximize log(D(G(z)))\n",
    "                # Train the faker with the output from the Detective (but don't train the Detective)\n",
    "                ####################################################################################\n",
    "                \n",
    "                netG.zero_grad()\n",
    "                label = Variable(torch.from_numpy(np.random.randint(0, n_classes, batch_size)).type(torch.LongTensor).cuda())\n",
    "                output = netD(fake)\n",
    "                errG = criterion(output.squeeze(), label)\n",
    "                errG.backward()\n",
    "                \n",
    "                D_G_z2 += output.data.mean()\n",
    "                errG_acum += errG.data[0]\n",
    "                optimizerG.step()\n",
    "            del input, noise, label, output\n",
    "\n",
    "        print('epoch = ',epoch)\n",
    "\n",
    "        end_iter = time.time()        \n",
    "\n",
    "        print('[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f Elapsed %.2f s'\n",
    "              % (epoch, num_epochs, errD_acum/D_steps, errG_acum/G_steps, D_x, D_G_z1, D_G_z2, end_iter-start_iter))\n",
    "\n",
    "        #Save a grid with the pictures from the dataset, up until 64\n",
    "        save_images(netG = netG, noise = fixed_noise, outputDir = outputDir, epoch = epoch)\n",
    "\n",
    "        if epoch % epoch_interval == 0:\n",
    "            # do checkpointing\n",
    "            save_models(netG = netG, netD = netD, outputDir = outputDir, epoch = epoch)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets train!\n",
      "epoch =  0\n",
      "[0/25] Loss_D: 729.8422 Loss_G: 3495.1323 D(x): -262.7891 D(G(z)): -307.1244 / -314.4555 Elapsed 508.72 s\n",
      "epoch =  1\n",
      "[1/25] Loss_D: 698.5504 Loss_G: 2774.3043 D(x): -195.6825 D(G(z)): -222.3928 / -223.8340 Elapsed 510.43 s\n",
      "epoch =  2\n",
      "[2/25] Loss_D: 777.0365 Loss_G: 2660.4712 D(x): -129.4316 D(G(z)): -178.4522 / -181.6207 Elapsed 515.80 s\n",
      "epoch =  3\n",
      "[3/25] Loss_D: 714.8009 Loss_G: 2854.6080 D(x): -125.4248 D(G(z)): -201.5618 / -208.3477 Elapsed 558.63 s\n",
      "epoch =  4\n",
      "[4/25] Loss_D: 615.1129 Loss_G: 3171.6410 D(x): -115.2403 D(G(z)): -222.8791 / -239.5917 Elapsed 544.50 s\n",
      "epoch =  5\n",
      "[5/25] Loss_D: 670.1696 Loss_G: 3307.7916 D(x): -71.3631 D(G(z)): -231.5065 / -250.7709 Elapsed 512.69 s\n",
      "epoch =  6\n",
      "[6/25] Loss_D: 632.2180 Loss_G: 3211.2435 D(x): -102.1951 D(G(z)): -258.1023 / -277.7794 Elapsed 537.03 s\n",
      "epoch =  7\n",
      "[7/25] Loss_D: 676.8400 Loss_G: 3244.1635 D(x): -93.8193 D(G(z)): -294.3224 / -310.1835 Elapsed 877.14 s\n",
      "epoch =  8\n",
      "[8/25] Loss_D: 685.3045 Loss_G: 3122.8718 D(x): -110.9023 D(G(z)): -288.0435 / -307.6367 Elapsed 980.27 s\n",
      "epoch =  9\n",
      "[9/25] Loss_D: 605.0088 Loss_G: 3407.2556 D(x): -114.2554 D(G(z)): -374.4847 / -388.3706 Elapsed 816.16 s\n",
      "epoch =  10\n",
      "[10/25] Loss_D: 658.6714 Loss_G: 3204.9371 D(x): -128.1202 D(G(z)): -362.0074 / -377.0193 Elapsed 849.41 s\n",
      "epoch =  11\n",
      "[11/25] Loss_D: 667.5278 Loss_G: 3141.8982 D(x): -140.7251 D(G(z)): -347.9977 / -366.3696 Elapsed 783.07 s\n",
      "epoch =  12\n",
      "[12/25] Loss_D: 483.8017 Loss_G: 3732.0518 D(x): -74.7010 D(G(z)): -435.2601 / -444.3314 Elapsed 793.46 s\n",
      "epoch =  13\n",
      "[13/25] Loss_D: 610.7238 Loss_G: 3355.1571 D(x): -93.2408 D(G(z)): -367.4810 / -387.0614 Elapsed 783.96 s\n",
      "epoch =  14\n",
      "[14/25] Loss_D: 742.8186 Loss_G: 3156.1534 D(x): -74.3273 D(G(z)): -319.9341 / -333.6149 Elapsed 771.78 s\n",
      "epoch =  15\n",
      "[15/25] Loss_D: 561.0719 Loss_G: 3542.4312 D(x): -46.5144 D(G(z)): -393.6427 / -406.2905 Elapsed 776.52 s\n",
      "epoch =  16\n",
      "[16/25] Loss_D: 665.3258 Loss_G: 3196.7642 D(x): -82.9288 D(G(z)): -313.2104 / -330.0164 Elapsed 775.61 s\n",
      "epoch =  17\n",
      "[17/25] Loss_D: 676.0839 Loss_G: 3244.8131 D(x): -45.3652 D(G(z)): -327.9148 / -343.4374 Elapsed 771.61 s\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Traceback (most recent call last):\n  File \"c:\\users\\apsvi\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 40, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"c:\\users\\apsvi\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 109, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"c:\\users\\apsvi\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 109, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"c:\\users\\apsvi\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 89, in default_collate\n    storage = batch[0].storage()._new_shared(numel)\n  File \"c:\\users\\apsvi\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\torch\\storage.py\", line 111, in _new_shared\n    return cls._new_using_filename(size)\nRuntimeError: Couldn't open shared file mapping: <torch_12576_3367665362>, error code: <1455> at D:\\Projects\\pytorch\\torch\\lib\\TH\\THAllocator.c:157\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-647a27881836>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mreal_labelSmooth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain_gan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetD\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnetG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputDir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_labelSmooth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-7b75c13edce9>\u001b[0m in \u001b[0;36mtrain_gan\u001b[1;34m(num_epochs, dataloader, netD, netG, outputDir, real_labelSmooth, epoch_interval, D_steps, G_steps)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0merrG_acum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                     \u001b[0mvutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'%s/real_samples.png'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0moutputDir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\apsvi\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[0mnext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[1;31m# Python 2 compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\apsvi\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Traceback (most recent call last):\n  File \"c:\\users\\apsvi\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 40, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"c:\\users\\apsvi\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 109, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"c:\\users\\apsvi\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 109, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"c:\\users\\apsvi\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 89, in default_collate\n    storage = batch[0].storage()._new_shared(numel)\n  File \"c:\\users\\apsvi\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\torch\\storage.py\", line 111, in _new_shared\n    return cls._new_using_filename(size)\nRuntimeError: Couldn't open shared file mapping: <torch_12576_3367665362>, error code: <1455> at D:\\Projects\\pytorch\\torch\\lib\\TH\\THAllocator.c:157\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "real_labelSmooth = 0.2\n",
    "\n",
    "train_gan(num_epochs, dataloader, netD,netG, outputDir, real_labelSmooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
