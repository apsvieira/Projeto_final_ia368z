{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System properties and libs currently in use\n",
    "- We have developed using python 3.5.x, pytorch 0.2.1\n",
    "- No significant attention was given to backwards compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 3.5.4 |Continuum Analytics, Inc.| (default, Aug 14 2017, 13:41:13) [MSC v.1900 64 bit (AMD64)]\n",
      "__pyTorch VERSION: 0.2.1+a4fc05a\n",
      "__CUDA VERSION\n",
      "__CUDNN VERSION: None\n",
      "__Number CUDA Devices: 1\n",
      "__Devices\n",
      "Active CUDA Device: GPU 0\n"
     ]
    }
   ],
   "source": [
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION')\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "print('Active CUDA Device: GPU', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities\n",
    "- Saving images and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_images(netG, fixed_noise, outputDir,epoch):\n",
    "    '''\n",
    "    Generates a batch of images from the given 'noise'.\n",
    "    Saves 64 of the generated samples to 'outputDir' system path.\n",
    "    Inputs are the network (netG), a 'noise' input, system path to which images will be saved (outputDir) and current 'epoch'.\n",
    "    '''\n",
    "    noise = Variable(fixed_noise)\n",
    "    fake = netG(noise)\n",
    "    vutils.save_image(fake.data[0:64,:,:,:],'%s/fake_samples_epoch_%03d.png' % (outputDir, epoch), nrow=8)\n",
    "\n",
    "def save_models(netG, netD, outputDir, epoch):\n",
    "    '''\n",
    "    Saves model state dictionary for generator and discriminator networks.\n",
    "    Inputs are the networks (netG, netD), the system path in which to save(outputDir) and the current 'epoch'.\n",
    "    '''\n",
    "    torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (outputDir, epoch))\n",
    "    torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (outputDir, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using CUDA. If it is not what you want, manually set this as False!\n"
     ]
    }
   ],
   "source": [
    "cudnn.benchmark = True\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"You are using CUDA. If it is not what you want, manually set this as False!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Directory\n",
    "This is where images will be saved to.\n",
    "\n",
    "If directory does not exist, it is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS error: [WinError 183] Não é possível criar um arquivo já existente: 'outputdir_train_classifier_lotufo'\n"
     ]
    }
   ],
   "source": [
    "outputDir = 'outputdir_train_classifier_lotufo'\n",
    "\n",
    "try:\n",
    "    os.makedirs(outputDir)\n",
    "except OSError as err:\n",
    "    print(\"OS error: {0}\".format(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset definition and hyperparameter setting\n",
    "- Changing dataset name alters network architecture parameters\n",
    "- Currently supporting few datasets\n",
    "- Hyperparameters defined according to Radford et al. (2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "chosen_dataset = 'MNIST'\n",
    "\n",
    "datasets = {\n",
    "    'MNIST': torchvision.datasets.MNIST,\n",
    "    'CIFAR10': torchvision.datasets.CIFAR10,\n",
    "    'ANIME': '/home/gabriel/Redes Neurais/Projeto_Final_GANS/Tutorial_2/dataset/min_anime-faces',\n",
    "}\n",
    "\n",
    "dataset = datasets[chosen_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "possible_parameters = {\n",
    "    'MNIST': {\n",
    "        'ndf': 64,\n",
    "        'ngf': 64,\n",
    "        'nz': 100,\n",
    "        'nc': 1,\n",
    "        'imageSize': 64,\n",
    "        'n_classes' : 10,\n",
    "        'ngpu': 1,\n",
    "    },\n",
    "    'CIFAR10': {\n",
    "        'ndf': 64,\n",
    "        'ngf': 64,\n",
    "        'nz': 100,\n",
    "        'nc': 3,\n",
    "        'imageSize' : 64,\n",
    "        'n_classes' : 10,\n",
    "        'ngpu' : 1,\n",
    "    },\n",
    "    'ANIME': {\n",
    "        'nc' : 3,\n",
    "        'ngpu' : 1,\n",
    "        'nz' : 100,\n",
    "        'ngf' : 64,\n",
    "        'ndf' : 64,\n",
    "        'imageSize' : 64,\n",
    "        'n_classes' : 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngf = possible_parameters[chosen_dataset]['ngf']\n",
    "ndf = possible_parameters[chosen_dataset]['ndf']\n",
    "nz = possible_parameters[chosen_dataset]['nz']\n",
    "nc = possible_parameters[chosen_dataset]['nc']\n",
    "imageSize = possible_parameters[chosen_dataset]['imageSize']\n",
    "n_classes = possible_parameters[chosen_dataset]['n_classes']\n",
    "ngpu = possible_parameters[chosen_dataset]['ngpu']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader length: 469\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'ANIME':\n",
    "    dataset = torchvision.datasets.ImageFolder(\n",
    "        root='/home/gabriel/Redes Neurais/Projeto_Final_GANS/Tutorial_2/dataset/min_anime-faces',\n",
    "        transform=transforms.Compose([\n",
    "                transforms.Scale((imageSize, imageSize)),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "    )\n",
    "else:\n",
    "    transform = transforms.Compose([\n",
    "                    transforms.Scale((imageSize, imageSize)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)), # bring images to (-1,1)\n",
    "                ]) \n",
    "    dataset_done = dataset('./datasets', train=True, download=True, transform=transform)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset_done, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "print('Dataloader length:', len(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição dos modelos\n",
    "- Model is a DCGAN\n",
    "- Images are sized (nc, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netD_DCGAN(nn.Module):\n",
    "    def __init__(self, ngpu, nz, nc, ndf, n_classes):\n",
    "        super(_netD_DCGAN, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.conv1 = nn.Conv2d(in_channels = nc, out_channels = ndf, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(in_channels = ndf, out_channels = ndf*2, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch2 = nn.BatchNorm2d(ndf * 2)\n",
    "        self.conv3 = nn.Conv2d(in_channels = ndf*2, out_channels = ndf*4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch3 = nn.BatchNorm2d(ndf * 4)\n",
    "        self.conv4 = nn.Conv2d(in_channels = ndf*4, out_channels = ndf*8, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch4 = nn.BatchNorm2d(ndf * 8)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(in_channels=ndf*8, out_channels=n_classes+1,kernel_size=4,stride=1,padding=0,bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2, inplace=True)\n",
    "        x = F.leaky_relu(self.batch2(self.conv2(x)), 0.2, inplace=True)\n",
    "        x = F.leaky_relu(self.batch3(self.conv3(x)), 0.2, inplace=True)\n",
    "        x = F.leaky_relu(self.batch4(self.conv4(x)), 0.2, inplace=True)\n",
    "        \n",
    "        x = self.final_conv(x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netG_DCGAN(nn.Module):\n",
    "    def __init__(self, ngpu, nz, nc , ngf):\n",
    "        super(_netG_DCGAN, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.convt1 = nn.ConvTranspose2d(in_channels=nz, out_channels=ngf * 8, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        self.batch1 = nn.BatchNorm2d(ngf*8)\n",
    "        self.convt2 = nn.ConvTranspose2d(in_channels=ngf * 8, out_channels=ngf * 4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch2 = nn.BatchNorm2d(ngf*4)\n",
    "        self.convt3 = nn.ConvTranspose2d(in_channels=ngf * 4, out_channels=ngf * 2, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch3 = nn.BatchNorm2d(ngf*2)\n",
    "        self.convt4 = nn.ConvTranspose2d(in_channels=ngf*2, out_channels=ngf, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch4 = nn.BatchNorm2d(ngf)\n",
    "        \n",
    "        self.final_convt = nn.ConvTranspose2d(in_channels=ngf, out_channels=nc, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.batch1(self.convt1(x)), 0.2, inplace=True)\n",
    "        x = F.leaky_relu(self.batch2(self.convt2(x)), 0.2, inplace=True)\n",
    "        x = F.leaky_relu(self.batch3(self.convt3(x)), 0.2, inplace=True)\n",
    "        x = F.leaky_relu(self.batch4(self.convt4(x)), 0.2, inplace=True)\n",
    "        \n",
    "        x = self.final_convt(x)\n",
    "        x = F.tanh(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "netG = _netG_DCGAN(ngpu, nz, nc, ngf)\n",
    "netD = _netD_DCGAN(ngpu, nz, nc, ndf, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializador de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netG_DCGAN (\n",
      "  (convt1): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  (batch1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (convt2): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (convt3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (convt4): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (final_convt): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      ") \n",
      " _netD_DCGAN (\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv4): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (final_conv): Conv2d(512, 11, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "print(netG, '\\n', netD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses\n",
    "- Binary Cross-Entropy is used to differentiate real and fake images\n",
    "- Class loss should be Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sizes of the tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input images size: torch.Size([128, 3, 64, 64])\n",
      "Code size: torch.Size([128, 100, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "input = torch.FloatTensor(batch_size, 3, imageSize, imageSize)\n",
    "print('Input images size:', input.size())\n",
    "noise = torch.FloatTensor(batch_size, nz, 1, 1)\n",
    "fixed_noise = torch.FloatTensor(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "print('Code size:', noise.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label size: torch.Size([128, 10])\n"
     ]
    }
   ],
   "source": [
    "label = torch.LongTensor(batch_size,n_classes)\n",
    "print('Label size:', label.size())\n",
    "fake_label = 10\n",
    "real_label = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    netD.cuda()\n",
    "    netG.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    input,label = input.cuda(), label.cuda()\n",
    "    noise, fixed_noise = noise.cuda(), fixed_noise.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Parameters\n",
    "- Following the lead of Radford et al., 2015:\n",
    "\n",
    "    <b>\n",
    "    1. beta1 = 0.5\n",
    "    2. beta2 = 0.999\n",
    "    3. lr = 0.0002\n",
    "    </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta1, beta2 = 0.5, 0.999\n",
    "lr = 2.0e-4\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = lr, betas = (beta1, beta2))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = lr, betas = (beta1, beta2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Treinamento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_gan(num_epochs, dataloader, netD, netG, outputDir,\n",
    "              real_labelSmooth=0, epoch_interval=100, D_steps=1, G_steps=1):\n",
    "    \n",
    "    # This validation is subjective. WGAN-GP uses 100 steps on the critic (netD).\n",
    "    assert D_steps < 5, \"Keep it low, D_steps is too high.\"\n",
    "    assert G_steps < 3, \"Keep it low, G_steps is too high.\"\n",
    "    #assert batch_size % D_steps == 0, \"Use batch_size multiple of D_steps.\"\n",
    "    real_label = 1\n",
    "    print('Lets train!')\n",
    "    for epoch in range(num_epochs):\n",
    "        start_iter = time.time()  \n",
    "        D_x = 0\n",
    "        D_G_z1 = 0\n",
    "        D_G_z2 = 0\n",
    "        errD_acum = 0\n",
    "        errG_acum = 0\n",
    "\n",
    "        for batch, data in enumerate(dataloader, 0):\n",
    "            if (epoch == 0 and batch == 0):\n",
    "                    vutils.save_image(data[0][0:64,:,:,:], '%s/real_samples.png' % outputDir, nrow=8)\n",
    "            real_labelSmooth = np.minimum(real_labelSmooth * (1 - 0.05*epoch), 0)\n",
    "            for step in range(D_steps):\n",
    "                #############################################################\n",
    "                # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "                # 1A - Train the detective network in the Real Dataset\n",
    "                #############################################################\n",
    "                netD.zero_grad()\n",
    "                start = step*(int(data[0].size()[0]/D_steps))\n",
    "                end = (step+1)*int(data[0].size()[0]/D_steps)\n",
    "                \n",
    "                real_cpu = data[0][start:end]\n",
    "                real_cpu = real_cpu.cuda()\n",
    "                batch_size = real_cpu.size(0)\n",
    "                if np.random.random_sample() > real_labelSmooth:\n",
    "                    target = data[1][start:end].long().cuda()\n",
    "                else:\n",
    "                     target = torch.from_numpy(np.random.randint(0, n_classes, batch_size)).type(torch.LongTensor).cuda()\n",
    "                \n",
    "                input, label = Variable(real_cpu), Variable(target)\n",
    "\n",
    "                output = netD(input)\n",
    "                errD_real = criterion(output.squeeze(),label)\n",
    "                errD_real.backward()\n",
    "                \n",
    "                D_x += output.data.mean()\n",
    "                \n",
    "                #######################################################\n",
    "                # 1B - Train the detective network in the False Dataset\n",
    "                #######################################################\n",
    "                \n",
    "                noise = Variable(torch.FloatTensor(batch_size, nz, 1, 1).normal_(0,1).cuda())\n",
    "                fake = netG(noise)\n",
    "                label = Variable(torch.ones(batch_size).long().fill_(fake_label).cuda())\n",
    "                output = netD(fake.detach()) # \".detach()\" to avoid backprop through G\n",
    "                errD_fake = criterion(output.squeeze(), label)\n",
    "                errD_fake.backward() # gradients for fake and real data will be accumulated\n",
    "                \n",
    "                D_G_z1 += output.data.mean()\n",
    "                errD_acum += errD_real.data[0] + errD_fake.data[0]\n",
    "                optimizerD.step()\n",
    "\n",
    "            for step in range(G_steps):\n",
    "                ####################################################################################\n",
    "                # (2) Update G network: maximize log(D(G(z)))\n",
    "                # Train the faker with the output from the Detective (but don't train the Detective)\n",
    "                ####################################################################################\n",
    "                \n",
    "                netG.zero_grad()\n",
    "                label = Variable(torch.from_numpy(np.random.randint(0, n_classes, batch_size)).type(torch.LongTensor).cuda())\n",
    "                output = netD(fake)\n",
    "                errG = criterion(output.squeeze(), label)\n",
    "                errG.backward()\n",
    "                \n",
    "                D_G_z2 += output.data.mean()\n",
    "                errG_acum += errG.data[0]\n",
    "                optimizerG.step()\n",
    "\n",
    "        print('epoch = ',epoch)\n",
    "\n",
    "        end_iter = time.time()        \n",
    "\n",
    "        print('[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f Elapsed %.2f s'\n",
    "              % (epoch, num_epochs, errD_acum/D_steps, errG_acum/G_steps, D_x, D_G_z1, D_G_z2, end_iter-start_iter))\n",
    "\n",
    "        #Save a grid with the pictures from the dataset, up until 64\n",
    "        save_images(netG = netG, fixed_noise=  fixed_noise, outputDir = outputDir, epoch = epoch)\n",
    "\n",
    "        if epoch % epoch_interval == 0:\n",
    "            # do checkpointing\n",
    "            save_models(netG = netG, netD = netD, outputDir = outputDir, epoch = epoch)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets train!\n",
      "epoch =  0\n",
      "[0/100] Loss_D: 216.8969 Loss_G: 5048.7693 D(x): -534.2349 D(G(z)): -563.4504 / -572.0943 Elapsed 456.28 s\n",
      "epoch =  1\n",
      "[1/100] Loss_D: 116.9204 Loss_G: 5252.2843 D(x): -563.9303 D(G(z)): -565.1684 / -566.3051 Elapsed 500.89 s\n",
      "epoch =  2\n",
      "[2/100] Loss_D: 131.1793 Loss_G: 5593.9937 D(x): -751.0082 D(G(z)): -764.4365 / -766.7596 Elapsed 523.86 s\n",
      "epoch =  3\n",
      "[3/100] Loss_D: 107.8414 Loss_G: 5823.4385 D(x): -867.2332 D(G(z)): -889.6692 / -891.7118 Elapsed 553.70 s\n",
      "epoch =  4\n",
      "[4/100] Loss_D: 87.2908 Loss_G: 6251.6002 D(x): -1012.9475 D(G(z)): -1057.5924 / -1060.7061 Elapsed 559.72 s\n",
      "epoch =  5\n",
      "[5/100] Loss_D: 114.6014 Loss_G: 6417.8736 D(x): -1191.3581 D(G(z)): -1257.2946 / -1262.2539 Elapsed 571.47 s\n",
      "epoch =  6\n",
      "[6/100] Loss_D: 82.6125 Loss_G: 6543.2967 D(x): -1404.9888 D(G(z)): -1465.4694 / -1469.9347 Elapsed 588.78 s\n",
      "epoch =  7\n",
      "[7/100] Loss_D: 138.8122 Loss_G: 6747.5682 D(x): -1303.9360 D(G(z)): -1372.2513 / -1378.0012 Elapsed 583.83 s\n",
      "epoch =  8\n",
      "[8/100] Loss_D: 59.9659 Loss_G: 7007.4935 D(x): -1348.7683 D(G(z)): -1428.3669 / -1435.4887 Elapsed 593.43 s\n",
      "epoch =  9\n",
      "[9/100] Loss_D: 97.9306 Loss_G: 6903.9584 D(x): -1445.7213 D(G(z)): -1525.9791 / -1532.7705 Elapsed 573.21 s\n",
      "epoch =  10\n",
      "[10/100] Loss_D: 71.9103 Loss_G: 7147.9606 D(x): -1556.1028 D(G(z)): -1644.9453 / -1652.4171 Elapsed 538.53 s\n",
      "epoch =  11\n",
      "[11/100] Loss_D: 71.0181 Loss_G: 7162.7217 D(x): -1700.9546 D(G(z)): -1769.5156 / -1776.1000 Elapsed 536.67 s\n",
      "epoch =  12\n",
      "[12/100] Loss_D: 73.7503 Loss_G: 7514.6530 D(x): -1740.5847 D(G(z)): -1858.8774 / -1868.5862 Elapsed 541.48 s\n",
      "epoch =  13\n",
      "[13/100] Loss_D: 81.0145 Loss_G: 7501.7665 D(x): -1698.2325 D(G(z)): -1873.8514 / -1881.5314 Elapsed 552.09 s\n",
      "epoch =  14\n",
      "[14/100] Loss_D: 87.3025 Loss_G: 7107.2293 D(x): -1808.5837 D(G(z)): -1847.7225 / -1854.2704 Elapsed 547.66 s\n",
      "epoch =  15\n",
      "[15/100] Loss_D: 61.2673 Loss_G: 7546.9801 D(x): -1915.5940 D(G(z)): -2022.1829 / -2029.6550 Elapsed 543.45 s\n",
      "epoch =  16\n",
      "[16/100] Loss_D: 77.1609 Loss_G: 7662.3131 D(x): -1912.5841 D(G(z)): -2063.2421 / -2072.0319 Elapsed 553.89 s\n",
      "epoch =  17\n",
      "[17/100] Loss_D: 31.6899 Loss_G: 7901.4801 D(x): -1971.1312 D(G(z)): -2156.1854 / -2166.2289 Elapsed 539.92 s\n",
      "epoch =  18\n",
      "[18/100] Loss_D: 90.8321 Loss_G: 7672.3900 D(x): -1937.7001 D(G(z)): -2036.9756 / -2048.2164 Elapsed 635.16 s\n",
      "epoch =  19\n",
      "[19/100] Loss_D: 48.3722 Loss_G: 7983.4114 D(x): -2054.7664 D(G(z)): -2212.0468 / -2221.4997 Elapsed 640.63 s\n",
      "epoch =  20\n",
      "[20/100] Loss_D: 71.5452 Loss_G: 8035.6750 D(x): -2065.7926 D(G(z)): -2246.1960 / -2255.0475 Elapsed 564.36 s\n",
      "epoch =  21\n",
      "[21/100] Loss_D: 91.8346 Loss_G: 7634.0037 D(x): -2179.6457 D(G(z)): -2245.0527 / -2253.0295 Elapsed 593.02 s\n",
      "epoch =  22\n",
      "[22/100] Loss_D: 40.0404 Loss_G: 7987.9325 D(x): -2261.8771 D(G(z)): -2398.6695 / -2408.2612 Elapsed 637.83 s\n",
      "epoch =  23\n",
      "[23/100] Loss_D: 72.0041 Loss_G: 8083.0175 D(x): -2278.9293 D(G(z)): -2405.2713 / -2413.4784 Elapsed 632.43 s\n",
      "epoch =  24\n",
      "[24/100] Loss_D: 32.9520 Loss_G: 8515.8680 D(x): -2296.7079 D(G(z)): -2516.5785 / -2524.1213 Elapsed 601.40 s\n",
      "epoch =  25\n",
      "[25/100] Loss_D: 86.1640 Loss_G: 8286.9854 D(x): -2302.3059 D(G(z)): -2464.0684 / -2474.0992 Elapsed 648.05 s\n",
      "epoch =  26\n",
      "[26/100] Loss_D: 27.2447 Loss_G: 8550.8792 D(x): -2369.9736 D(G(z)): -2588.4421 / -2599.5976 Elapsed 627.83 s\n",
      "epoch =  27\n",
      "[27/100] Loss_D: 68.4908 Loss_G: 8424.5516 D(x): -2456.2943 D(G(z)): -2631.2842 / -2642.2669 Elapsed 606.37 s\n",
      "epoch =  28\n",
      "[28/100] Loss_D: 48.1128 Loss_G: 8569.0846 D(x): -2491.3731 D(G(z)): -2675.7109 / -2689.1030 Elapsed 659.53 s\n",
      "epoch =  29\n",
      "[29/100] Loss_D: 29.9661 Loss_G: 9025.5980 D(x): -2519.2545 D(G(z)): -2823.8370 / -2832.8982 Elapsed 582.36 s\n",
      "epoch =  30\n",
      "[30/100] Loss_D: 58.3314 Loss_G: 8692.3096 D(x): -2576.8633 D(G(z)): -2737.6016 / -2750.6297 Elapsed 594.48 s\n",
      "epoch =  31\n",
      "[31/100] Loss_D: 32.2484 Loss_G: 9070.8812 D(x): -2628.5477 D(G(z)): -2896.9673 / -2907.6015 Elapsed 620.43 s\n",
      "epoch =  32\n",
      "[32/100] Loss_D: 39.7803 Loss_G: 9095.3686 D(x): -2610.4266 D(G(z)): -2902.4998 / -2915.1683 Elapsed 581.99 s\n",
      "epoch =  33\n",
      "[33/100] Loss_D: 30.7651 Loss_G: 9113.0148 D(x): -2589.4368 D(G(z)): -2902.6425 / -2915.8337 Elapsed 582.05 s\n",
      "epoch =  34\n",
      "[34/100] Loss_D: 64.2283 Loss_G: 8969.9159 D(x): -2494.6376 D(G(z)): -2819.4360 / -2826.6525 Elapsed 599.52 s\n",
      "epoch =  35\n",
      "[35/100] Loss_D: 40.0544 Loss_G: 8817.8156 D(x): -2612.1256 D(G(z)): -2793.2096 / -2807.8598 Elapsed 604.79 s\n",
      "epoch =  36\n",
      "[36/100] Loss_D: 33.4142 Loss_G: 9380.5929 D(x): -2611.8026 D(G(z)): -2973.5606 / -2984.7972 Elapsed 615.22 s\n",
      "epoch =  37\n",
      "[37/100] Loss_D: 31.7337 Loss_G: 9427.3376 D(x): -2636.5998 D(G(z)): -3040.6323 / -3051.5639 Elapsed 610.31 s\n",
      "epoch =  38\n",
      "[38/100] Loss_D: 51.8425 Loss_G: 9248.6470 D(x): -2689.1169 D(G(z)): -2986.7045 / -3001.6497 Elapsed 588.40 s\n",
      "epoch =  39\n",
      "[39/100] Loss_D: 52.2493 Loss_G: 8882.8149 D(x): -2817.5687 D(G(z)): -2953.7154 / -2969.1210 Elapsed 566.36 s\n",
      "epoch =  40\n",
      "[40/100] Loss_D: 34.3503 Loss_G: 9264.9145 D(x): -2775.8819 D(G(z)): -3077.6641 / -3087.8987 Elapsed 564.73 s\n",
      "epoch =  41\n",
      "[41/100] Loss_D: 18.7722 Loss_G: 9678.7440 D(x): -2727.1086 D(G(z)): -3147.6963 / -3156.6930 Elapsed 565.85 s\n",
      "epoch =  42\n",
      "[42/100] Loss_D: 60.0086 Loss_G: 9096.6468 D(x): -2846.4997 D(G(z)): -3021.7722 / -3036.8587 Elapsed 558.57 s\n",
      "epoch =  43\n",
      "[43/100] Loss_D: 29.4924 Loss_G: 9548.1417 D(x): -2797.0760 D(G(z)): -3158.9151 / -3167.7795 Elapsed 578.99 s\n",
      "epoch =  44\n",
      "[44/100] Loss_D: 17.6541 Loss_G: 9670.3837 D(x): -2731.0290 D(G(z)): -3191.9405 / -3203.4661 Elapsed 594.50 s\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "real_labelSmooth = 0.3\n",
    "\n",
    "train_gan(num_epochs, dataloader, netD,netG, outputDir, real_labelSmooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
