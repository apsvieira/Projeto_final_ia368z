{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch as tc\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "#get_ipython().magic('matplotlib inline')\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import sys,os\n",
    "import math\n",
    "import gc\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 3.5.3 |Anaconda 4.4.0 (64-bit)| (default, Mar  6 2017, 11:58:13) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "__pyTorch VERSION: 0.2.0_4\n",
      "__CUDA VERSION\n",
      "__CUDNN VERSION: 6021\n",
      "__Number CUDA Devices: 1\n",
      "__Devices\n",
      "Active CUDA Device: GPU 0\n"
     ]
    }
   ],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "import torch\n",
    "import sys\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION')\n",
    "#from subprocess import call\n",
    "#call([\"nvcc\", \"--version\"])\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "#call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
    "print('Active CUDA Device: GPU', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_images(netG, noise, outputDir,epoch):\n",
    "   # the first 64 samples from the mini-batch are saved.\n",
    "   fake,_ = netG(fixed_noise)\n",
    "   vutils.save_image(fake.data[0:64,:,:,:],\n",
    "\t\t   '%s/fake_samples_epoch_%03d.png' % (outputDir, epoch), nrow=8)\n",
    "\n",
    "def save_models(netG, netD, outputDir, epoch):\n",
    "   torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (outputDir, epoch))\n",
    "   torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (outputDir, epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netD_1(nn.Module):\n",
    "    def __init__(self, ngpu, nz, nc, ndf,  n_extra_layers_d, n_classes):\n",
    "        super(_netD_1, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.conv1 = nn.Conv2d(in_channels = nc, out_channels = ndf, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(in_channels = ndf, out_channels = ndf*2, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch2 = nn.BatchNorm2d(ndf * 2)\n",
    "        self.conv3 = nn.Conv2d(in_channels = ndf*2, out_channels = ndf*4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch3 = nn.BatchNorm2d(ndf * 4)\n",
    "        self.conv4 = nn.Conv2d(in_channels = ndf*4, out_channels = ndf*8, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch4 = nn.BatchNorm2d(ndf * 8)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(in_channels=ndf*8, out_channels=n_classes,kernel_size=2,stride=1,padding=0,bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print('0',x.size())\n",
    "\n",
    "        x = F.leaky_relu(self.conv1(x),0.02,inplace=True)\n",
    "        print('1',x.size())\n",
    "        x = F.leaky_relu(self.batch2(self.conv2(x)),0.02,inplace=True)\n",
    "        print('2',x.size())\n",
    "        x = F.leaky_relu(self.batch3(self.conv3(x)),0.02,inplace=True)\n",
    "        print('3',x.size())\n",
    "        x = F.leaky_relu(self.batch4(self.conv4(x)),0.02,inplace=True)   \n",
    "        print('antes final',x.size())\n",
    "        x = self.final_conv(x)\n",
    "        print('depois final',x.size())\n",
    "        \n",
    "        x = F.sigmoid(x)\n",
    "\n",
    "        return(x)\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using CUDA. If it is not what you want, manually set this as False!\n"
     ]
    }
   ],
   "source": [
    "cudnn.benchmark = True\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "#use_gpu = False\n",
    "if use_gpu:\n",
    "\tprint(\"You are using CUDA. If it is not what you want, manually set this as False!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nc = 3\n",
    "ngpu = 1\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "n_extra_d = 0\n",
    "n_extra_g = 1 # Aqui a jogada é que o gerador deve ser mais poderoso q o detetive\n",
    "imageSize = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should, in the future, be set in CLI\n",
    "chosen_dataset = 'CIFAR10'\n",
    "\n",
    "datasets = {\n",
    "    'MNIST': torchvision.datasets.MNIST,\n",
    "    'CIFAR10': torchvision.datasets.CIFAR10,\n",
    "    'ANIME': '/home/gabriel/Redes Neurais/Projeto_Final_GANS/Tutorial_2/dataset/min_anime-faces',\n",
    "    #'FashionMNIST': torchvision.datasets.FashionMNIST\n",
    "}\n",
    "\n",
    "dataset = datasets[chosen_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.datasets.cifar.CIFAR10'>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_parameters = {\n",
    "    'MNIST': {\n",
    "        'ndf': 64,\n",
    "        'ngf': 64,\n",
    "        'nz': 50,\n",
    "        'nc': 1,\n",
    "        'n_classes' : 10,\n",
    "    },\n",
    "    'CIFAR10': {\n",
    "        'ndf': 32,\n",
    "        'ngf': 32,\n",
    "        'nz': 100,\n",
    "        'nc': 3,\n",
    "        'n_extra_d' : 0,\n",
    "        'n_extra_g' : 1, # Aqui a jogada é que o gerador deve ser mais poderoso q o detetive\n",
    "        'imageSize' : 32,\n",
    "        'n_classes' : 10,\n",
    "        'ngpu' : 1,\t\t\n",
    "    },\n",
    "    'ANIME': {\n",
    "        'nc' : 3,\n",
    "        'ngpu' : 1,\n",
    "        'nz' : 100,\n",
    "        'ngf' : 64,\n",
    "        'ndf' : 64,\n",
    "        'n_extra_d' : 0,\n",
    "        'n_extra_g' : 0, # Aqui a jogada é que o gerador deve ser mais poderoso q o detetive\n",
    "        'imageSize' : 64,\n",
    "        'n_classes' : 1\n",
    "\n",
    "\n",
    "    }\n",
    "    #'FashionMNIST': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngf = possible_parameters[chosen_dataset]['ngf']\n",
    "ndf = possible_parameters[chosen_dataset]['ndf']\n",
    "nz = possible_parameters[chosen_dataset]['nz']\n",
    "nc = possible_parameters[chosen_dataset]['nc']\n",
    "imageSize = possible_parameters[chosen_dataset]['imageSize']\n",
    "n_classes = possible_parameters[chosen_dataset]['n_classes']\n",
    "ngpu = possible_parameters[chosen_dataset]['ngpu']\n",
    "n_extra_d = possible_parameters[chosen_dataset]['n_extra_d']\n",
    "n_extra_g = possible_parameters[chosen_dataset]['n_extra_g']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset_done = dataset('./datasets', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'ANIME':\n",
    "    dataset = dset.ImageFolder(\n",
    "    root='/home/gabriel/Redes Neurais/Projeto_Final_GANS/Tutorial_2/dataset/min_anime-faces',\n",
    "    transform=transforms.Compose([\n",
    "            transforms.Scale((imageSize, imageSize)),\n",
    "            # transforms.CenterCrop(opt.imageSize),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)), # bring images to (-1,1)\n",
    "        ])\n",
    "    )\n",
    "else:\n",
    "    transform=transforms.Compose([\n",
    "                transforms.Scale((imageSize, imageSize)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)), # bring images to (-1,1)\n",
    "            ]) \n",
    "    dataset_done = dataset('./datasets', train=True, download=True, transform=transform)\n",
    "    dataloader = tc.utils.data.DataLoader(dataset_done, batch_size=batch_size, shuffle=True, num_workers=1\n",
    "                                          )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netD_1(nn.Module):\n",
    "    def __init__(self, ngpu, nz, nc, ndf,  n_extra_layers_d, n_classes):\n",
    "        super(_netD_1, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.conv1 = nn.Conv2d(in_channels = nc, out_channels = ndf, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(in_channels = ndf, out_channels = ndf*2, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch2 = nn.BatchNorm2d(ndf * 2)\n",
    "        self.conv3 = nn.Conv2d(in_channels = ndf*2, out_channels = ndf*4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch3 = nn.BatchNorm2d(ndf * 4)\n",
    "        self.conv4 = nn.Conv2d(in_channels = ndf*4, out_channels = ndf*8, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch4 = nn.BatchNorm2d(ndf * 8)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(in_channels=ndf*8, out_channels=n_classes,kernel_size=2,stride=1,padding=0,bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print('0',x.size())\n",
    "\n",
    "        x = F.leaky_relu(self.conv1(x),0.02,inplace=True)\n",
    "        #print('1',x.size())\n",
    "        x = F.leaky_relu(self.batch2(self.conv2(x)),0.02,inplace=True)\n",
    "        #print('2',x.size())\n",
    "        x = F.leaky_relu(self.batch3(self.conv3(x)),0.02,inplace=True)\n",
    "        #print('3',x.size())\n",
    "        x = F.leaky_relu(self.batch4(self.conv4(x)),0.02,inplace=True)   \n",
    "        #print('antes final',x.size())\n",
    "        x = self.final_conv(x)\n",
    "        #print('depois final',x.size())\n",
    "        \n",
    "        x = F.sigmoid(x)\n",
    "\n",
    "        return(x)\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netG_1(nn.Module):\n",
    "    def __init__(self, ngpu, nz, nc , ngf, n_extra_layers_g):\n",
    "        super(_netG_1, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        self.convt1 = nn.ConvTranspose2d(in_channels = nz, out_channels = ngf * 8, kernel_size=4, stride=2, padding=0, bias=False)\n",
    "        self.batch1 = nn.BatchNorm2d(ngf*8)\n",
    "        self.convt2 = nn.ConvTranspose2d(in_channels = ngf * 8, out_channels = ngf * 4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch2 = nn.BatchNorm2d(ngf*4)\n",
    "        self.convt3 = nn.ConvTranspose2d(in_channels = ngf * 4, out_channels = ngf * 2, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch3 = nn.BatchNorm2d(ngf*2)\n",
    "        self.convt4 = nn.ConvTranspose2d(in_channels = ngf*2, out_channels = ngf, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch4 = nn.BatchNorm2d(ngf)\n",
    "        \n",
    "        self.final_conv = nn.ConvTranspose2d(in_channels=ngf, out_channels=nc,kernel_size=1,stride=1,padding=0,bias=False)\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.batch1(self.convt1(x)),0.02,inplace=True)\n",
    "        x = F.leaky_relu(self.batch2(self.convt2(x)),0.02,inplace=True)\n",
    "        x = F.leaky_relu(self.batch3(self.convt3(x)),0.02,inplace=True)\n",
    "        x = F.leaky_relu(self.batch4(self.convt4(x)),0.02,inplace=True)\n",
    "        x = self.final_conv(x)\n",
    "        x = F.tanh(x)\n",
    "        \n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 100 3 32 1\n",
      "1 100 3 32 0 10\n"
     ]
    }
   ],
   "source": [
    "print(ngpu, nz, nc, ngf, n_extra_g)\n",
    "netG = _netG_1(ngpu, nz, nc, ngf, n_extra_g)\n",
    "#netG_parallel = torch.nn.DataParallel(_netG_1(ngpu, nz, nc, ngf, n_extra_g))\n",
    "print(ngpu, nz, nc, ndf, n_extra_d,n_classes)\n",
    "netD = _netD_1(ngpu, nz, nc, ndf, n_extra_d,n_classes)\n",
    "#netD_parallel = torch.nn.DataParallel(_netD_1(ngpu, nz, nc, ndf, n_extra_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netD_1 (\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (final_conv): Conv2d(256, 10, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(netD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializador de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "\tclassname = m.__class__.__name__\n",
    "\tif classname.find('Conv') != -1:\n",
    "\t\tm.weight.data.normal_(0.0, 0.02)\n",
    "\telif classname.find('BatchNorm') != -1:\n",
    "\t\tm.weight.data.normal_(1.0, 0.02)\n",
    "\t\tm.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "netG.apply(weights_init)\n",
    "#netG_parallel.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "#netD_parallel.apply(weights_init)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "criterion_MSE = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sizes of the tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 100, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "input = torch.FloatTensor(batch_size, 3, imageSize, imageSize)\n",
    "print(input.size())\n",
    "noise = torch.FloatTensor(batch_size, nz, 1, 1)\n",
    "print(noise.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary=False\n",
    "#Ele testa pergunta se vc quer que o seu Z venha da distribuição bernoulli\n",
    "if binary:\n",
    "\tbernoulli_prob = torch.FloatTensor(batch_size, nz, 1, 1).fill_(0.5)\n",
    "\tfixed_noise = torch.bernoulli(bernoulli_prob)\n",
    "else:\n",
    "\tfixed_noise = torch.FloatTensor(batch_size, nz, 1, 1).normal_(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "label = torch.FloatTensor(batch_size,n_classes)\n",
    "print(label.size())\n",
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_hot = torch.LongTensor(64, 10).zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast then to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "\tnetD.cuda()\n",
    "\tnetG.cuda()\n",
    "\t#netD_parallel.cuda()\n",
    "\t#netG_parallel.cuda()\n",
    "\tcriterion = criterion.cuda()\n",
    "\tcriterion_MSE = criterion_MSE.cuda()\n",
    "\tinput,label = input.cuda(), label.cuda()\n",
    "\tnoise, fixed_noise = noise.cuda(), fixed_noise.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn then on Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = Variable(input)\n",
    "label = Variable(label)\n",
    "noise = Variable(noise)\n",
    "fixed_noise = Variable(fixed_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = Variable(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "print(type(label))\n",
    "print(label.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parâmetros de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta1, beta2 = 0.9,0.999\n",
    "lr = 2.0e-4\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = lr, betas = (beta1, beta2))\n",
    "#optimizerD = optim.Adam(netD_parallel.parameters(), lr = lr, betas = (beta1, beta2))\n",
    "\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = lr, betas = (beta1, beta2))\n",
    "#optimizerG = optim.Adam(netG_parallel.parameters(), lr = lr, betas = (beta1, beta2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS error: [Errno 17] File exists: 'outputdir_train_classifier'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "outputDir = 'outputdir_train_classifier'\n",
    "\n",
    "try:\n",
    "\tos.makedirs(outputDir)\n",
    "except OSError as err:\n",
    "\tprint(\"OS error: {0}\".format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.autograd.variable.Variable'>\n"
     ]
    }
   ],
   "source": [
    "print(type(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "a = tc.randn(64,3,32,32)\n",
    "a = Variable(a)\n",
    "output = netD(a.cuda())\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "b = tc.randn(64,100,1,1)\n",
    "b = Variable(b)\n",
    "output = netG(b.cuda())\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Treinamento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_our(num_epochs, dataloader, netD, netG, d_labelSmooth, outputDir,\n",
    "\t\t\t\t\t\tmodel_option =1,binary = False, epoch_interval = 100,\n",
    "\t\t\t\t\t\tD_steps = 1, G_steps = 1):\n",
    "\tuse_gpu = tc.cuda.is_available()\n",
    "\tprint('Lets train!')\n",
    "\t#if (batch_size/D_steps is not 0):\n",
    "\t\t#raise ValueError('Use batch_size multiple of D_steps')\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tstart_iter = time.time()  \n",
    "\t\tD_x = 0\n",
    "\t\tD_G_z1 = 0\n",
    "\t\tD_G_z2 = 0\n",
    "\t\terrD_acum = 0\n",
    "\t\terrG_acum = 0\n",
    "\t\t\n",
    "\n",
    "\t\tfor j, data in enumerate(dataloader, 0):\n",
    "\n",
    "\t\t\tfor z in range(D_steps):\n",
    "\t\t\t\tif z > 3:\n",
    "\t\t\t\t\traise ValueError('KEEP IT LOW!')\n",
    "\t\t\t\n",
    "\t\t\t\t############################\n",
    "\t\t\t\t# (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "\t\t\t\t# 1A - Train the detective network in the Real Dataset\n",
    "\t\t\t\t###########################\n",
    "\t\t\t\t# train with real\n",
    "\t\t\t\tnetD.zero_grad()\n",
    "\t\t\t\tone_hot.data.fill_(0)\n",
    "\t\t\t\t#real_cpu, _ = data\n",
    "\t\t\t\t#print('ITALOS',data[0].size()[0])\n",
    "\t\t\t\tstart = z*(int(data[0].size()[0]/D_steps))\n",
    "\t\t\t\tend = (z+1)*int(data[0].size()[0]/D_steps)\n",
    "\t\t\t\t#real_cpu = data[0][z*(int(data[0].size()[0]/D_steps)):(z+1)*int(data[0].size()[0]/D_steps)]\t\t\t\t\n",
    "\t\t\t\tinp, target = data\n",
    "\t\t\t\ttarget_ = tc.unsqueeze(target,1)\n",
    "\t\t\t\tone_hot.scatter_(1, target_, 1)\n",
    "\t\t\t\treal_cpu = data[0][start:end]\n",
    "\t\t\t\tif (epoch == 0 and z == 0 ):\n",
    "\t\t\t\t\tvutils.save_image(real_cpu[0:64,:,:,:],\n",
    "\t\t\t\t\t'%s/real_samples.png' % outputDir, nrow=8)\n",
    "\t\t\t\t\n",
    "\t\t\t\tbatch_size = real_cpu.size(0)\n",
    "\t\t\t\tinput.data.resize_(real_cpu.size()).copy_(real_cpu)\n",
    "\t\t\t\tlabel.data.resize_(batch_size,10).copy_(one_hot.data) # use smooth label for discriminator\n",
    "\t\t\t\toutput = netD(input)\n",
    "\t\t\t\terrD_real = criterion(output, label)\n",
    "\t\t\t\terrD_real.backward()\n",
    "\n",
    "\t\t\t\t#######################################################\n",
    "\t\t\t\t#######################################################\n",
    "\t\t\t\t# 1B - Train the detective network in the False Dataset\n",
    "\t\t\t\t#######################################################\n",
    "\n",
    "\t\t\t\tD_x += output.data.mean()\n",
    "\t\t\t\t# train with fake\n",
    "\t\t\t\tnoise.data.resize_(batch_size, nz, 1, 1)\n",
    "\t\t\t\tif binary:\n",
    "\t\t\t\t\tbernoulli_prob.resize_(noise.data.size())\n",
    "\t\t\t\t\tnoise.data.copy_(2*(torch.bernoulli(bernoulli_prob)-0.5))\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tnoise.data.normal_(0, 1)\n",
    "\t\t\t\tfake = netG(noise)\n",
    "#\t\t\t\tfake,z_prediction = netG(noise)\n",
    "\t\t\t\tlabel.data.fill_(fake_label)\n",
    "\t\t\t\toutput = netD(fake.detach()) # add \".detach()\" to avoid backprop through G\n",
    "\t\t\t\terrD_fake = criterion(output, label)\n",
    "\t\t\t\terrD_fake.backward() # gradients for fake/real will be accumulated\n",
    "\t\t\t\t# ERROR MEAN\n",
    "\t\t\t\tD_G_z1 += output.data.mean()\n",
    "\t\t\t\t\n",
    "\t\t\t\terrD_acum += errD_real.data[0] + errD_fake.data[0]\n",
    "\t\t\t\t\n",
    "\t\t\t\toptimizerD.step() # .step() can be called once the gradients are computed\n",
    "\n",
    "\t\t\t\t#######################################################\n",
    "\t\t\t\n",
    "\t\t\t\t# PARADA PARA VER O Q ESTÁ ACONTENDO\n",
    "\t\t\n",
    "\t\t\tfor a in range(G_steps):\n",
    "\t\t\t\t#print('interacao = ',a, 'de ',G_steps )\n",
    "\t\t\t\t# G_steps > D_steps (G_steps \\geq D_steps)\n",
    "\t\t\t\tif a > 3:\n",
    "\t\t\t\t\traise ValueError('KEEP IT LOW!')\n",
    "\t\t\n",
    "\t\t\t\t#######################################################\n",
    "\t\t\t\t# (2) Update G network: maximize log(D(G(z)))\n",
    "\t\t\t\t#  Train the faker with de output from the Detective (but don't train the Detective)\n",
    "\t\t\t\t#############3#########################################\n",
    "\t\t\t\tnetG.zero_grad()\n",
    "\t\t\t\tlabel.data.fill_(real_label) # fake labels are real for generator cost\n",
    "\t\t\t\toutput = netD(fake)\n",
    "\t\t\t\terrG = criterion(output, label)\n",
    "\t\t\t\terrG.backward(retain_graph = True) # True if backward through the graph for the second time\n",
    "\t\t\t\t#errG.backward() # True if backward through the graph for the second time\n",
    "\t\t\t\t\n",
    "\t\t\t\tif model_option == 2: # with z predictor\n",
    "\t\t\t\t\terrG_z = criterion_MSE(z_prediction, noise)\n",
    "\t\t\t\t\terrG_z.backward()\n",
    "\t\t\t\tD_G_z2 += output.data.mean()\n",
    "\t\t\t\terrG_acum += errG.data[0]\n",
    "\t\t\t\t#D_G_z2 = output.data.mean()\n",
    "\t\t\t\t#errG_acum = errG\t\t\t\t\n",
    "\t\t\t\toptimizerG.step()\n",
    "\n",
    "\t\tprint('epoch = ',epoch)\n",
    "\t\t\n",
    "\t\tend_iter = time.time()        \n",
    "\t\t#Print the info\n",
    "\t\tprint('[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f Elapsed %.2f s'\n",
    "\t\t\t  % (epoch, num_epochs, errD_acum/D_steps, errG_acum/G_steps, D_x, D_G_z1, D_G_z2, end_iter-start_iter))\n",
    "\n",
    "\t\t#Save a grid with the pictures from the dataset, up until 64\n",
    "\t\tsave_images(netG = netG, noise = fixed_noise, outputDir = outputDir, epoch = epoch)\n",
    "\t\t   \n",
    "\t\tif epoch % epoch_interval == 0:\n",
    "\t\t\t# do checkpointing\n",
    "\t\t\tsave_models(netG = netG, netD = netD, outputDir = outputDir, epoch = epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets train!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/anaconda3/envs/py35/lib/python3.5/site-packages/torch/nn/functional.py:767: UserWarning: Using a target size (torch.Size([64, 10])) that is different to the input size (torch.Size([64, 10, 1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor [64 x 10] and index [16 x 1] to have the same size in dimension 1 at /opt/conda/conda-bld/pytorch_1503968623488/work/torch/lib/TH/generic/THTensorMath.c:518",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-814-8bbe9a214960>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0md_labelSmooth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_our\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnetG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_labelSmooth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputDir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-813-0592e3819e92>\u001b[0m in \u001b[0;36mtrain_our\u001b[0;34m(num_epochs, dataloader, netD, netG, d_labelSmooth, outputDir, model_option, binary, epoch_interval, D_steps, G_steps)\u001b[0m\n\u001b[1;32m     35\u001b[0m                                 \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                                 \u001b[0mtarget_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                                 \u001b[0mone_hot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                                 \u001b[0mreal_cpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gabriel/anaconda3/envs/py35/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mscatter_\u001b[0;34m(self, dim, index, source)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mScatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscatter_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gabriel/anaconda3/envs/py35/lib/python3.5/site-packages/torch/autograd/_functions/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, dim, index, source, inplace)\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor [64 x 10] and index [16 x 1] to have the same size in dimension 1 at /opt/conda/conda-bld/pytorch_1503968623488/work/torch/lib/TH/generic/THTensorMath.c:518"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 100\n",
    "d_labelSmooth = 0.2\n",
    "\n",
    "train_our(num_epochs, dataloader, netD,netG,d_labelSmooth, outputDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
