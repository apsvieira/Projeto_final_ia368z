{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch as tc\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "#get_ipython().magic('matplotlib inline')\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import sys,os\n",
    "import math\n",
    "import gc\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 3.5.3 |Anaconda 4.4.0 (64-bit)| (default, Mar  6 2017, 11:58:13) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "__pyTorch VERSION: 0.2.0_4\n",
      "__CUDA VERSION\n",
      "__CUDNN VERSION: 6021\n",
      "__Number CUDA Devices: 1\n",
      "__Devices\n",
      "Active CUDA Device: GPU 0\n"
     ]
    }
   ],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "import torch\n",
    "import sys\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION')\n",
    "#from subprocess import call\n",
    "#call([\"nvcc\", \"--version\"])\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "#call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
    "print('Active CUDA Device: GPU', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_images(netG, noise, outputDir,epoch):\n",
    "   # the first 64 samples from the mini-batch are saved.\n",
    "   fake,_ = netG(fixed_noise)\n",
    "   vutils.save_image(fake.data[0:64,:,:,:],'%s/fake_samples_epoch_%03d.png' % (outputDir, epoch), nrow=8)\n",
    "\n",
    "def save_models(netG, netD, outputDir, epoch):\n",
    "   torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (outputDir, epoch))\n",
    "   torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (outputDir, epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netD_1(nn.Module):\n",
    "    def __init__(self, ngpu, nz, nc, ndf,  n_extra_layers_d, n_classes):\n",
    "        super(_netD_1, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.conv1 = nn.Conv2d(in_channels = nc, out_channels = ndf, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(in_channels = ndf, out_channels = ndf*2, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch2 = nn.BatchNorm2d(ndf * 2)\n",
    "        self.conv3 = nn.Conv2d(in_channels = ndf*2, out_channels = ndf*4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch3 = nn.BatchNorm2d(ndf * 4)\n",
    "        self.conv4 = nn.Conv2d(in_channels = ndf*4, out_channels = ndf*8, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch4 = nn.BatchNorm2d(ndf * 8)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(in_channels=ndf*8, out_channels=n_classes,kernel_size=2,stride=1,padding=0,bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print('0',x.size())\n",
    "\n",
    "        x = F.leaky_relu(self.conv1(x),0.02,inplace=True)\n",
    "        print('1',x.size())\n",
    "        x = F.leaky_relu(self.batch2(self.conv2(x)),0.02,inplace=True)\n",
    "        print('2',x.size())\n",
    "        x = F.leaky_relu(self.batch3(self.conv3(x)),0.02,inplace=True)\n",
    "        print('3',x.size())\n",
    "        x = F.leaky_relu(self.batch4(self.conv4(x)),0.02,inplace=True)   \n",
    "        print('antes final',x.size())\n",
    "        x = self.final_conv(x)\n",
    "        print('depois final',x.size())\n",
    "        \n",
    "        x = F.sigmoid(x)\n",
    "\n",
    "        return(x)\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using CUDA. If it is not what you want, manually set this as False!\n"
     ]
    }
   ],
   "source": [
    "cudnn.benchmark = True\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "#use_gpu = False\n",
    "if use_gpu:\n",
    "\tprint(\"You are using CUDA. If it is not what you want, manually set this as False!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nc = 3\n",
    "ngpu = 1\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "n_extra_d = 0\n",
    "n_extra_g = 1 # Aqui a jogada é que o gerador deve ser mais poderoso q o detetive\n",
    "imageSize = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This should, in the future, be set in CLI\n",
    "chosen_dataset = 'CIFAR10'\n",
    "\n",
    "datasets = {\n",
    "    'MNIST': torchvision.datasets.MNIST,\n",
    "    'CIFAR10': torchvision.datasets.CIFAR10,\n",
    "    'ANIME': '/home/gabriel/Redes Neurais/Projeto_Final_GANS/Tutorial_2/dataset/min_anime-faces',\n",
    "    #'FashionMNIST': torchvision.datasets.FashionMNIST\n",
    "}\n",
    "\n",
    "dataset = datasets[chosen_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.datasets.cifar.CIFAR10'>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "possible_parameters = {\n",
    "    'MNIST': {\n",
    "        'ndf': 64,\n",
    "        'ngf': 64,\n",
    "        'nz': 50,\n",
    "        'nc': 1,\n",
    "        'n_classes' : 10,\n",
    "    },\n",
    "    'CIFAR10': {\n",
    "        'ndf': 32,\n",
    "        'ngf': 32,\n",
    "        'nz': 100,\n",
    "        'nc': 3,\n",
    "        'n_extra_d' : 0,\n",
    "        'n_extra_g' : 1, # Aqui a jogada é que o gerador deve ser mais poderoso q o detetive\n",
    "        'imageSize' : 32,dataset_done = dataset('./datasets', train=True, download=True, transform=transform)\n",
    "        'n_classes' : 10,\n",
    "        'ngpu' : 1,\t\t\n",
    "    },\n",
    "    'ANIME': {\n",
    "        'nc' : 3,\n",
    "        'ngpu' : 1,\n",
    "        'nz' : 100,\n",
    "        'ngf' : 64,\n",
    "        'ndf' : 64,\n",
    "        'n_extra_d' : 0,\n",
    "        'n_extra_g' : 0, # Aqui a jogada é que o gerador deve ser mais poderoso q o detetive\n",
    "        'imageSize' : 64,\n",
    "        'n_classes' : 1\n",
    "\n",
    "\n",
    "    }\n",
    "    #'FashionMNIST': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngf = possible_parameters[chosen_dataset]['ngf']\n",
    "ndf = possible_parameters[chosen_dataset]['ndf']\n",
    "nz = possible_parameters[chosen_dataset]['nz']\n",
    "nc = possible_parameters[chosen_dataset]['nc']\n",
    "imageSize = possible_parameters[chosen_dataset]['imageSize']\n",
    "n_classes = possible_parameters[chosen_dataset]['n_classes']\n",
    "ngpu = possible_parameters[chosen_dataset]['ngpu']\n",
    "n_extra_d = possible_parameters[chosen_dataset]['n_extra_d']\n",
    "n_extra_g = possible_parameters[chosen_dataset]['n_extra_g']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'ANIME':\n",
    "    dataset = dset.ImageFolder(\n",
    "    root='/home/gabriel/Redes Neurais/Projeto_Final_GANS/Tutorial_2/dataset/min_anime-faces',\n",
    "    transform=transforms.Compose([\n",
    "            transforms.Scale((imageSize, imageSize)),\n",
    "            # transforms.CenterCrop(opt.imageSize),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)), # bring images to (-1,1)\n",
    "        ])\n",
    "    )\n",
    "else:\n",
    "    transform=transforms.Compose([\n",
    "                transforms.Scale((imageSize, imageSize)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)), # bring images to (-1,1)\n",
    "            ]) \n",
    "    dataset_done = dataset('./datasets', train=True, download=True, transform=transform)\n",
    "    dataloader = tc.utils.data.DataLoader(dataset_done, batch_size=batch_size, shuffle=True, num_workers=1\n",
    "                                          )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netD_1(nn.Module):\n",
    "    def __init__(self, ngpu, nz, nc, ndf,  n_extra_layers_d, n_classes):\n",
    "        super(_netD_1, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.conv1 = nn.Conv2d(in_channels = nc, out_channels = ndf, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(in_channels = ndf, out_channels = ndf*2, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch2 = nn.BatchNorm2d(ndf * 2)\n",
    "        self.conv3 = nn.Conv2d(in_channels = ndf*2, out_channels = ndf*4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch3 = nn.BatchNorm2d(ndf * 4)\n",
    "        self.conv4 = nn.Conv2d(in_channels = ndf*4, out_channels = ndf*8, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch4 = nn.BatchNorm2d(ndf * 8)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(in_channels=ndf*8, out_channels=n_classes+1,kernel_size=2,stride=1,padding=0,bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print('0',x.size())\n",
    "\n",
    "        x = F.leaky_relu(self.conv1(x),0.02,inplace=True)\n",
    "        #print('1',x.size())\n",
    "        x = F.leaky_relu(self.batch2(self.conv2(x)),0.02,inplace=True)\n",
    "        #print('2',x.size())\n",
    "        x = F.leaky_relu(self.batch3(self.conv3(x)),0.02,inplace=True)\n",
    "        #print('3',x.size())\n",
    "        x = F.leaky_relu(self.batch4(self.conv4(x)),0.02,inplace=True)   \n",
    "        #print('antes final',x.size())\n",
    "        x = self.final_conv(x)\n",
    "        #print('depois final',x.size())\n",
    "        \n",
    "        x = F.sigmoid(x)\n",
    "\n",
    "        return(x)\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netG_1(nn.Module):\n",
    "    def __init__(self, ngpu, nz, nc , ngf, n_extra_layers_g):\n",
    "        super(_netG_1, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.z = None\n",
    "        self.convt1 = nn.ConvTranspose2d(in_channels = nz, out_channels = ngf * 8, kernel_size=4, stride=2, padding=0, bias=False)\n",
    "        self.batch1 = nn.BatchNorm2d(ngf*8)\n",
    "        self.convt2 = nn.ConvTranspose2d(in_channels = ngf * 8, out_channels = ngf * 4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch2 = nn.BatchNorm2d(ngf*4)\n",
    "        self.convt3 = nn.ConvTranspose2d(in_channels = ngf * 4, out_channels = ngf * 2, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch3 = nn.BatchNorm2d(ngf*2)\n",
    "        self.convt4 = nn.ConvTranspose2d(in_channels = ngf*2, out_channels = ngf, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch4 = nn.BatchNorm2d(ngf)\n",
    "        \n",
    "        self.final_conv = nn.ConvTranspose2d(in_channels=ngf, out_channels=nc,kernel_size=1,stride=1,padding=0,bias=False)\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.batch1(self.convt1(x)),0.02,inplace=True)\n",
    "        x = F.leaky_relu(self.batch2(self.convt2(x)),0.02,inplace=True)\n",
    "        x = F.leaky_relu(self.batch3(self.convt3(x)),0.02,inplace=True)\n",
    "        x = F.leaky_relu(self.batch4(self.convt4(x)),0.02,inplace=True)\n",
    "        x = self.final_conv(x)\n",
    "        x = F.tanh(x)\n",
    "        \n",
    "        return (x),self.z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 100 3 32 1\n",
      "1 100 3 32 0 10\n"
     ]
    }
   ],
   "source": [
    "print(ngpu, nz, nc, ngf, n_extra_g)\n",
    "netG = _netG_1(ngpu, nz, nc, ngf, n_extra_g)\n",
    "#netG_parallel = torch.nn.DataParallel(_netG_1(ngpu, nz, nc, ngf, n_extra_g))\n",
    "print(ngpu, nz, nc, ndf, n_extra_d,n_classes)\n",
    "netD = _netD_1(ngpu, nz, nc, ndf, n_extra_d,n_classes)\n",
    "#netD_parallel = torch.nn.DataParallel(_netD_1(ngpu, nz, nc, ndf, n_extra_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netD_1 (\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (final_conv): Conv2d(256, 11, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(netD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializador de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "\tclassname = m.__class__.__name__\n",
    "\tif classname.find('Conv') != -1:\n",
    "\t\tm.weight.data.normal_(0.0, 0.02)\n",
    "\telif classname.find('BatchNorm') != -1:\n",
    "\t\tm.weight.data.normal_(1.0, 0.02)\n",
    "\t\tm.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "netG.apply(weights_init)\n",
    "#netG_parallel.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "#netD_parallel.apply(weights_init)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "criterion_MSE = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sizes of the tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 100, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "input = torch.FloatTensor(batch_size, 3, imageSize, imageSize)\n",
    "print(input.size())\n",
    "noise = torch.FloatTensor(batch_size, nz, 1, 1)\n",
    "print(noise.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary=False\n",
    "#Ele testa pergunta se vc quer que o seu Z venha da distribuição bernoulli\n",
    "if binary:\n",
    "\tbernoulli_prob = torch.FloatTensor(batch_size, nz, 1, 1).fill_(0.5)\n",
    "\tfixed_noise = torch.bernoulli(bernoulli_prob)\n",
    "else:\n",
    "\tfixed_noise = torch.FloatTensor(batch_size, nz, 1, 1).normal_(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "label = torch.FloatTensor(batch_size,n_classes)\n",
    "print(label.size())\n",
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_hot = torch.LongTensor(64, n_classes+1).zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast then to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "\tnetD.cuda()\n",
    "\tnetG.cuda()\n",
    "\t#netD_parallel.cuda()\n",
    "\t#netG_parallel.cuda()\n",
    "\tcriterion = criterion.cuda()\n",
    "\tcriterion_MSE = criterion_MSE.cuda()\n",
    "\tinput,label = input.cuda(), label.cuda()\n",
    "\tnoise, fixed_noise = noise.cuda(), fixed_noise.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "print(label.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn then on Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Variable(input)\n",
    "label = Variable(label)\n",
    "noise = Variable(noise)\n",
    "fixed_noise = Variable(fixed_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_hot = Variable(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "print(type(label))\n",
    "print(label.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parâmetros de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta1, beta2 = 0.9,0.999\n",
    "lr = 2.0e-4\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = lr, betas = (beta1, beta2))\n",
    "#optimizerD = optim.Adam(netD_parallel.parameters(), lr = lr, betas = (beta1, beta2))\n",
    "\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = lr, betas = (beta1, beta2))\n",
    "#optimizerG = optim.Adam(netG_parallel.parameters(), lr = lr, betas = (beta1, beta2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS error: [Errno 17] File exists: 'outputdir_train_classifier'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "outputDir = 'outputdir_train_classifier'\n",
    "\n",
    "try:\n",
    "\tos.makedirs(outputDir)\n",
    "except OSError as err:\n",
    "\tprint(\"OS error: {0}\".format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.autograd.variable.Variable'>\n"
     ]
    }
   ],
   "source": [
    "print(type(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "b = tc.randn(64,100,1,1)\n",
    "b = Variable(b)\n",
    "output,_ = netG(b.cuda())\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "a = tc.randn(10)\n",
    "print(a.size())\n",
    "a.squeeze()\n",
    "print(a.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Treinamento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_our(num_epochs, dataloader, netD, netG, d_labelSmooth, outputDir,\n",
    "                model_option =1,binary = False, epoch_interval = 100,\n",
    "                D_steps = 1, G_steps = 1):\n",
    "        use_gpu = tc.cuda.is_available()\n",
    "        print('Lets train!')\n",
    "        #if (batch_size/D_steps is not 0):\n",
    "        #raise ValueError('Use batch_size multiple of D_steps')\n",
    "        for epoch in range(num_epochs):\n",
    "            start_iter = time.time()  \n",
    "            D_x = 0\n",
    "            D_G_z1 = 0\n",
    "            D_G_z2 = 0\n",
    "            errD_acum = 0\n",
    "            errG_acum = 0\n",
    "\n",
    "\n",
    "            for j, data in enumerate(dataloader, 0):\n",
    "\n",
    "                for z in range(D_steps):\n",
    "                    if z > 3:\n",
    "                        raise ValueError('KEEP IT LOW!')\n",
    "\n",
    "                    ############################\n",
    "                    # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "                    # 1A - Train the detective network in the Real Dataset\n",
    "                    ###########################\n",
    "                    # train with real\n",
    "                    netD.zero_grad()\n",
    "                    one_hot.data.fill_(0)\n",
    "                    #real_cpu, _ = data\n",
    "                    #print('ITALOS',data[0].size()[0])\n",
    "                    start = z*(int(data[0].size()[0]/D_steps))\n",
    "                    end = (z+1)*int(data[0].size()[0]/D_steps)\n",
    "                    #real_cpu = data[0][z*(int(data[0].size()[0]/D_steps)):(z+1)*int(data[0].size()[0]/D_steps)]\t\t\t\t\n",
    "                    inp, target = data\n",
    "                    \n",
    "                    #Aqui começa um one-hot-encoding\n",
    "                    target_ = tc.unsqueeze(target,1)\n",
    "                    one_hot.data.resize_(target_.size()[0],one_hot.size()[1])\n",
    "                    one_hot.scatter_(1, target_, 1)\n",
    "                    \n",
    "                    \n",
    "                    real_cpu = data[0][start:end]\n",
    "                    if (epoch == 0 and z == 0 ):\n",
    "                        vutils.save_image(real_cpu[0:64,:,:,:],\n",
    "                        '%s/real_samples.png' % outputDir, nrow=8)\n",
    "\n",
    "                    batch_size = real_cpu.size(0)\n",
    "                    input.data.resize_(real_cpu.size()).copy_(real_cpu)\n",
    "                    ones_ = Variable(tc.ones(inp.size(0),1)).cuda()\n",
    "                    label.data.resize_(inp.size(0),label.size(1))\n",
    "                    label2 = Variable(tc.cat((label.data,ones_.data),1))\n",
    "                    \n",
    "                    label2.data.resize_(batch_size,one_hot.size(1)).copy_(one_hot.data) # use smooth label for discriminator\n",
    "                    \n",
    "                    output = netD(input)\n",
    "                    errD_real = criterion(output.squeeze(),label2)\n",
    "                    errD_real.backward()\n",
    "\n",
    "                    #######################################################\n",
    "                    #######################################################\n",
    "                    # 1B - Train the detective network in the False Dataset\n",
    "                    #######################################################\n",
    "\n",
    "                    D_x += output.data.mean()\n",
    "                    # train with fake\n",
    "                    noise.data.resize_(batch_size, nz, 1, 1)\n",
    "                    if binary:\n",
    "                        bernoulli_prob.resize_(noise.data.size())\n",
    "                        noise.data.copy_(2*(torch.bernoulli(bernoulli_prob)-0.5))\n",
    "                    else:\n",
    "                        noise.data.normal_(0, 1)\n",
    "                    fake,_ = netG(noise)\n",
    "                    #fake,z_prediction = netG(noise)\n",
    "                    label.data.fill_(fake_label)\n",
    "                    output = netD(fake.detach()) # add \".detach()\" to avoid backprop through G\n",
    "                    label3 = Variable(tc.cat((label.data,tc.zeros(inp.size(0),1).cuda()),1))\n",
    "                    errD_fake = criterion(output, label3)\n",
    "                    errD_fake.backward() # gradients for fake/real will be accumulated\n",
    "                    # ERROR MEAN\n",
    "                    D_G_z1 += output.data.mean()\n",
    "\n",
    "                    errD_acum += errD_real.data[0] + errD_fake.data[0]\n",
    "\n",
    "                    optimizerD.step() # .step() can be called once the gradients are computed\n",
    "\n",
    "                    #######################################################\n",
    "\n",
    "                    # PARADA PARA VER O Q ESTÁ ACONTENDO\n",
    "\n",
    "                for a in range(G_steps):\n",
    "                    #print('interacao = ',a, 'de ',G_steps )\n",
    "                    # G_steps > D_steps (G_steps \\geq D_steps)\n",
    "                    if a > 3:\n",
    "                        raise ValueError('KEEP IT LOW!')\n",
    "\n",
    "                    #######################################################\n",
    "                    # (2) Update G network: maximize log(D(G(z)))\n",
    "                    #  Train the faker with de output from the Detective (but don't train the Detective)\n",
    "                    #############3#########################################\n",
    "                    netG.zero_grad()\n",
    "                    label.data.fill_(real_label) # fake labels are real for generator cost\n",
    "                    output = netD(fake)\n",
    "                    errG = criterion(output, label2)\n",
    "                    errG.backward(retain_graph = True) # True if backward through the graph for the second time\n",
    "                    #errG.backward() # True if backward through the graph for the second time\n",
    "\n",
    "                    if model_option == 2: # with z predictor\n",
    "                        errG_z = criterion_MSE(z_prediction, noise)\n",
    "                        errG_z.backward()\n",
    "                    D_G_z2 += output.data.mean()\n",
    "                    errG_acum += errG.data[0]\n",
    "                    #D_G_z2 = output.data.mean()\n",
    "                    #errG_acum = errG\t\t\t\t\n",
    "                    optimizerG.step()\n",
    "\n",
    "            print('epoch = ',epoch)\n",
    "\n",
    "            end_iter = time.time()        \n",
    "            #Print the info\n",
    "            print('[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f Elapsed %.2f s'\n",
    "                  % (epoch, num_epochs, errD_acum/D_steps, errG_acum/G_steps, D_x, D_G_z1, D_G_z2, end_iter-start_iter))\n",
    "\n",
    "            #Save a grid with the pictures from the dataset, up until 64\n",
    "            save_images(netG = netG, noise = fixed_noise, outputDir = outputDir, epoch = epoch)\n",
    "\n",
    "            if epoch % epoch_interval == 0:\n",
    "                # do checkpointing\n",
    "                save_models(netG = netG, netD = netD, outputDir = outputDir, epoch = epoch)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets train!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/anaconda3/envs/py35/lib/python3.5/site-packages/torch/nn/functional.py:767: UserWarning: Using a target size (torch.Size([64, 11])) that is different to the input size (torch.Size([64, 11, 1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-312-8bbe9a214960>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0md_labelSmooth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_our\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnetG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_labelSmooth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputDir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-311-8b5bd56fe7c5>\u001b[0m in \u001b[0;36mtrain_our\u001b[0;34m(num_epochs, dataloader, netD, netG, d_labelSmooth, outputDir, model_option, binary, epoch_interval, D_steps, G_steps)\u001b[0m\n\u001b[1;32m    109\u001b[0m                         \u001b[0merrG_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion_MSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                         \u001b[0merrG_z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                     \u001b[0mD_G_z2\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                     \u001b[0merrG_acum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0merrG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                     \u001b[0;31m#D_G_z2 = output.data.mean()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-140:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gabriel/anaconda3/envs/py35/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/gabriel/anaconda3/envs/py35/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/gabriel/anaconda3/envs/py35/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/gabriel/anaconda3/envs/py35/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/gabriel/anaconda3/envs/py35/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/gabriel/anaconda3/envs/py35/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/gabriel/anaconda3/envs/py35/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 100\n",
    "d_labelSmooth = 0.2\n",
    "\n",
    "train_our(num_epochs, dataloader, netD,netG,d_labelSmooth, outputDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
