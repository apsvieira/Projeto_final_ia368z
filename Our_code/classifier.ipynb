{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch as tc\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "#get_ipython().magic('matplotlib inline')\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import sys,os\n",
    "import math\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 3.5.3 |Anaconda 4.4.0 (64-bit)| (default, Mar  6 2017, 11:58:13) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "__pyTorch VERSION: 0.2.0_4\n",
      "__CUDA VERSION\n",
      "__CUDNN VERSION: 6021\n",
      "__Number CUDA Devices: 1\n",
      "__Devices\n",
      "Active CUDA Device: GPU 0\n"
     ]
    }
   ],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "import torch\n",
    "import sys\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION')\n",
    "#from subprocess import call\n",
    "#call([\"nvcc\", \"--version\"])\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "#call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
    "print('Active CUDA Device: GPU', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using CUDA. If it is not what you want, manually set this as False!\n"
     ]
    }
   ],
   "source": [
    "cudnn.benchmark = True\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "#use_gpu = False\n",
    "if use_gpu:\n",
    "\tprint(\"You are using CUDA. If it is not what you want, manually set this as False!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nc = 3\n",
    "ngpu = 1\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "n_extra_d = 0\n",
    "n_extra_g = 1 # Aqui a jogada é que o gerador deve ser mais poderoso q o detetive\n",
    "imageSize = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should, in the future, be set in CLI\n",
    "chosen_dataset = 'CIFAR10'\n",
    "\n",
    "datasets = {\n",
    "    'MNIST': torchvision.datasets.MNIST,\n",
    "    'CIFAR10': torchvision.datasets.CIFAR10,\n",
    "    'ANIME': '/home/gabriel/Redes Neurais/Projeto_Final_GANS/Tutorial_2/dataset/min_anime-faces',\n",
    "    #'FashionMNIST': torchvision.datasets.FashionMNIST\n",
    "}\n",
    "\n",
    "dataset = datasets[chosen_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.datasets.cifar.CIFAR10'>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_parameters = {\n",
    "    'MNIST': {\n",
    "        'ndf': 64,\n",
    "        'ngf': 64,\n",
    "        'nz': 50,\n",
    "        'nc': 1,\n",
    "        'n_classes' : 10,\n",
    "    },\n",
    "    'CIFAR10': {\n",
    "        'ndf': 64,\n",
    "        'ngf': 64,\n",
    "        'nz': 100,\n",
    "        'nc': 3,\n",
    "        'n_extra_d' : 0,\n",
    "        'n_extra_g' : 1, # Aqui a jogada é que o gerador deve ser mais poderoso q o detetive\n",
    "        'imageSize' : 32,\n",
    "        'n_classes' : 10\n",
    "    },\n",
    "    'ANIME': {\n",
    "        'nc' : 3,\n",
    "        'ngpu' : 1,\n",
    "        'nz' : 100,\n",
    "        'ngf' : 64,\n",
    "        'ndf' : 64,\n",
    "        'n_extra_d' : 0,\n",
    "        'n_extra_g' : 1, # Aqui a jogada é que o gerador deve ser mais poderoso q o detetive\n",
    "        'imageSize' : 64,\n",
    "        'n_classes' : 1\n",
    "\n",
    "\n",
    "    }\n",
    "    #'FashionMNIST': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngf = possible_parameters[chosen_dataset]['ngf']\n",
    "ndf = possible_parameters[chosen_dataset]['ndf']\n",
    "code_size = possible_parameters[chosen_dataset]['nz']\n",
    "n_channels = possible_parameters[chosen_dataset]['nc']\n",
    "imageSize = possible_parameters[chosen_dataset]['imageSize']\n",
    "n_classes = possible_parameters[chosen_dataset]['n_classes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_done = dataset('./datasets', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./datasets/cifar-10-python.tar.gz\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'ANIME':\n",
    "    dataset = dset.ImageFolder(\n",
    "    root='/home/gabriel/Redes Neurais/Projeto_Final_GANS/Tutorial_2/dataset/min_anime-faces',\n",
    "    transform=transforms.Compose([\n",
    "            transforms.Scale((imageSize, imageSize)),\n",
    "            # transforms.CenterCrop(opt.imageSize),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)), # bring images to (-1,1)\n",
    "        ])\n",
    "    )\n",
    "else:\n",
    "    transform=transforms.Compose([\n",
    "                transforms.Scale((imageSize, imageSize)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)), # bring images to (-1,1)\n",
    "            ]) \n",
    "    dataset_done = dataset('./datasets', train=True, download=True, transform=transform)\n",
    "    dataloader = tc.utils.data.DataLoader(dataset_done, batch_size=batch_size, shuffle=True, num_workers=1\n",
    "                                          )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _netD_1(nn.Module):\n",
    "\tdef __init__(self, ngpu, nz, nc, ndf,  n_extra_layers_d):\n",
    "\t\tsuper(_netD_1, self).__init__()\n",
    "\t\tself.ngpu = ngpu\n",
    "\t\tmain = nn.Sequential(\n",
    "\t\t\t# input is (nc) x 96 x 96\n",
    "\t\t\tnn.Conv2d(nc, ndf, 4, 2, 1, bias=False), # 5,3,1 for 96x96\n",
    "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\t\t\t# state size. (ndf) x 32 x 32\n",
    "\t\t\tnn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "\t\t\tnn.BatchNorm2d(ndf * 2),\n",
    "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\t\t\t# state size. (ndf*2) x 16 x 16\n",
    "\t\t\tnn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "\t\t\tnn.BatchNorm2d(ndf * 4),\n",
    "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\t\t\t# state size. (ndf*4) x 8 x 8\n",
    "\t\t\tnn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "\t\t\tnn.BatchNorm2d(ndf * 8),\n",
    "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\t\t\t# state size. (ndf*8) x 4 x 4\n",
    "\t\t)\n",
    "\n",
    "\t\t# Extra layers\n",
    "\t\tfor t in range(n_extra_layers_d):\n",
    "\t\t\tmain.add_module('extra-layers-{0}.{1}.conv'.format(t, ndf * 8),\n",
    "\t\t\t\t\t\t\tnn.Conv2d(ndf * 8, ndf * 8, 3, 1, 1, bias=False))\n",
    "\t\t\tmain.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, ndf * 8),\n",
    "\t\t\t\t\t\t\tnn.BatchNorm2d(ndf * 8))\n",
    "\t\t\tmain.add_module('extra-layers-{0}.{1}.relu'.format(t, ndf * 8),\n",
    "\t\t\t\t\t\t\tnn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "\n",
    "\t\tmain.add_module('final_layers.conv', nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False))\n",
    "\t\tmain.add_module('final_layers.sigmoid', nn.Sigmoid())\n",
    "\t\t# state size. 1 x 1 x 1\n",
    "\t\tself.main = main\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tgpu_ids = None\n",
    "\t\tif isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "\t\t\tgpu_ids = range(self.ngpu)\n",
    "\t\toutput = nn.parallel.data_parallel(self.main, input, gpu_ids)\n",
    "\t\treturn output.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netG_1(nn.Module):\n",
    "\tdef __init__(self, ngpu, nz, nc , ngf, n_extra_layers_g):\n",
    "\t\tsuper(_netG_1, self).__init__()\n",
    "\t\tself.ngpu = ngpu\n",
    "\t\t#self.nz = nz\n",
    "\t\t#self.nc = nc\n",
    "\t\t#self.ngf = ngf\n",
    "\t\tmain = nn.Sequential(\n",
    "\t\t\t# input is Z, going into a convolution\n",
    "\t\t\t# state size. nz x 1 x 1\n",
    "\t\t\tnn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "\t\t\tnn.BatchNorm2d(ngf * 8),\n",
    "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\t\t\t# state size. (ngf*8) x 4 x 4\n",
    "\t\t\tnn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "\t\t\tnn.BatchNorm2d(ngf * 4),\n",
    "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\t\t\t# state size. (ngf*4) x 8 x 8\n",
    "\t\t\tnn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "\t\t\tnn.BatchNorm2d(ngf * 2),\n",
    "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\t\t\t# state size. (ngf*2) x 16 x 16\n",
    "\t\t\tnn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "\t\t\tnn.BatchNorm2d(ngf),\n",
    "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\t\t\t# state size. (ngf) x 32 x 32\n",
    "\t\t)\n",
    "\n",
    "\t\t# Extra layers\n",
    "\t\tfor t in range(n_extra_layers_g):\n",
    "\t\t\tmain.add_module('extra-layers-{0}.{1}.conv'.format(t, ngf),\n",
    "\t\t\t\t\t\t\tnn.Conv2d(ngf, ngf, 3, 1, 1, bias=False))\n",
    "\t\t\tmain.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, ngf),\n",
    "\t\t\t\t\t\t\tnn.BatchNorm2d(ngf))\n",
    "\t\t\tmain.add_module('extra-layers-{0}.{1}.relu'.format(t, ngf),\n",
    "\t\t\t\t\t\t\tnn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "\t\tmain.add_module('final_layer.deconv', \n",
    "\t\t\t\t\t\t nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False)) # 5,3,1 for 96x96\n",
    "\t\tmain.add_module('final_layer.tanh', \n",
    "\t\t\t\t\t\t nn.Tanh())\n",
    "\t\t\t# state size. (nc) x 96 x 96\n",
    "\n",
    "\t\tself.main = main\n",
    "\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tgpu_ids = None\n",
    "\t\tif isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "\t\t\tgpu_ids = range(self.ngpu)\n",
    "\t\treturn nn.parallel.data_parallel(self.main, input, gpu_ids), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "netG = _netG_1(ngpu, nz, nc, ngf, n_extra_g)\n",
    "#netG_parallel = torch.nn.DataParallel(_netG_1(ngpu, nz, nc, ngf, n_extra_g))\n",
    "\n",
    "netD = _netD_1(ngpu, nz, nc, ndf, n_extra_d)\n",
    "#netD_parallel = torch.nn.DataParallel(_netD_1(ngpu, nz, nc, ndf, n_extra_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializador de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "\tclassname = m.__class__.__name__\n",
    "\tif classname.find('Conv') != -1:\n",
    "\t\tm.weight.data.normal_(0.0, 0.02)\n",
    "\telif classname.find('BatchNorm') != -1:\n",
    "\t\tm.weight.data.normal_(1.0, 0.02)\n",
    "\t\tm.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "netG.apply(weights_init)\n",
    "#netG_parallel.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "#netD_parallel.apply(weights_init)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "criterion_MSE = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sizes of the tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 100, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "input = torch.FloatTensor(batch_size, 3, imageSize, imageSize)\n",
    "print(input.size())\n",
    "noise = torch.FloatTensor(batch_size, nz, 1, 1)\n",
    "print(noise.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(dataloader.dataset.train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary=False\n",
    "#Ele testa pergunta se vc quer que o seu Z venha da distribuição bernoulli\n",
    "if binary:\n",
    "\tbernoulli_prob = torch.FloatTensor(batch_size, nz, 1, 1).fill_(0.5)\n",
    "\tfixed_noise = torch.bernoulli(bernoulli_prob)\n",
    "else:\n",
    "\tfixed_noise = torch.FloatTensor(batch_size, nz, 1, 1).normal_(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "label = torch.FloatTensor(batch_size,n_classes)\n",
    "print(label.size())\n",
    "real_label = 1\n",
    "fake_label = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast then to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "\tnetD.cuda()\n",
    "\tnetG.cuda()\n",
    "\t#netD_parallel.cuda()\n",
    "\t#netG_parallel.cuda()\n",
    "\tcriterion = criterion.cuda()\n",
    "\tcriterion_MSE = criterion_MSE.cuda()\n",
    "\tinput,label = input.cuda(), label.cuda()\n",
    "\tnoise, fixed_noise = noise.cuda(), fixed_noise.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn then on Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Variable data has to be a tensor, but got Variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-3aeb0f5de83e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfixed_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixed_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Variable data has to be a tensor, but got Variable"
     ]
    }
   ],
   "source": [
    "input = Variable(input)\n",
    "label = Variable(label)\n",
    "noise = Variable(noise)\n",
    "fixed_noise = Variable(fixed_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parâmetros de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta1, beta2 = 0.9,0.999\n",
    "lr = 2.0e-4\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = lr, betas = (beta1, beta2))\n",
    "#optimizerD = optim.Adam(netD_parallel.parameters(), lr = lr, betas = (beta1, beta2))\n",
    "\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = lr, betas = (beta1, beta2))\n",
    "#optimizerG = optim.Adam(netG_parallel.parameters(), lr = lr, betas = (beta1, beta2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "outputDir = 'outputdir_train_sobrescreve'\n",
    "\n",
    "try:\n",
    "\tos.makedirs(outputDir)\n",
    "except OSError as err:\n",
    "\tprint(\"OS error: {0}\".format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.FloatTensor'>\n",
      "torch.Size([64, 3, 32, 32])\n",
      "<class 'torch.LongTensor'>\n",
      "torch.Size([64])\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for k,data in enumerate(dataloader):\n",
    "    if k>0:\n",
    "        break\n",
    "    inp,label = data\n",
    "    print(type(inp))\n",
    "    print(inp.size())\n",
    "    print(type(label))\n",
    "    print(label.size())\n",
    "    print(label.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
