{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch as tc\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "#get_ipython().magic('matplotlib inline')\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import sys,os\n",
    "import math\n",
    "import gc\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 3.5.3 |Anaconda 4.4.0 (64-bit)| (default, Mar  6 2017, 11:58:13) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "__pyTorch VERSION: 0.2.0_4\n",
      "__CUDA VERSION\n",
      "__CUDNN VERSION: 6021\n",
      "__Number CUDA Devices: 1\n",
      "__Devices\n",
      "Active CUDA Device: GPU 0\n"
     ]
    }
   ],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "import torch\n",
    "import sys\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION')\n",
    "#from subprocess import call\n",
    "#call([\"nvcc\", \"--version\"])\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "#call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
    "print('Active CUDA Device: GPU', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_images(netG, noise, outputDir,epoch):\n",
    "   # the first 64 samples from the mini-batch are saved.\n",
    "   fake,_ = netG(fixed_noise)\n",
    "   vutils.save_image(fake.data[0:64,:,:,:],'%s/fake_samples_epoch_%03d.png' % (outputDir, epoch), nrow=8)\n",
    "\n",
    "def save_models(netG, netD, outputDir, epoch):\n",
    "   torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (outputDir, epoch))\n",
    "   torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (outputDir, epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netD_1(nn.Module):\n",
    "    def __init__(self, ngpu, nz, nc, ndf,  n_extra_layers_d, n_classes):\n",
    "        super(_netD_1, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.conv1 = nn.Conv2d(in_channels = nc, out_channels = ndf, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(in_channels = ndf, out_channels = ndf*2, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch2 = nn.BatchNorm2d(ndf * 2)\n",
    "        self.conv3 = nn.Conv2d(in_channels = ndf*2, out_channels = ndf*4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch3 = nn.BatchNorm2d(ndf * 4)\n",
    "        self.conv4 = nn.Conv2d(in_channels = ndf*4, out_channels = ndf*8, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch4 = nn.BatchNorm2d(ndf * 8)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(in_channels=ndf*8, out_channels=n_classes,kernel_size=2,stride=1,padding=0,bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print('0',x.size())\n",
    "\n",
    "        x = F.leaky_relu(self.conv1(x),0.02,inplace=True)\n",
    "        print('1',x.size())\n",
    "        x = F.leaky_relu(self.batch2(self.conv2(x)),0.02,inplace=True)\n",
    "        print('2',x.size())\n",
    "        x = F.leaky_relu(self.batch3(self.conv3(x)),0.02,inplace=True)\n",
    "        print('3',x.size())\n",
    "        x = F.leaky_relu(self.batch4(self.conv4(x)),0.02,inplace=True)   \n",
    "        print('antes final',x.size())\n",
    "        x = self.final_conv(x)\n",
    "        print('depois final',x.size())\n",
    "        \n",
    "        x = F.sigmoid(x)\n",
    "\n",
    "        return(x)\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using CUDA. If it is not what you want, manually set this as False!\n"
     ]
    }
   ],
   "source": [
    "cudnn.benchmark = True\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "#use_gpu = False\n",
    "if use_gpu:\n",
    "\tprint(\"You are using CUDA. If it is not what you want, manually set this as False!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nc = 3\n",
    "ngpu = 1\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "n_extra_d = 0\n",
    "n_extra_g = 1 # Aqui a jogada é que o gerador deve ser mais poderoso q o detetive\n",
    "imageSize = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This should, in the future, be set in CLI\n",
    "chosen_dataset = 'CIFAR10'\n",
    "\n",
    "datasets = {\n",
    "    'MNIST': torchvision.datasets.MNIST,\n",
    "    'CIFAR10': torchvision.datasets.CIFAR10,\n",
    "    'ANIME': '/home/gabriel/Redes Neurais/Projeto_Final_GANS/Tutorial_2/dataset/min_anime-faces',\n",
    "    #'FashionMNIST': torchvision.datasets.FashionMNIST\n",
    "}\n",
    "\n",
    "dataset = datasets[chosen_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.datasets.cifar.CIFAR10'>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "possible_parameters = {\n",
    "    'MNIST': {\n",
    "        'ndf': 64,\n",
    "        'ngf': 64,\n",
    "        'nz': 50,\n",
    "        'nc': 1,\n",
    "        'n_classes' : 10,\n",
    "    },\n",
    "    'CIFAR10': {\n",
    "        'ndf': 32,\n",
    "        'ngf': 32,\n",
    "        'nz': 100,\n",
    "        'nc': 3,\n",
    "        'n_extra_d' : 0,\n",
    "        'n_extra_g' : 1, # Aqui a jogada é que o gerador deve ser mais poderoso q o detetive\n",
    "        'imageSize' : 32,dataset_done = dataset('./datasets', train=True, download=True, transform=transform)\n",
    "        'n_classes' : 10,\n",
    "        'ngpu' : 1,\t\t\n",
    "    },\n",
    "    'ANIME': {\n",
    "        'nc' : 3,\n",
    "        'ngpu' : 1,\n",
    "        'nz' : 100,\n",
    "        'ngf' : 64,\n",
    "        'ndf' : 64,\n",
    "        'n_extra_d' : 0,\n",
    "        'n_extra_g' : 0, # Aqui a jogada é que o gerador deve ser mais poderoso q o detetive\n",
    "        'imageSize' : 64,\n",
    "        'n_classes' : 1\n",
    "\n",
    "\n",
    "    }\n",
    "    #'FashionMNIST': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngf = possible_parameters[chosen_dataset]['ngf']\n",
    "ndf = possible_parameters[chosen_dataset]['ndf']\n",
    "nz = possible_parameters[chosen_dataset]['nz']\n",
    "nc = possible_parameters[chosen_dataset]['nc']\n",
    "imageSize = possible_parameters[chosen_dataset]['imageSize']\n",
    "n_classes = possible_parameters[chosen_dataset]['n_classes']\n",
    "ngpu = possible_parameters[chosen_dataset]['ngpu']\n",
    "n_extra_d = possible_parameters[chosen_dataset]['n_extra_d']\n",
    "n_extra_g = possible_parameters[chosen_dataset]['n_extra_g']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'ANIME':\n",
    "    dataset = dset.ImageFolder(\n",
    "    root='/home/gabriel/Redes Neurais/Projeto_Final_GANS/Tutorial_2/dataset/min_anime-faces',\n",
    "    transform=transforms.Compose([\n",
    "            transforms.Scale((imageSize, imageSize)),\n",
    "            # transforms.CenterCrop(opt.imageSize),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)), # bring images to (-1,1)\n",
    "        ])\n",
    "    )\n",
    "else:\n",
    "    transform=transforms.Compose([\n",
    "                transforms.Scale((imageSize, imageSize)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)), # bring images to (-1,1)\n",
    "            ]) \n",
    "    dataset_done = dataset('./datasets', train=True, download=True, transform=transform)\n",
    "    dataloader = tc.utils.data.DataLoader(dataset_done, batch_size=batch_size, shuffle=True, num_workers=1\n",
    "                                          )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netD_1(nn.Module):\n",
    "    def __init__(self, ngpu, nz, nc, ndf,  n_extra_layers_d, n_classes):\n",
    "        super(_netD_1, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.conv1 = nn.Conv2d(in_channels = nc, out_channels = ndf, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(in_channels = ndf, out_channels = ndf*2, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch2 = nn.BatchNorm2d(ndf * 2)\n",
    "        self.conv3 = nn.Conv2d(in_channels = ndf*2, out_channels = ndf*4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch3 = nn.BatchNorm2d(ndf * 4)\n",
    "        self.conv4 = nn.Conv2d(in_channels = ndf*4, out_channels = ndf*8, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch4 = nn.BatchNorm2d(ndf * 8)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(in_channels=ndf*8, out_channels=n_classes,kernel_size=2,stride=1,padding=0,bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print('0',x.size())\n",
    "\n",
    "        x = F.leaky_relu(self.conv1(x),0.02,inplace=True)\n",
    "        #print('1',x.size())\n",
    "        x = F.leaky_relu(self.batch2(self.conv2(x)),0.02,inplace=True)\n",
    "        #print('2',x.size())\n",
    "        x = F.leaky_relu(self.batch3(self.conv3(x)),0.02,inplace=True)\n",
    "        #print('3',x.size())\n",
    "        x = F.leaky_relu(self.batch4(self.conv4(x)),0.02,inplace=True)   \n",
    "        #print('antes final',x.size())\n",
    "        x = self.final_conv(x)\n",
    "        #print('depois final',x.size())\n",
    "        \n",
    "        x = F.sigmoid(x)\n",
    "\n",
    "        return(x)\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netG_1(nn.Module):\n",
    "    def __init__(self, ngpu, nz, nc , ngf, n_extra_layers_g):\n",
    "        super(_netG_1, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.z = None\n",
    "        self.convt1 = nn.ConvTranspose2d(in_channels = nz, out_channels = ngf * 8, kernel_size=4, stride=2, padding=0, bias=False)\n",
    "        self.batch1 = nn.BatchNorm2d(ngf*8)\n",
    "        self.convt2 = nn.ConvTranspose2d(in_channels = ngf * 8, out_channels = ngf * 4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch2 = nn.BatchNorm2d(ngf*4)\n",
    "        self.convt3 = nn.ConvTranspose2d(in_channels = ngf * 4, out_channels = ngf * 2, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch3 = nn.BatchNorm2d(ngf*2)\n",
    "        self.convt4 = nn.ConvTranspose2d(in_channels = ngf*2, out_channels = ngf, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch4 = nn.BatchNorm2d(ngf)\n",
    "        \n",
    "        self.final_conv = nn.ConvTranspose2d(in_channels=ngf, out_channels=nc,kernel_size=1,stride=1,padding=0,bias=False)\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.batch1(self.convt1(x)),0.02,inplace=True)\n",
    "        x = F.leaky_relu(self.batch2(self.convt2(x)),0.02,inplace=True)\n",
    "        x = F.leaky_relu(self.batch3(self.convt3(x)),0.02,inplace=True)\n",
    "        x = F.leaky_relu(self.batch4(self.convt4(x)),0.02,inplace=True)\n",
    "        x = self.final_conv(x)\n",
    "        x = F.tanh(x)\n",
    "        \n",
    "        return (x),self.z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 100 3 32 1\n",
      "1 100 3 32 0 10\n"
     ]
    }
   ],
   "source": [
    "print(ngpu, nz, nc, ngf, n_extra_g)\n",
    "netG = _netG_1(ngpu, nz, nc, ngf, n_extra_g)\n",
    "#netG_parallel = torch.nn.DataParallel(_netG_1(ngpu, nz, nc, ngf, n_extra_g))\n",
    "print(ngpu, nz, nc, ndf, n_extra_d,n_classes)\n",
    "netD = _netD_1(ngpu, nz, nc, ndf, n_extra_d,n_classes)\n",
    "#netD_parallel = torch.nn.DataParallel(_netD_1(ngpu, nz, nc, ndf, n_extra_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netD_1 (\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (final_conv): Conv2d(256, 10, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(netD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializador de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "\tclassname = m.__class__.__name__\n",
    "\tif classname.find('Conv') != -1:\n",
    "\t\tm.weight.data.normal_(0.0, 0.02)\n",
    "\telif classname.find('BatchNorm') != -1:\n",
    "\t\tm.weight.data.normal_(1.0, 0.02)\n",
    "\t\tm.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "netG.apply(weights_init)\n",
    "#netG_parallel.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "#netD_parallel.apply(weights_init)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "criterion_MSE = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sizes of the tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 100, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "input = torch.FloatTensor(batch_size, 3, imageSize, imageSize)\n",
    "print(input.size())\n",
    "noise = torch.FloatTensor(batch_size, nz, 1, 1)\n",
    "print(noise.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary=False\n",
    "#Ele testa pergunta se vc quer que o seu Z venha da distribuição bernoulli\n",
    "if binary:\n",
    "\tbernoulli_prob = torch.FloatTensor(batch_size, nz, 1, 1).fill_(0.5)\n",
    "\tfixed_noise = torch.bernoulli(bernoulli_prob)\n",
    "else:\n",
    "\tfixed_noise = torch.FloatTensor(batch_size, nz, 1, 1).normal_(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "label = torch.FloatTensor(batch_size,n_classes)\n",
    "print(label.size())\n",
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_hot = torch.LongTensor(64, 10).zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast then to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "\tnetD.cuda()\n",
    "\tnetG.cuda()\n",
    "\t#netD_parallel.cuda()\n",
    "\t#netG_parallel.cuda()\n",
    "\tcriterion = criterion.cuda()\n",
    "\tcriterion_MSE = criterion_MSE.cuda()\n",
    "\tinput,label = input.cuda(), label.cuda()\n",
    "\tnoise, fixed_noise = noise.cuda(), fixed_noise.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn then on Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = Variable(input)\n",
    "label = Variable(label)\n",
    "noise = Variable(noise)\n",
    "fixed_noise = Variable(fixed_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_hot = Variable(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "print(type(label))\n",
    "print(label.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parâmetros de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta1, beta2 = 0.9,0.999\n",
    "lr = 2.0e-4\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = lr, betas = (beta1, beta2))\n",
    "#optimizerD = optim.Adam(netD_parallel.parameters(), lr = lr, betas = (beta1, beta2))\n",
    "\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = lr, betas = (beta1, beta2))\n",
    "#optimizerG = optim.Adam(netG_parallel.parameters(), lr = lr, betas = (beta1, beta2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS error: [Errno 17] File exists: 'outputdir_train_classifier'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "outputDir = 'outputdir_train_classifier'\n",
    "\n",
    "try:\n",
    "\tos.makedirs(outputDir)\n",
    "except OSError as err:\n",
    "\tprint(\"OS error: {0}\".format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.autograd.variable.Variable'>\n"
     ]
    }
   ],
   "source": [
    "print(type(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "a = tc.randn(64,3,32,32)\n",
    "a = Variable(a)\n",
    "output = netD(a.cuda())\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "b = tc.randn(64,100,1,1)\n",
    "b = Variable(b)\n",
    "output,_ = netG(b.cuda())\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Treinamento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_our(num_epochs, dataloader, netD, netG, d_labelSmooth, outputDir,\n",
    "                model_option =1,binary = False, epoch_interval = 100,\n",
    "                D_steps = 1, G_steps = 1):\n",
    "        use_gpu = tc.cuda.is_available()\n",
    "        print('Lets train!')\n",
    "        #if (batch_size/D_steps is not 0):\n",
    "        #raise ValueError('Use batch_size multiple of D_steps')\n",
    "        for epoch in range(num_epochs):\n",
    "            start_iter = time.time()  \n",
    "            D_x = 0\n",
    "            D_G_z1 = 0\n",
    "            D_G_z2 = 0\n",
    "            errD_acum = 0\n",
    "            errG_acum = 0\n",
    "\n",
    "\n",
    "            for j, data in enumerate(dataloader, 0):\n",
    "\n",
    "                for z in range(D_steps):\n",
    "                    if z > 3:\n",
    "                        raise ValueError('KEEP IT LOW!')\n",
    "\n",
    "                    ############################\n",
    "                    # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "                    # 1A - Train the detective network in the Real Dataset\n",
    "                    ###########################\n",
    "                    # train with real\n",
    "                    netD.zero_grad()\n",
    "                    one_hot.data.fill_(0)\n",
    "                    #real_cpu, _ = data\n",
    "                    #print('ITALOS',data[0].size()[0])\n",
    "                    start = z*(int(data[0].size()[0]/D_steps))\n",
    "                    end = (z+1)*int(data[0].size()[0]/D_steps)\n",
    "                    #real_cpu = data[0][z*(int(data[0].size()[0]/D_steps)):(z+1)*int(data[0].size()[0]/D_steps)]\t\t\t\t\n",
    "                    inp, target = data\n",
    "                    \n",
    "                    #Aqui começa um one-hot-encoding\n",
    "                    target_ = tc.unsqueeze(target,1)\n",
    "                    one_hot.data.resize_(target_.size()[0],one_hot.size()[1])\n",
    "                    one_hot.scatter_(1, target_, 1)\n",
    "                    \n",
    "                    \n",
    "                    real_cpu = data[0][start:end]\n",
    "                    if (epoch == 0 and z == 0 ):\n",
    "                        vutils.save_image(real_cpu[0:64,:,:,:],\n",
    "                        '%s/real_samples.png' % outputDir, nrow=8)\n",
    "\n",
    "                    batch_size = real_cpu.size(0)\n",
    "                    input.data.resize_(real_cpu.size()).copy_(real_cpu)\n",
    "                    label.data.resize_(batch_size,10).copy_(one_hot.data) # use smooth label for discriminator\n",
    "                    output = netD(input)\n",
    "                    errD_real = criterion(output, label)\n",
    "                    errD_real.backward()\n",
    "\n",
    "                    #######################################################\n",
    "                    #######################################################\n",
    "                    # 1B - Train the detective network in the False Dataset\n",
    "                    #######################################################\n",
    "\n",
    "                    D_x += output.data.mean()\n",
    "                    # train with fake\n",
    "                    noise.data.resize_(batch_size, nz, 1, 1)\n",
    "                    if binary:\n",
    "                        bernoulli_prob.resize_(noise.data.size())\n",
    "                        noise.data.copy_(2*(torch.bernoulli(bernoulli_prob)-0.5))\n",
    "                    else:\n",
    "                        noise.data.normal_(0, 1)\n",
    "                    fake,_ = netG(noise)\n",
    "                    #fake,z_prediction = netG(noise)\n",
    "                    label.data.fill_(fake_label)\n",
    "                    output = netD(fake.detach()) # add \".detach()\" to avoid backprop through G\n",
    "                    errD_fake = criterion(output, label)\n",
    "                    errD_fake.backward() # gradients for fake/real will be accumulated\n",
    "                    # ERROR MEAN\n",
    "                    D_G_z1 += output.data.mean()\n",
    "\n",
    "                    errD_acum += errD_real.data[0] + errD_fake.data[0]\n",
    "\n",
    "                    optimizerD.step() # .step() can be called once the gradients are computed\n",
    "\n",
    "                    #######################################################\n",
    "\n",
    "                    # PARADA PARA VER O Q ESTÁ ACONTENDO\n",
    "\n",
    "                for a in range(G_steps):\n",
    "                    #print('interacao = ',a, 'de ',G_steps )\n",
    "                    # G_steps > D_steps (G_steps \\geq D_steps)\n",
    "                    if a > 3:\n",
    "                        raise ValueError('KEEP IT LOW!')\n",
    "\n",
    "                    #######################################################\n",
    "                    # (2) Update G network: maximize log(D(G(z)))\n",
    "                    #  Train the faker with de output from the Detective (but don't train the Detective)\n",
    "                    #############3#########################################\n",
    "                    netG.zero_grad()\n",
    "                    label.data.fill_(real_label) # fake labels are real for generator cost\n",
    "                    output = netD(fake)\n",
    "                    errG = criterion(output, label)\n",
    "                    errG.backward(retain_graph = True) # True if backward through the graph for the second time\n",
    "                    #errG.backward() # True if backward through the graph for the second time\n",
    "\n",
    "                    if model_option == 2: # with z predictor\n",
    "                        errG_z = criterion_MSE(z_prediction, noise)\n",
    "                        errG_z.backward()\n",
    "                    D_G_z2 += output.data.mean()\n",
    "                    errG_acum += errG.data[0]\n",
    "                    #D_G_z2 = output.data.mean()\n",
    "                    #errG_acum = errG\t\t\t\t\n",
    "                    optimizerG.step()\n",
    "\n",
    "            print('epoch = ',epoch)\n",
    "\n",
    "            end_iter = time.time()        \n",
    "            #Print the info\n",
    "            print('[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f Elapsed %.2f s'\n",
    "                  % (epoch, num_epochs, errD_acum/D_steps, errG_acum/G_steps, D_x, D_G_z1, D_G_z2, end_iter-start_iter))\n",
    "\n",
    "            #Save a grid with the pictures from the dataset, up until 64\n",
    "            save_images(netG = netG, noise = fixed_noise, outputDir = outputDir, epoch = epoch)\n",
    "\n",
    "            if epoch % epoch_interval == 0:\n",
    "                # do checkpointing\n",
    "                save_models(netG = netG, netD = netD, outputDir = outputDir, epoch = epoch)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets train!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/anaconda3/envs/py35/lib/python3.5/site-packages/torch/nn/functional.py:767: UserWarning: Using a target size (torch.Size([64, 10])) that is different to the input size (torch.Size([64, 10, 1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0\n",
      "[0/100] Loss_D: 200.4538 Loss_G: 4079.8048 D(x): 69.4667 D(G(z)): 12.7520 / 10.1603 Elapsed 36.21 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/anaconda3/envs/py35/lib/python3.5/site-packages/torch/nn/functional.py:767: UserWarning: Using a target size (torch.Size([16, 10])) that is different to the input size (torch.Size([16, 10, 1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  1\n",
      "[1/100] Loss_D: 155.9696 Loss_G: 5177.3644 D(x): 72.2219 D(G(z)): 5.4599 / 4.0878 Elapsed 12.21 s\n",
      "epoch =  2\n",
      "[2/100] Loss_D: 136.2811 Loss_G: 5645.0893 D(x): 72.3585 D(G(z)): 5.2437 / 3.9629 Elapsed 11.95 s\n",
      "epoch =  3\n",
      "[3/100] Loss_D: 124.8946 Loss_G: 5737.3866 D(x): 71.0206 D(G(z)): 6.4195 / 5.0253 Elapsed 12.27 s\n",
      "epoch =  4\n",
      "[4/100] Loss_D: 111.4992 Loss_G: 6067.4802 D(x): 72.0444 D(G(z)): 5.5479 / 4.4584 Elapsed 12.57 s\n",
      "epoch =  5\n",
      "[5/100] Loss_D: 100.8540 Loss_G: 6401.3759 D(x): 72.3893 D(G(z)): 5.1753 / 4.0798 Elapsed 13.27 s\n",
      "epoch =  6\n",
      "[6/100] Loss_D: 91.9679 Loss_G: 6610.7180 D(x): 72.3190 D(G(z)): 5.3092 / 4.2363 Elapsed 12.46 s\n",
      "epoch =  7\n",
      "[7/100] Loss_D: 84.8860 Loss_G: 6786.4017 D(x): 72.2089 D(G(z)): 5.3989 / 4.2534 Elapsed 13.23 s\n",
      "epoch =  8\n",
      "[8/100] Loss_D: 77.8033 Loss_G: 6972.3742 D(x): 72.2361 D(G(z)): 5.5003 / 4.4111 Elapsed 13.19 s\n",
      "epoch =  9\n",
      "[9/100] Loss_D: 71.4859 Loss_G: 7162.3768 D(x): 72.2961 D(G(z)): 5.4397 / 4.3679 Elapsed 12.99 s\n",
      "epoch =  10\n",
      "[10/100] Loss_D: 66.9786 Loss_G: 7216.8363 D(x): 71.7178 D(G(z)): 6.0738 / 4.8877 Elapsed 13.21 s\n",
      "epoch =  11\n",
      "[11/100] Loss_D: 64.0695 Loss_G: 7329.9448 D(x): 71.0108 D(G(z)): 6.8464 / 5.5891 Elapsed 12.52 s\n",
      "epoch =  12\n",
      "[12/100] Loss_D: 61.1106 Loss_G: 7433.0893 D(x): 70.3305 D(G(z)): 7.5153 / 6.1567 Elapsed 13.50 s\n",
      "epoch =  13\n",
      "[13/100] Loss_D: 56.9696 Loss_G: 7560.7122 D(x): 70.1099 D(G(z)): 7.7132 / 6.4784 Elapsed 13.32 s\n",
      "epoch =  14\n",
      "[14/100] Loss_D: 52.1128 Loss_G: 7720.2423 D(x): 70.3081 D(G(z)): 7.6331 / 6.4665 Elapsed 12.91 s\n",
      "epoch =  15\n",
      "[15/100] Loss_D: 49.6569 Loss_G: 7922.4688 D(x): 70.1761 D(G(z)): 7.7352 / 6.5543 Elapsed 13.50 s\n",
      "epoch =  16\n",
      "[16/100] Loss_D: 46.5563 Loss_G: 8038.8716 D(x): 69.9818 D(G(z)): 7.9480 / 6.8002 Elapsed 12.71 s\n",
      "epoch =  17\n",
      "[17/100] Loss_D: 43.7014 Loss_G: 8251.2357 D(x): 70.1063 D(G(z)): 7.8768 / 6.7727 Elapsed 13.68 s\n",
      "epoch =  18\n",
      "[18/100] Loss_D: 40.1529 Loss_G: 8394.9765 D(x): 70.2167 D(G(z)): 7.7990 / 6.7536 Elapsed 12.82 s\n",
      "epoch =  19\n",
      "[19/100] Loss_D: 37.2315 Loss_G: 8645.4550 D(x): 70.5578 D(G(z)): 7.4661 / 6.4320 Elapsed 13.17 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 100\n",
    "d_labelSmooth = 0.2\n",
    "\n",
    "train_our(num_epochs, dataloader, netD,netG,d_labelSmooth, outputDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
