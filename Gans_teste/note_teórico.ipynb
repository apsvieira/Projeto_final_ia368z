{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Como fica o tamanho da dimensão para a operação de Conv2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$ out\\_dim = \\lceil\\frac{in\\_dim - kernel\\_size + 1 + 2*padding}{stride}\\rceil $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convt = nn.ConvTranspose2d(in_channels=nz, out_channels=ngf * 8, kernel_size=(4,4), stride=stride, padding=1, bias=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para stride =  1 A dimensao gerada é =  torch.Size([1, 24, 34, 34])\n",
      "Para stride =  2 A dimensao gerada é =  torch.Size([1, 24, 17, 17])\n",
      "Para stride =  3 A dimensao gerada é =  torch.Size([1, 24, 12, 12])\n",
      "Para stride =  4 A dimensao gerada é =  torch.Size([1, 24, 9, 9])\n",
      "Para stride =  5 A dimensao gerada é =  torch.Size([1, 24, 7, 7])\n",
      "Para stride =  6 A dimensao gerada é =  torch.Size([1, 24, 6, 6])\n",
      "Para stride =  7 A dimensao gerada é =  torch.Size([1, 24, 5, 5])\n",
      "Para stride =  8 A dimensao gerada é =  torch.Size([1, 24, 5, 5])\n",
      "Para stride =  9 A dimensao gerada é =  torch.Size([1, 24, 4, 4])\n",
      "Para stride =  10 A dimensao gerada é =  torch.Size([1, 24, 4, 4])\n",
      "Para stride =  11 A dimensao gerada é =  torch.Size([1, 24, 4, 4])\n",
      "Para stride =  12 A dimensao gerada é =  torch.Size([1, 24, 3, 3])\n",
      "Para stride =  13 A dimensao gerada é =  torch.Size([1, 24, 3, 3])\n",
      "Para stride =  14 A dimensao gerada é =  torch.Size([1, 24, 3, 3])\n",
      "Para stride =  15 A dimensao gerada é =  torch.Size([1, 24, 3, 3])\n",
      "Para stride =  16 A dimensao gerada é =  torch.Size([1, 24, 3, 3])\n",
      "Para stride =  17 A dimensao gerada é =  torch.Size([1, 24, 2, 2])\n",
      "Para stride =  18 A dimensao gerada é =  torch.Size([1, 24, 2, 2])\n",
      "Para stride =  19 A dimensao gerada é =  torch.Size([1, 24, 2, 2])\n",
      "Para stride =  20 A dimensao gerada é =  torch.Size([1, 24, 2, 2])\n",
      "Para stride =  21 A dimensao gerada é =  torch.Size([1, 24, 2, 2])\n",
      "Para stride =  22 A dimensao gerada é =  torch.Size([1, 24, 2, 2])\n",
      "Para stride =  23 A dimensao gerada é =  torch.Size([1, 24, 2, 2])\n",
      "Para stride =  24 A dimensao gerada é =  torch.Size([1, 24, 2, 2])\n",
      "Para stride =  25 A dimensao gerada é =  torch.Size([1, 24, 2, 2])\n",
      "Para stride =  26 A dimensao gerada é =  torch.Size([1, 24, 2, 2])\n",
      "Para stride =  27 A dimensao gerada é =  torch.Size([1, 24, 2, 2])\n",
      "Para stride =  28 A dimensao gerada é =  torch.Size([1, 24, 2, 2])\n",
      "Para stride =  29 A dimensao gerada é =  torch.Size([1, 24, 2, 2])\n",
      "Para stride =  30 A dimensao gerada é =  torch.Size([1, 24, 2, 2])\n",
      "Para stride =  31 A dimensao gerada é =  torch.Size([1, 24, 2, 2])\n",
      "Para stride =  32 A dimensao gerada é =  torch.Size([1, 24, 2, 2])\n",
      "Para stride =  33 A dimensao gerada é =  torch.Size([1, 24, 2, 2])\n",
      "Para stride =  34 A dimensao gerada é =  torch.Size([1, 24, 1, 1])\n",
      "Para stride =  35 A dimensao gerada é =  torch.Size([1, 24, 1, 1])\n",
      "Para stride =  36 A dimensao gerada é =  torch.Size([1, 24, 1, 1])\n",
      "Para stride =  37 A dimensao gerada é =  torch.Size([1, 24, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "### stride = 1\n",
    "in_dim = 12\n",
    "img_size = 32 \n",
    "for i in range(img_size + 5):\n",
    "    entrada1 = torch.randn(1,in_dim, img_size, img_size)\n",
    "\n",
    "    entrada1 = Variable(entrada1)\n",
    "    convt = nn.Conv2d(in_channels=in_dim, out_channels=in_dim*2, kernel_size=(3,3), stride = i+1, padding = 2)\n",
    "    \n",
    "\n",
    "    entrada2 = convt(entrada1)\n",
    "    #print('Para padding = ',i, 'A dimensao gerada é = ',entrada2.size())\n",
    "    print('Para stride = ',i+1, 'A dimensao gerada é = ',entrada2.size())\n",
    "    #plt.imshow(entrada2.data.numpy()[0,0,:,:],cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede D do Lotufo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netD_DCGAN_MNIST(nn.Module):\n",
    "    def __init__(self, nc, ndf=64):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 28 x 28\n",
    "            nn.Conv2d(nc, ndf, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf) x 14 x 14\n",
    "            nn.Conv2d(ndf, ndf * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*2) x 7 x 7\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*4) x 4 x 4\n",
    "            nn.Conv2d(ndf * 4, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output.view(-1, 1).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na hora de fazer a codificação da informação, por redes convolucionais, é interessante jogar a imagem para o teto da metade $\\lceil \\frac{n}{2} \\rceil$ e usar stride um pouco maior e padding no ínicio (para guardar mais da informação da imagem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nossa rede D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que não dimnuímos o padding e o stride do meio pra frente da rede, talvez para fazer a\n",
    "imagem bater direitinho pois ela tinha tamanho de 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netD_DCGAN(nn.Module):\n",
    "    def __init__(self, ngpu, nz, nc, ndf, n_classes):\n",
    "        super(_netD_DCGAN, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.conv1 = nn.Conv2d(in_channels = nc, out_channels = ndf, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(in_channels = ndf, out_channels = ndf*2, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch2 = nn.BatchNorm2d(ndf * 2)\n",
    "        self.conv3 = nn.Conv2d(in_channels = ndf*2, out_channels = ndf*4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch3 = nn.BatchNorm2d(ndf * 4)\n",
    "        self.conv4 = nn.Conv2d(in_channels = ndf*4, out_channels = ndf*8, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch4 = nn.BatchNorm2d(ndf * 8)\n",
    "        \n",
    "        #self.final_conv = nn.Conv2d(in_channels=ndf*8, out_channels=n_classes+1,kernel_size=4,stride=1,padding=0,bias=False)\n",
    "        self.final_conv = nn.Conv2d(in_channels=ndf*8, out_channels=1,kernel_size=4,stride=1,padding=0,bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2, inplace=True)\n",
    "        x = F.leaky_relu(self.batch2(self.conv2(x)), 0.2, inplace=True)\n",
    "        x = F.leaky_relu(self.batch3(self.conv3(x)), 0.2, inplace=True)\n",
    "        x = F.leaky_relu(self.batch4(self.conv4(x)), 0.2, inplace=True)\n",
    "        x = self.final_conv(x)\n",
    "        x = F.sigmoid(x)\n",
    "        \n",
    "        #x = x.view(-1, 1).squeeze(1)\n",
    "\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Como fica o tamanho da dimensão para a operação de Conv2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$ out\\_dim = stride*in\\_dim +kernel\\_size -stride -2*padding $$\n",
    "\n",
    "Até a dimensão mínima que  é (1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para padding =  0 A dimensao gerada é =  torch.Size([1, 800, 13, 13])\n",
      "Para padding =  1 A dimensao gerada é =  torch.Size([1, 800, 11, 11])\n",
      "Para padding =  2 A dimensao gerada é =  torch.Size([1, 800, 9, 9])\n",
      "Para padding =  3 A dimensao gerada é =  torch.Size([1, 800, 7, 7])\n",
      "Para padding =  4 A dimensao gerada é =  torch.Size([1, 800, 5, 5])\n",
      "Para padding =  5 A dimensao gerada é =  torch.Size([1, 800, 3, 3])\n",
      "Para padding =  6 A dimensao gerada é =  torch.Size([1, 800, 1, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (100 x 10 x 10). Calculated output size: (800 x -1 x -1). Output size is too small at /opt/conda/conda-bld/pytorch_1503968623488/work/torch/lib/THNN/generic/SpatialFullDilatedConvolution.c:107",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-99398860726f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mentrada2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentrada1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Para padding = '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'A dimensao gerada é = '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mentrada2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#print('Para stride = ',i+1, 'A dimensao gerada é = ',entrada2.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gabriel/anaconda3/envs/py35/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gabriel/anaconda3/envs/py35/lib/python3.5/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    564\u001b[0m         return F.conv_transpose2d(\n\u001b[1;32m    565\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gabriel/anaconda3/envs/py35/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv_transpose2d\u001b[0;34m(input, weight, bias, stride, padding, output_padding, groups, dilation)\u001b[0m\n\u001b[1;32m    172\u001b[0m     f = ConvNd(_pair(stride), _pair(padding), _pair(dilation), True,\n\u001b[1;32m    173\u001b[0m                _pair(output_padding), groups, torch.backends.cudnn.benchmark, torch.backends.cudnn.enabled)\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (100 x 10 x 10). Calculated output size: (800 x -1 x -1). Output size is too small at /opt/conda/conda-bld/pytorch_1503968623488/work/torch/lib/THNN/generic/SpatialFullDilatedConvolution.c:107"
     ]
    }
   ],
   "source": [
    "stride = 1\n",
    "in_dim = 100\n",
    "img_size = 32 \n",
    "for i in range(img_size + 5):\n",
    "    entrada1 = torch.randn(1,in_dim, 10, 10)\n",
    "\n",
    "    entrada1 = Variable(entrada1)\n",
    "    \n",
    "    convt = nn.ConvTranspose2d(in_channels=in_dim, out_channels= in_dim * 8, kernel_size=(4,4), stride=stride, padding=i,bias=False)\n",
    "\n",
    "\n",
    "    entrada2 = convt(entrada1)\n",
    "    print('Para padding = ',i, 'A dimensao gerada é = ',entrada2.size())\n",
    "    #print('Para stride = ',i+1, 'A dimensao gerada é = ',entrada2.size())\n",
    "    #plt.imshow(entrada2.data.numpy()[0,0,:,:],cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede G do Lotufo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netG_DCGAN_MNIST(nn.Module):\n",
    "    def __init__(self, nz, nc, ngf=64):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(nz, ngf * 4, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # state size. (ngf*4) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # state size. (ngf*2) x 7 x 7\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # state size. (ngf) x 14 x 14\n",
    "            nn.ConvTranspose2d(ngf, nc, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.Tanh()\n",
    "            \n",
    "            # state size. (nc) x 28 x 28\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não sei direito como funciona a convolução transposta, mas parece que ele apenas fez o caminho inverso da rede Conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note que a conv2 tem parametros diferentes da rede D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netG_DCGAN(nn.Module):\n",
    "    def __init__(self, ngpu, nz, nc , ngf):\n",
    "        super(_netG_DCGAN, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.convt1 = nn.ConvTranspose2d(in_channels=nz, out_channels=ngf * 8, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        self.batch1 = nn.BatchNorm2d(ngf*8)\n",
    "        self.convt2 = nn.ConvTranspose2d(in_channels=ngf * 8, out_channels=ngf * 4, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        self.batch2 = nn.BatchNorm2d(ngf*4)\n",
    "        self.convt3 = nn.ConvTranspose2d(in_channels=ngf * 4, out_channels=ngf * 2, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch3 = nn.BatchNorm2d(ngf*2)\n",
    "        self.convt4 = nn.ConvTranspose2d(in_channels=ngf*2, out_channels=ngf, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch4 = nn.BatchNorm2d(ngf)\n",
    "        \n",
    "        self.final_convt = nn.ConvTranspose2d(in_channels=ngf, out_channels=nc, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print('1', x.size())\n",
    "        x = F.leaky_relu(self.batch1(self.convt1(x)), 0.2, inplace=True)\n",
    "        print('2', x.size())\n",
    "        \n",
    "        x = F.leaky_relu(self.batch2(self.convt2(x)), 0.2, inplace=True)\n",
    "        print('3', x.size())\n",
    "        \n",
    "        x = F.leaky_relu(self.batch3(self.convt3(x)), 0.2, inplace=True)\n",
    "        print('4', x.size())\n",
    "        \n",
    "        x = F.leaky_relu(self.batch4(self.convt4(x)), 0.2, inplace=True)\n",
    "        print('5', x.size())\n",
    "        \n",
    "        x = self.final_convt(x)\n",
    "        print('6', x.size())\n",
    "        \n",
    "        \n",
    "        x = F.tanh(x)\n",
    "        print('7', x.size())\n",
    "        \n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultado do teste da rede do Lotufo com LeakyRELU no lugar das RELU's e a rede igual a dele"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Relu_MNIST.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets\t\t     naive_DCGAN.ipynb\t\t\tREADME.MD\r\n",
      "leaky.png\t\t     note_teórico.ipynb\t\t\tRelu_MNIST.png\r\n",
      "naive_DCGAN_all_leaky.ipynb  outputdir_train_classifier_teste2\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEAKY RELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](leaky.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
