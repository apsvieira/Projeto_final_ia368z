{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System properties and libs currently in use\n",
    "- We have developed using python 3.5.x, pytorch 0.2.1\n",
    "- No significant attention was given to backwards compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 3.6.3 (default, Oct 31 2017, 12:34:50) \n",
      "[GCC 5.4.0 20160609]\n",
      "__pyTorch VERSION: 0.2.0_3\n",
      "__CUDA VERSION\n",
      "__CUDNN VERSION: 6021\n",
      "__Number CUDA Devices: 4\n",
      "__Devices\n",
      "Active CUDA Device: GPU 0\n"
     ]
    }
   ],
   "source": [
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION')\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "print('Active CUDA Device: GPU', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities\n",
    "- Saving images and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(netG, fixed_noise, outputDir,epoch):\n",
    "    '''\n",
    "    Generates a batch of images from the given 'noise'.\n",
    "    Saves 64 of the generated samples to 'outputDir' system path.\n",
    "    Inputs are the network (netG), a 'noise' input, system path to which images will be saved (outputDir) and current 'epoch'.\n",
    "    '''\n",
    "    noise = Variable(fixed_noise)\n",
    "    netG.eval()\n",
    "    fake = netG(noise)\n",
    "    netG.train()\n",
    "    vutils.save_image(fake.data[0:64,:,:,:],'%s/fake_samples_epoch_%03d.png' % (outputDir, epoch), nrow=8)\n",
    "\n",
    "def save_models(netG, netD, outputDir, epoch):\n",
    "    '''\n",
    "    Saves model state dictionary for generator and discriminator networks.\n",
    "    Inputs are the networks (netG, netD), the system path in which to save(outputDir) and the current 'epoch'.\n",
    "    '''\n",
    "    torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (outputDir, epoch))\n",
    "    torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (outputDir, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using CUDA. If it is not what you want, manually set this as False!\n"
     ]
    }
   ],
   "source": [
    "cudnn.benchmark = True\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"You are using CUDA. If it is not what you want, manually set this as False!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Directory\n",
    "This is where images will be saved to.\n",
    "\n",
    "If directory does not exist, it is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS error: [Errno 17] File exists: 'DCGAN_LS_decay_02'\n"
     ]
    }
   ],
   "source": [
    "outputDir = 'DCGAN_LS_decay_02'\n",
    "\n",
    "try:\n",
    "    os.makedirs(outputDir)\n",
    "except OSError as err:\n",
    "    print(\"OS error: {0}\".format(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset definition and hyperparameter setting\n",
    "- Changing dataset name alters network architecture parameters\n",
    "- Currently supporting few datasets\n",
    "- Hyperparameters defined according to Radford et al. (2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "chosen_dataset = 'MNIST'\n",
    "\n",
    "datasets = {\n",
    "    'MNIST': torchvision.datasets.MNIST,\n",
    "    'CIFAR10': torchvision.datasets.CIFAR10,\n",
    "    'ANIME': '/home/gabriel/Redes Neurais/Projeto_Final_GANS/Tutorial_2/dataset/min_anime-faces',\n",
    "}\n",
    "\n",
    "dataset = datasets[chosen_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_parameters = {\n",
    "    'MNIST': {\n",
    "        'ndf': 64,\n",
    "        'ngf': 64,\n",
    "        'nz': 100,\n",
    "        'nc': 1,\n",
    "        'imageSize': 64,\n",
    "        'n_classes' : 10,\n",
    "        'ngpu': 1,\n",
    "    },\n",
    "    'CIFAR10': {\n",
    "        'ndf': 64,\n",
    "        'ngf': 64,\n",
    "        'nz': 100,\n",
    "        'nc': 3,\n",
    "        'imageSize' : 64,\n",
    "        'n_classes' : 10,\n",
    "        'ngpu' : 1,\n",
    "    },\n",
    "    'ANIME': {\n",
    "        'nc' : 3,\n",
    "        'ngpu' : 1,\n",
    "        'nz' : 100,\n",
    "        'ngf' : 64,\n",
    "        'ndf' : 64,\n",
    "        'imageSize' : 64,\n",
    "        'n_classes' : 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngf = possible_parameters[chosen_dataset]['ngf']\n",
    "ndf = possible_parameters[chosen_dataset]['ndf']\n",
    "nz = possible_parameters[chosen_dataset]['nz']\n",
    "nc = possible_parameters[chosen_dataset]['nc']\n",
    "imageSize = possible_parameters[chosen_dataset]['imageSize']\n",
    "n_classes = possible_parameters[chosen_dataset]['n_classes']\n",
    "ngpu = possible_parameters[chosen_dataset]['ngpu']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader length: 938\n",
      "Dataset: <torchvision.datasets.mnist.MNIST object at 0x7f5595d88ef0>\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'ANIME':\n",
    "    dataset = torchvision.datasets.ImageFolder(\n",
    "        root='/home/gabriel/Redes Neurais/Projeto_Final_GANS/Tutorial_2/dataset/min_anime-faces',\n",
    "        transform=transforms.Compose([\n",
    "                transforms.Scale((imageSize, imageSize)),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "    )\n",
    "else:\n",
    "    transform = transforms.Compose([\n",
    "                    transforms.Scale((imageSize, imageSize)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)), # bring images to (-1,1)\n",
    "                ]) \n",
    "    dataset_done = dataset('./datasets', train=True, download=True, transform=transform)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset_done, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "print('Dataloader length:', len(dataloader))\n",
    "print(\"Dataset:\", dataloader.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição dos modelos\n",
    "- Model is a DCGAN\n",
    "- Images are sized (nc, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _netD_DCGAN(nn.Module):\n",
    "    def __init__(self, ngpu, nz, nc, ndf, n_classes):\n",
    "        super(_netD_DCGAN, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.conv1 = nn.Conv2d(in_channels = nc, out_channels = ndf, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(in_channels = ndf, out_channels = ndf*2, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch2 = nn.BatchNorm2d(ndf * 2)\n",
    "        self.conv3 = nn.Conv2d(in_channels = ndf*2, out_channels = ndf*4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch3 = nn.BatchNorm2d(ndf * 4)\n",
    "        self.conv4 = nn.Conv2d(in_channels = ndf*4, out_channels = ndf*8, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch4 = nn.BatchNorm2d(ndf * 8)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(in_channels=ndf*8, out_channels=n_classes+1,kernel_size=4,stride=1,padding=0,bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2, inplace=True)\n",
    "        x = F.leaky_relu(self.batch2(self.conv2(x)), 0.2, inplace=True)\n",
    "        x = F.leaky_relu(self.batch3(self.conv3(x)), 0.2, inplace=True)\n",
    "        x = F.leaky_relu(self.batch4(self.conv4(x)), 0.2, inplace=True)\n",
    "        \n",
    "        x = self.final_conv(x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _netG_DCGAN(nn.Module):\n",
    "    def __init__(self, ngpu, nz, nc , ngf):\n",
    "        super(_netG_DCGAN, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.convt1 = nn.ConvTranspose2d(in_channels=nz, out_channels=ngf * 8, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        self.batch1 = nn.BatchNorm2d(ngf*8)\n",
    "        self.convt2 = nn.ConvTranspose2d(in_channels=ngf * 8, out_channels=ngf * 4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch2 = nn.BatchNorm2d(ngf*4)\n",
    "        self.convt3 = nn.ConvTranspose2d(in_channels=ngf * 4, out_channels=ngf * 2, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch3 = nn.BatchNorm2d(ngf*2)\n",
    "        self.convt4 = nn.ConvTranspose2d(in_channels=ngf*2, out_channels=ngf, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch4 = nn.BatchNorm2d(ngf)\n",
    "        \n",
    "        self.final_convt = nn.ConvTranspose2d(in_channels=ngf, out_channels=nc, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.batch1(self.convt1(x)), 0.2, inplace=True)\n",
    "        x = F.leaky_relu(self.batch2(self.convt2(x)), 0.2, inplace=True)\n",
    "        x = F.leaky_relu(self.batch3(self.convt3(x)), 0.2, inplace=True)\n",
    "        x = F.leaky_relu(self.batch4(self.convt4(x)), 0.2, inplace=True)\n",
    "        \n",
    "        x = self.final_convt(x)\n",
    "        x = F.tanh(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = _netG_DCGAN(ngpu, nz, nc, ngf)\n",
    "netD = _netD_DCGAN(ngpu, nz, nc, ndf, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializador de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netG_DCGAN (\n",
      "  (convt1): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  (batch1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (convt2): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (convt3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (convt4): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (final_convt): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      ") \n",
      " _netD_DCGAN (\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv4): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (final_conv): Conv2d(512, 11, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "print(netG, '\\n', netD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses\n",
    "- Binary Cross-Entropy is used to differentiate real and fake images when using a discriminator\n",
    "- Class loss should be Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sizes of the tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input images size: torch.Size([64, 3, 64, 64])\n",
      "Code size: torch.Size([64, 100, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "input = torch.FloatTensor(batch_size, 3, imageSize, imageSize)\n",
    "print('Input images size:', input.size())\n",
    "noise = torch.FloatTensor(batch_size, nz, 1, 1)\n",
    "fixed_noise = torch.FloatTensor(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "print('Code size:', noise.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label size: torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "label = torch.LongTensor(batch_size,n_classes)\n",
    "print('Label size:', label.size())\n",
    "fake_label = 10\n",
    "real_label = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    netD = netD.cuda(1)\n",
    "    netG = netG.cuda(1)\n",
    "    criterion = criterion.cuda(1)\n",
    "    input,label = input.cuda(1), label.cuda(1)\n",
    "    noise, fixed_noise = noise.cuda(1), fixed_noise.cuda(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Parameters\n",
    "- Following the lead of Radford et al., 2015:\n",
    "\n",
    "    <b>\n",
    "    1. beta1 = 0.5\n",
    "    2. beta2 = 0.999\n",
    "    3. lr = 0.0002\n",
    "    </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1, beta2 = 0.5, 0.999\n",
    "lr = 2.0e-4\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = lr/2, betas = (beta1, beta2))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = lr, betas = (beta1, beta2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Treinamento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(num_epochs, dataloader, netD, netG, outputDir,\n",
    "              real_labelSmooth=0, epoch_interval=100, D_steps=1, G_steps=1, atenuation_epochs=25):\n",
    "    \n",
    "    # This validation is subjective. WGAN-GP uses 100 steps on the critic (netD).\n",
    "    assert D_steps < 5, \"Keep it low, D_steps is too high.\"\n",
    "    assert G_steps < 3, \"Keep it low, G_steps is too high.\"\n",
    "    #assert batch_size % D_steps == 0, \"Use batch_size multiple of D_steps.\"\n",
    "    real_label = 1\n",
    "    print('Lets train!')\n",
    "    for epoch in range(num_epochs):\n",
    "        start_iter = time.time()  \n",
    "        D_x = 0\n",
    "        D_G_z1 = 0\n",
    "        D_G_z2 = 0\n",
    "        errD_acum = 0\n",
    "        errG_acum = 0\n",
    "\n",
    "        for batch, data in enumerate(dataloader, 0):\n",
    "            if (epoch == 0 and batch == 0):\n",
    "                    vutils.save_image(data[0][0:64,:,:,:], '%s/real_samples.png' % outputDir, nrow=8)\n",
    "            real_labelSmooth = np.maximum(real_labelSmooth * (1 - (1/atenuation_epochs)*epoch), 0)\n",
    "            for step in range(D_steps):\n",
    "                #############################################################\n",
    "                # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "                # 1A - Train the detective network in the Real Dataset\n",
    "                #############################################################\n",
    "                netD.zero_grad()\n",
    "                start = step*(int(data[0].size()[0]/D_steps))\n",
    "                end = (step+1)*int(data[0].size()[0]/D_steps)\n",
    "                \n",
    "                real_cpu = data[0][start:end]\n",
    "                real_cpu = real_cpu.cuda(1)\n",
    "                batch_size = real_cpu.size(0)\n",
    "                if np.random.random_sample() > real_labelSmooth:\n",
    "                    target = data[1][start:end].long().cuda(1)\n",
    "                else:\n",
    "                     target = torch.from_numpy(np.random.randint(0, n_classes, batch_size)).type(torch.LongTensor).cuda(1)\n",
    "                \n",
    "                input, label = Variable(real_cpu), Variable(target)\n",
    "\n",
    "                output = netD(input)\n",
    "                errD_real = criterion(output.squeeze(),label)\n",
    "                errD_real.backward()\n",
    "                \n",
    "                D_x += output.data.mean()\n",
    "                \n",
    "                #######################################################\n",
    "                # 1B - Train the detective network in the False Dataset\n",
    "                #######################################################\n",
    "                \n",
    "                noise = Variable(torch.FloatTensor(batch_size, nz, 1, 1).normal_(0,1).cuda(1))\n",
    "                fake = netG(noise)\n",
    "                label = Variable(torch.ones(batch_size).long().fill_(fake_label).cuda(1))\n",
    "                output = netD(fake.detach()) # \".detach()\" to avoid backprop through G\n",
    "                errD_fake = criterion(output.squeeze(), label)\n",
    "                errD_fake.backward() # gradients for fake and real data will be accumulated\n",
    "                \n",
    "                D_G_z1 += output.data.mean()\n",
    "                errD_acum += errD_real.data[0] + errD_fake.data[0]\n",
    "                optimizerD.step()\n",
    "\n",
    "            for step in range(G_steps):\n",
    "                ####################################################################################\n",
    "                # (2) Update G network: maximize log(D(G(z)))\n",
    "                # Train the faker with the output from the Detective (but don't train the Detective)\n",
    "                ####################################################################################\n",
    "                \n",
    "                netG.zero_grad()\n",
    "                label = Variable(torch.from_numpy(np.random.randint(0, n_classes, batch_size)).type(torch.LongTensor).cuda(1))\n",
    "                output = netD(fake)\n",
    "                errG = criterion(output.squeeze(), label)\n",
    "                errG.backward()\n",
    "                \n",
    "                D_G_z2 += output.data.mean()\n",
    "                errG_acum += errG.data[0]\n",
    "                optimizerG.step()\n",
    "\n",
    "        end_iter = time.time()        \n",
    "\n",
    "        print('[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f Elapsed %.2f s'\n",
    "              % (epoch, num_epochs, errD_acum/D_steps, errG_acum/G_steps, D_x, D_G_z1, D_G_z2, end_iter-start_iter))\n",
    "\n",
    "        #Save a grid with the pictures from the dataset, up until 64\n",
    "        save_images(netG = netG, fixed_noise=  fixed_noise, outputDir = outputDir, epoch = epoch)\n",
    "\n",
    "        if epoch % epoch_interval == 0:\n",
    "            # do checkpointing\n",
    "            save_models(netG = netG, netD = netD, outputDir = outputDir, epoch = epoch)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets train!\n",
      "[0/50] Loss_D: 1575.8536 Loss_G: 6060.0755 D(x): -548.2751 D(G(z)): -627.7168 / -633.0502 Elapsed 166.75 s\n",
      "[1/50] Loss_D: 369.8914 Loss_G: 7939.7908 D(x): -883.8605 D(G(z)): -933.0339 / -936.7240 Elapsed 165.94 s\n",
      "[2/50] Loss_D: 264.4880 Loss_G: 9759.7001 D(x): -1330.6217 D(G(z)): -1385.3563 / -1390.2121 Elapsed 166.75 s\n",
      "[3/50] Loss_D: 228.4350 Loss_G: 10679.6820 D(x): -1650.7920 D(G(z)): -1699.3168 / -1704.8233 Elapsed 167.74 s\n",
      "[4/50] Loss_D: 195.3261 Loss_G: 11602.0827 D(x): -2002.0165 D(G(z)): -2081.4157 / -2089.2980 Elapsed 167.61 s\n",
      "[5/50] Loss_D: 179.4568 Loss_G: 12214.9109 D(x): -2258.7871 D(G(z)): -2356.0134 / -2364.5855 Elapsed 168.10 s\n",
      "[6/50] Loss_D: 151.7119 Loss_G: 12710.9035 D(x): -2547.7531 D(G(z)): -2652.0682 / -2660.7710 Elapsed 168.45 s\n",
      "[7/50] Loss_D: 192.3366 Loss_G: 12746.7633 D(x): -2698.8626 D(G(z)): -2801.4757 / -2810.1356 Elapsed 168.55 s\n",
      "[8/50] Loss_D: 142.1245 Loss_G: 13371.9690 D(x): -3050.8823 D(G(z)): -3166.9864 / -3174.9208 Elapsed 168.35 s\n",
      "[9/50] Loss_D: 118.4694 Loss_G: 13895.9563 D(x): -3278.3193 D(G(z)): -3437.4857 / -3446.3458 Elapsed 167.93 s\n",
      "[10/50] Loss_D: 135.6091 Loss_G: 14171.4067 D(x): -3345.6401 D(G(z)): -3520.1650 / -3531.0040 Elapsed 168.34 s\n",
      "[11/50] Loss_D: 138.4740 Loss_G: 14409.3230 D(x): -3605.7656 D(G(z)): -3797.3566 / -3810.4933 Elapsed 168.54 s\n",
      "[12/50] Loss_D: 127.2166 Loss_G: 14397.6154 D(x): -3758.2236 D(G(z)): -3965.7945 / -3976.1436 Elapsed 168.07 s\n",
      "[13/50] Loss_D: 140.1187 Loss_G: 14491.9811 D(x): -3829.8863 D(G(z)): -4016.1321 / -4025.3608 Elapsed 168.34 s\n",
      "[14/50] Loss_D: 104.1189 Loss_G: 15004.8110 D(x): -3896.8403 D(G(z)): -4143.5842 / -4153.1407 Elapsed 168.52 s\n",
      "[15/50] Loss_D: 111.1863 Loss_G: 14772.5172 D(x): -3949.3777 D(G(z)): -4153.7442 / -4164.2492 Elapsed 168.29 s\n",
      "[16/50] Loss_D: 96.2170 Loss_G: 15523.3363 D(x): -3917.8895 D(G(z)): -4256.3374 / -4269.1977 Elapsed 168.44 s\n",
      "[17/50] Loss_D: 91.8378 Loss_G: 15520.0092 D(x): -3986.7198 D(G(z)): -4315.3640 / -4331.3942 Elapsed 168.36 s\n",
      "[18/50] Loss_D: 130.0205 Loss_G: 15474.4775 D(x): -4161.9748 D(G(z)): -4473.8582 / -4488.2802 Elapsed 168.37 s\n",
      "[19/50] Loss_D: 92.8822 Loss_G: 15867.1346 D(x): -4294.6664 D(G(z)): -4674.3977 / -4686.5025 Elapsed 168.19 s\n",
      "[20/50] Loss_D: 81.7443 Loss_G: 15837.9588 D(x): -4374.7661 D(G(z)): -4692.5629 / -4704.8598 Elapsed 168.12 s\n",
      "[21/50] Loss_D: 94.4210 Loss_G: 16174.4447 D(x): -4385.5930 D(G(z)): -4808.4658 / -4820.3494 Elapsed 167.99 s\n",
      "[22/50] Loss_D: 153.4350 Loss_G: 15488.3968 D(x): -4428.3022 D(G(z)): -4695.7127 / -4709.9110 Elapsed 168.55 s\n",
      "[23/50] Loss_D: 61.4052 Loss_G: 16546.7138 D(x): -4385.3256 D(G(z)): -4887.8409 / -4899.4933 Elapsed 168.33 s\n",
      "[24/50] Loss_D: 95.7005 Loss_G: 16405.6586 D(x): -4308.9169 D(G(z)): -4802.2511 / -4816.4372 Elapsed 168.67 s\n",
      "[25/50] Loss_D: 79.1554 Loss_G: 16559.3977 D(x): -4491.8807 D(G(z)): -4964.0172 / -4981.8115 Elapsed 168.27 s\n",
      "[26/50] Loss_D: 88.9470 Loss_G: 16825.6708 D(x): -4501.8797 D(G(z)): -5058.6400 / -5075.6901 Elapsed 168.27 s\n",
      "[27/50] Loss_D: 71.1233 Loss_G: 16984.3537 D(x): -4363.0382 D(G(z)): -4971.4045 / -4986.3547 Elapsed 168.59 s\n",
      "[28/50] Loss_D: 75.2028 Loss_G: 16935.5048 D(x): -4581.4484 D(G(z)): -5150.3357 / -5166.4464 Elapsed 168.16 s\n",
      "[29/50] Loss_D: 3.6050 Loss_G: 12646.3228 D(x): -4854.4845 D(G(z)): -6124.8968 / -6175.3630 Elapsed 168.40 s\n",
      "[30/50] Loss_D: 0.8785 Loss_G: 12858.3688 D(x): -5204.4280 D(G(z)): -7425.4533 / -7439.6845 Elapsed 168.37 s\n",
      "[31/50] Loss_D: 0.4047 Loss_G: 13267.5959 D(x): -5380.4657 D(G(z)): -6927.7056 / -6939.9245 Elapsed 168.17 s\n",
      "[32/50] Loss_D: 26.0657 Loss_G: 14603.7800 D(x): -4887.6063 D(G(z)): -6107.6593 / -6161.7986 Elapsed 168.31 s\n",
      "[33/50] Loss_D: 20.3477 Loss_G: 13530.9743 D(x): -4713.3136 D(G(z)): -5109.8869 / -5151.4416 Elapsed 168.79 s\n",
      "[34/50] Loss_D: 10.4438 Loss_G: 13444.8583 D(x): -4929.0871 D(G(z)): -5432.9272 / -5460.8136 Elapsed 168.56 s\n",
      "[35/50] Loss_D: 6.6508 Loss_G: 14024.9064 D(x): -5044.4822 D(G(z)): -5178.9589 / -5198.9105 Elapsed 168.30 s\n",
      "[36/50] Loss_D: 10.7461 Loss_G: 15232.2296 D(x): -4932.4242 D(G(z)): -6757.2668 / -6776.5443 Elapsed 168.57 s\n",
      "[37/50] Loss_D: 7.3188 Loss_G: 16700.9890 D(x): -4926.2116 D(G(z)): -6013.6801 / -6020.2953 Elapsed 168.38 s\n",
      "[38/50] Loss_D: 8.1981 Loss_G: 17286.3881 D(x): -4873.9705 D(G(z)): -5885.6282 / -5897.4031 Elapsed 168.60 s\n",
      "[39/50] Loss_D: 15.9234 Loss_G: 16097.1746 D(x): -5264.0195 D(G(z)): -5379.9259 / -5417.1737 Elapsed 168.02 s\n",
      "[40/50] Loss_D: 6.1990 Loss_G: 15850.2399 D(x): -5501.7924 D(G(z)): -6823.2473 / -6846.0113 Elapsed 168.44 s\n",
      "[41/50] Loss_D: 7.1614 Loss_G: 16385.2498 D(x): -5600.6313 D(G(z)): -5677.4532 / -5703.7665 Elapsed 168.66 s\n",
      "[42/50] Loss_D: 3.5332 Loss_G: 15317.6980 D(x): -5742.4838 D(G(z)): -6043.3352 / -6055.2989 Elapsed 168.19 s\n",
      "[43/50] Loss_D: 4.9489 Loss_G: 17078.1969 D(x): -5817.8307 D(G(z)): -6261.0665 / -6276.3779 Elapsed 168.32 s\n",
      "[44/50] Loss_D: 7.0230 Loss_G: 18112.4791 D(x): -5909.8084 D(G(z)): -6144.6001 / -6164.6739 Elapsed 168.38 s\n",
      "[45/50] Loss_D: 8.6054 Loss_G: 17426.9643 D(x): -5864.9705 D(G(z)): -6433.2197 / -6450.8386 Elapsed 168.43 s\n",
      "[46/50] Loss_D: 5.5025 Loss_G: 17653.4580 D(x): -5586.7103 D(G(z)): -7005.1638 / -7022.2197 Elapsed 168.74 s\n",
      "[47/50] Loss_D: 2.4498 Loss_G: 19749.4676 D(x): -5751.1416 D(G(z)): -7020.9503 / -7033.1168 Elapsed 168.66 s\n",
      "[48/50] Loss_D: 7.2840 Loss_G: 18945.3101 D(x): -5946.8703 D(G(z)): -6359.1191 / -6379.3116 Elapsed 168.59 s\n",
      "[49/50] Loss_D: 4.8100 Loss_G: 19123.2320 D(x): -6040.6155 D(G(z)): -6006.9450 / -6023.1894 Elapsed 168.96 s\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "real_labelSmooth = 0.25\n",
    "smoothing_epochs = 25\n",
    "train_gan(num_epochs, dataloader, netD, netG, outputDir, real_labelSmooth, atenuation_epochs=smoothing_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = dataset('./datasets', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
